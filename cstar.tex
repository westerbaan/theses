\chapter{$\text{C}^*$-algebras}
\begin{parsec}{20}
\begin{point}{10}
We redevelop the essentials of the theory of (unital) $C^*$-algebras
in this chapter.
Since we are ultimately interested
in von Neumann algebras
(a special type of $C^*$-algebras)
we will evade
delicate
topics such as tensor products (of $C^*$-algebras), 
quotients, approximate identities,
and $C^*$-algebras without a unit.
The zenith of this chapter
is \emph{Gelfand's representation theorem} (see~\sref{gelfand}),
the fact that every commutative (unital) $C^*$-algebra
is isomorphic
to the $C^*$-algebra
$C(X)$ of continuous functions on some compact Hausdorff space~$X$
--- it yields a duality
	between the category~$\CH$
	of compact Hausdorff spaces (and continuous maps)
and the category~$\cCstar{miu}$ of commutative $C^*$-algebras (and
unital $*$-homomorphisms,
the appropriate structure preserving maps), see~\sref{gelfand-equivalence}.

As the road to Gelfand's representation theorem 
is a bit winding ---
involving intricate relations between technical concepts --- 
we have put emphasis on the invertible and  positive elements
so that the important
theorems about them may serve as landmarks along the way:
\begin{enumerate}
\item
first we show that the norm
on a $C^*$-algebra
is determined by the invertible elements
(via the \emph{spectral radius}), see~\sref{norm-spectrum};

\item
then we construct a \emph{square root} of a positive element in~\sref{sqrt};

\item
and finally we
show that an element of a commutative $C^*$-algebra
is not invertible iff it is mapped to~$0$
by some multiplicative state, see~\sref{inv-mult-state}.
\end{enumerate}
At every step along the way
the positive and invertible elements 
(and the norm, multiplicative states, multiplication
and other structure on a $C^*$-algebra)
are bound more tightly together
until Gelfand's representation theorem emerges.

To make this chapter
more accessible
we have removed
much material
from the ordinary development
of $C^*$-algebras
such as the more general theory of Banach algebras
(and its pathology).
This forces us
to take a slightly different path than is usual in the literature 
(see e.g.~\sref{gelfand-mazur-predicament}).

After Gelfand's representation theorem
we deal with two smaller topics:
that a $C^*$-algebra
may be represented as a concrete $C^*$-algebra
of bounded operators on a Hilbert space (see~\sref{gns}),
and that the $N\times N$-matrices with entries drawn from a 
$C^*$-algebra~$\scrA$
form a $C^*$-algebra~$M_N(\scrA)$
(see~\sref{cstar-matrices}).
We end
with an overture to von Neumann algebras---the
topic of the next chapter.
\end{point}%
\end{parsec}
\section{Definition and Examples}
\begin{parsec}{30}
\begin{point}{10}{Definition}
A \Define{$C^*$-algebra}\index{Cstar-algebra@$C^*$-algebra}
is a complex vector space~$\scrA$
endowed with
\begin{enumerate}
\item
a binary operation,
called \Define{multiplication}
(and denoted as such),
which is associative, and linear in both coordinates;
\item
an element~$1$, called 
\Define{unit}\index{unit!of a {$C^*$-algebra}},
such that $1\cdot a = a = a\cdot 1$
for all~$a\in \scrA$;
\item
a unary operation $\Define{(\,\cdot\,)^*}$,
called \Define{involution} %
\index{$(\,\cdot\,)^*$ !involution on a $C^*$-algebra}%
\index{involution!on a $C^*$-algebra}
such that $(a^*)^*=a$,
$(ab)^*=b^*a^*$,
$(\lambda a)^* = \bar\lambda a^*$,
and $(a+b)^* = a^*+b^*$
for all~$a,b\in\scrA$ and~$\lambda\in \C$;
\item
a complete \Define{norm}%
\index{$"\"|\,\cdot\,"\"|$, norm!on a $C^*$-algebra}
$\Define{\|\,\cdot\,\|}$
such that
$\|ab\|\leq\|a\|\|b\|$
for all~$a,b\in\scrA$,
and 
\begin{equation*}
\label{eq:Cstar-identity}
\|a^*a\|\ =\ \|a\|^2
\end{equation*}
holds; this equality is called the \Define{$C^*$-identity}.%
\index{Cstar-identity@$C^*$-identity}
\end{enumerate}
The $C^*$-algebra $\scrA$ is called \Define{commutative}%
\index{Cstar-algebra@$C^*$-algebra!commutative}
if $ab=ba$ for all~$a,b\in\scrA$.
\begin{point}{20}{Warning}%
In the literature it is usually not
required that a $C^*$-algebra
possess a unit; but when it does it is called
a \Define{unital $C^*$-algebra}.%
\index{unital!$C^*$-algebra}
\end{point}
\end{point}
\begin{point}{30}{Example}%
The vector space~$\C$ of \Define{complex numbers}%
\index{C@$\C$, the complex numbers!as a $C^*$-algebra}
forms a commutative  $C^*$-algebra
in which
multiplication and~$1$
have their usual meaning.
Involution is given by conjugation ($z^*=\bar{z}$),
and norm by modulus ($\|z\|=|z|$).
\end{point}
\begin{point}{40}{Example}%
A \Define{$C^*$-subalgebra}%
\index{Cstar-subalgebra@$C^*$-subalgebra}
of a $C^*$-algebra~$\scrA$
is a subset~$\scrB$ of~$\scrA$,
which is a linear subspace of~$\scrA$,
contains the unit, $1$, is closed under multiplication
and involution, 
and is closed with respect to the norm of~$\scrA$;
such a $C^*$-subalgebra of~$\scrA$
is itself a $C^*$-algebra
when endowed with the operations and norm
of~$\scrA$.
\end{point}
\begin{point}{50}[cstar-product]{Example}%
One can form products (in the categorical sense,
see~\sref{cstar-product-2}) of $C^*$-algebras as follows.
Let~$\scrA_i$ be a $C^*$-algebra
for every element~$i$ of some index set~$I$.
The \Define{direct sum}%
\index{direct sum!of $C^*$-algebras}
\index{$\bigoplus$, direct sum!$\bigoplus_i \scrA_i$, of $C^*$-algebras}
of the family $(\scrA_i)_i$
is the $C^*$-algebra
denoted by \Define{$\bigoplus_{i\in I}\scrA_i$} on the set
\begin{equation*}
\textstyle
\bigl\{\ 
a\in \prod_{i\in I}\scrA_i\colon\  \sup_{i \in I} \|a(i)\|< \infty \ 
\bigr\}
\end{equation*}
whose operations are defined coordinatewise,
and whose norm is a \Define{supremum norm}%
\index{supremum norm}%
\index{$"\"|\,\cdot\,"\"|$, norm!supremum $\sim$}
given by $\|a\|=\sup_{i}\|a(i)\|$.
If each~$\scrA_i$ is commutative,
then~$\bigoplus_{i\in I}\scrA_i$
is commutative.

In particular,
taking~$\scrA_i\equiv \C$,
we see that
the vector space~\Define{$\ell^\infty(X)$}%
\index{linfty@$\ell^\infty(X)$}%
\index{linfty@$\ell^\infty(X)$!as a $C^*$-algebra}
of bounded complex-valued functions
on a set~$X$ forms a commutative $C^*$-algebra
with pointwise operations and supremum norm.
\end{point}
\begin{point}{60}{Example}%
The \Define{bounded continuous functions
on a topological space}~$X$
form a commutative $C^*$-subalgebra~\Define{$BC(X)$}%
\index{$BC(X)$! as a $C^*$-algebra}
of~$\ell^\infty(X)$ (see above).
In particular,
since a continuous function on a compact Hausdorff space is 
automatically bounded,
we see that the \Define{continuous functions
on a compact Hausdorff space} $X$
form a commutative $C^*$-algebra~\Define{$C(X)$}%
\index{$C(X)$}%
\index{$C(X)$!as a $C^*$-algebra}
with pointwise operations and sup-norm.
We'll see that every commutative $C^*$-algebra
is isomorphic to a~$C(X)$
in~\sref{gelfand}.
\end{point}
\begin{point}{70}[cstar-matrices-example]{Example}%
An example of a non-commutative
$C^*$-algebra
is
the vector space~\Define{$M_n$}%
\index{$M_n$, the $n\times n$-matrices!as a $C^*$-algebra}
of \Define{$n\times n$-matrices} ($n>1$) over~$\C$
with the usual (matrix) multiplication,
the identity matrix as unit,
and conjugate transpose
as involution
(so~$(A^*)_{ij} = \overline{A_{ji}}$).
The norm~$\|A\|$ of a matrix~$A$ in~$M_n$
is less obvious,
being
the \emph{operator norm}
(cf.~\sref{bounded-linear-maps})
of the associated linear map~$v\mapsto Av,\ \C^n\to\C^n$,
that is,
$\|A\|$ is
the least number~$r\geq 0$
with $\|Av\|_2\leq r\|v\|_2$
for all~$v\in \C^n$
(where $\|w\|_2=(\sum_i \left|w_i\right|^2)^{\nicefrac{1}{2}}$
denotes the $2$-norm
of~$w\in \C^n$).

It is not entirely obvious that~$\|A^*A\|=\|A\|^2$
holds
and that $M_n$ is complete.
We will prove these facts in the more general setting
of bounded operators between Hilbert spaces, 
see~\sref{adjoinables-cstar-algebra}.
Suffice it to say, $\C^n$ is a Hilbert space
with~$\left<v,w\right>=\sum_i \overline{v}_iw_i$
as inner product,
each matrix gives a (bounded) linear map $v\mapsto Av,\C^n\to \C^n$,
and the conjugate transpose $A^*$ is \emph{adjoint} to~$A$
in the sense that $\left<v,Aw\right> = \left<A^*v,w\right>$
for all~$v,w\in\C^n$.
\end{point}
\begin{point}{80}{Remark}%
\index{Cstar-algebra@$C^*$-algebra!finite dimensional}
Combining~\sref{cstar-product}
and~\sref{cstar-matrices-example}
we see that
$\bigoplus_k M_{n_k}$
is a finite-dimensional
$C^*$-algebra
for any tuple $n_1,\dotsc,n_K$
of natural numbers.
In fact,
any finite-dimensional $C^*$-algebra
is of this form
as we'll see in~\sref{fdcstar}.\footnote{Although
clearly related to the 
Wedderburn--Artin theorem,
see e.g.~\cite{nicholson1993},
this description of finite-dimensional
$C^*$-algebras
does not seem to be an immediate consequence of it.}
\end{point}
\end{parsec}
\subsection{Operators}
\begin{parsec}{40}[hilb]%
\begin{point}{10}[example-hilb]{Example}%
Let us now turn to perhaps the most important
and difficult example:
we'll show that the vector space~\Define{$\scrB(\scrH)$}%
\index{BH@$\scrB(\scrH)$!as a $C^*$-algebra}
of \Define{bounded operators
on a Hilbert space}~$\scrH$ forms a $C^*$-algebra
when endowed with the operator 
norm.
Multiplication is given by composition,
involution by taking the \emph{adjoint} (see~\sref{hilb-def}),
and unit by the identity operator.
A \Define{concrete $C^*$-algebra}%
\index{Cstar-algebra@$C^*$-algebra!concrete}
or
a \Define{$C^*$-algebra of bounded operators}%
\index{Cstar-algebra@$C^*$-algebra!of bounded operators}
refers to a $C^*$-subalgebra of~$\scrB(\scrH)$.
We will eventually see that every $C^*$-algebra is isomorphic to a $C^*$-algebra
of bounded operators in~\sref{gelfand-naimark}.
\end{point}
\begin{point}{20}[bounded-linear-maps]{Definition}%
Let~$\scrX$ and~$\scrY$ be normed
vector spaces.
We say that~$r\in [0,\infty)$
is a \Define{bound}%
\index{bound!for a linear map}
for a linear map (=\Define{operator}%
\index{operator}) 
$T\colon \scrX\to\scrY$
when  $\|Tx\|\leq r\|x\|$ for all~$x\in \scrX$,
and we say that~$T$ is \Define{bounded}%
\index{operator!bounded}
when there is such a bound.
In that case~$T$ has a least bound,
which is called the \Define{operator norm}%
\index{operator norm}%
\index{$"\"|\,\cdot\,"\"|$, norm!of an operator} of~$T$,
and is denoted by~$\Define{\|T\|}$.
The vector space of bounded operators
from~$\scrX$ to~$\scrY$
is denoted by~$\Define{\scrB(\scrX,\scrY)}$,%
\index{BXY@$\scrB(\scrX,\scrY)$}
and the vector space of bounded operators
from~$\scrX$ to itself is denoted by~$\scrB(\scrX)$.%
\index{BX@$\scrB(\scrX)$}
\end{point}
\begin{point}{30}[bounded-operators-basic]{Exercise}%
Let~$\scrX$, $\scrY$ and~$\scrZ$ be normed complex vector spaces.
\begin{enumerate}
\item
Show that the operator norm on~$\scrB(\scrX,\scrY)$
is, indeed, a norm.
\item
Let~$T\colon \scrX\to \scrY$ and~$S\colon \scrY\to\scrZ$
be bounded operators.
Show that $ST$ is bounded by~$\|S\|\|T\|$,
so that~$\|ST\|\leq\|S\|\|T\|$.
\item
Show that the identity operator $\id\colon \scrX\to \scrX$
is bounded by~$1$.
\end{enumerate}
\end{point}
\begin{point}{40}[operator-norm-ball]{Exercise}%
Let $T\colon \scrX\to\scrY$
be a bounded operator between normed vector spaces,
	and let~$r\in[0,\infty)$.
Show that 
	\begin{equation*}
		\textstyle
		r\|T\|\ =\ \sup_{x\in (\scrX)_r} \|Tx\|,
	\end{equation*}
where $\Define{(\scrX)_r}=\{x\in \scrX\colon \|x\|\leq r\}$.%
\index{*ballr@$(\scrX)_r$, $r$-ball}%
\index{*ball@$(\scrX)_1$, unit ball}
	(The set~$(\scrX)_1$
	is called the \Define{unit ball} of~$\scrX$.)%
\index{unit ball}
\end{point}
\begin{point}{50}[operator-norm-complete]{Lemma}%
The operator norm on~$\scrB(\scrX,\scrY)$ is complete
when~$\scrY$ is a complete normed vector space.
\begin{point}{60}{Proof}%
Let~$(T_n)_n$ be a Cauchy sequence in~$\scrB(\scrX,\scrY)$.
We must show that~$(T_n)_n$ converges to some
bounded operator $T\colon \scrX\to\scrY$.
Let~$x\in \scrX$ be given.
Since 
\begin{equation*}
\|\,T_nx - T_mx\,\|\ =\ \|\,(T_n-T_m)\,x\,\|\ \leq\  \|T_n-T_m\|\,\|x\|
\end{equation*}
and~$\|T_n-T_m\|\to 0$ as~$n,m\to \infty$ 
(because~$(T_k)_k$ is Cauchy),
we see that $\|\,T_nx-T_mx\,\|\to 0$ as $n,m\to \infty$,
and so $(T_nx)_n$ is a Cauchy sequence in~$\scrY$.
Since~$\scrY$ is complete,
 $(T_nx)_n$ converges,
and  we may define $Tx:=\lim_n T_nx$,
giving a map $T\colon \scrX\to \scrY$,
which is easily seen to be linear
(by continuity of addition and scalar multiplication).

It remains to be shown that~$T$ is bounded,
and that~$(T_n)_n$ converges to~$T$ with respect to the operator norm.
Let~$\varepsilon>0$ be given, and pick~$N$ such that
$\|T_n-T_m\|\leq \frac{1}{2}\varepsilon$ for all~$n,m\geq N$.
Then for every~$x\in \scrX$
we can find~$M\geq N$ with 
$\|T x - T_m x\|\leq \frac{1}{2}\varepsilon\|x\|$ for all $m\geq M$,
and so,
for $n\geq N$, $m\geq M$,
\begin{equation*}
\|(T - T_n) x\| \ \leq\ \|T x - T_mx\|\,+\,\|T_m x - T_n x\|
\ \leq\  \varepsilon\|x\|
\end{equation*}
giving that~$T-T_n$ is bounded
and $\|T-T_n\|\leq \varepsilon$ for all~$n\geq N$.
Whence~$T$ is bounded too,
and $(T_n)_n$ converges to~$T$.\qed
\end{point}
\end{point}
\begin{point}{70}[bounded-operators-banach-algebra]%
From~\sref{bounded-operators-basic}
and~\sref{operator-norm-complete}
it is clear that the complex vector space
of bounded operators~$\scrB(\scrX)$
on a complete normed vector space~$\scrX$
with composition as multiplication
and the identity operator as unit
satisfies all the requirements
to be a $C^*$-algebra that do not involve the involution, $(\,\cdot\,)^*$
(that is, $\scrB(\scrX)$ is a \Define{Banach algebra}).
To get an involution,
we need the additional structure
provided by a Hilbert space as follows.
\end{point}
\begin{point}{80}[hilb-def]{Definition}%
An \Define{inner product}%
\index{inner product!$\C$-valued}
on a complex vector space~$V$ 
is a map $\left<\,\cdot\,,\,\cdot\,\right>\colon V\times V\to \C$%
\index{$\left<\,\cdot\,,\,\cdot\,\right>$, inner product!$\C$-valued}
such that,
for all~$x,y\in V$,
$\left<x,\,\cdot\,\right>\colon V\to V$ is linear;
$\left<x,x\right>\geq 0$;
and
$\left<x,y\right>=\overline{\left<y,x\right>}$.
We say that the inner product is \Define{definite}%
\index{inner product!$\C$-valued!definite}
when~$\left<x,x\right>=0\implies x=0$ for~$x\in V$.
A \Define{pre-Hilbert space}~$\scrH$%
\index{pre-Hilbert space}
is a complex vector space endowed with a definite inner product.
We'll shortly see that every such~$\scrH$
carries a norm
given by
 $\|x\|:= \left<x,x\right>^{\nicefrac{1}{2}}$;
if~$\scrH$ is complete with respect to this norm,
we say that~$\scrH$ is a \Define{Hilbert space}.%
\index{Hilbert space}

Let~$\scrH$ and~$\scrK$ be pre-Hilbert spaces.
We say that an operator~$T\colon \scrH\to \scrK$
is \Define{adjoint}%
\index{adjoint of an operator}
to an operator
$S\colon \scrK\to \scrH$ 
when
\begin{equation*}
\left<Tx,y\right> \ = \ \left<x,Sy\right>
\qquad\text{for all $x\in \scrH$ and $y\in \scrK$.}
\end{equation*}
In that case, we call~$T$ \Define{adjointable}.%
\index{adjointable!operator}%
\index{operator!adjointable}
We'll see (in~\sref{uniqueness-adjoint})
that such adjointable~$T$ is adjoint to exactly one~$S$,
which we denote by~\Define{$T^*$}.%
\index{$(\,\cdot\,)^*$!adjoint of an operator}
\end{point}
\begin{point}{90}[hilb-basic-examples]{Example}%
We endow $\C^N$%
\index{C@$\C$, the complex numbers!as a Hilbert space}
(where~$N$ is a natural number)
with the inner product
given by
$\left<x,y\right>=\sum_i \overline{x}_iy_i$,
making it a Hilbert space.

The space~$\Define{c_{00}}$%
\index{c00@$c_{00}$!as a pre-Hilbert space}
of sequences $x_1,x_2,\dotsc$
for which~$x_n$ is non-zero
for finitely many~$n$'s
is an example of a 
pre-Hilbert
which is not complete
when  endowed with $\left<x,y\right>=\sum_{n=0}^\infty \overline{x}_ny_n$
as inner product.

For an example
of an infinite-dimensional Hilbert
space,
we'll have to wait until~\sref{hilb-sum}
where 
we'll show
that the sequences $x_1,x_2,\dotsc$
with $\sum_n \left|x_n\right|^2<\infty$
form a Hilbert space~$\Define{\ell^2}$%
\index{l2@$\ell^2$!as a Hilbert space}
with $\left<x,y\right>=\sum_{n=0}^\infty \overline{x}_ny_n$
as its inner product,
because at this point it is not
even clear that this sum converges.
\end{point}
\begin{point}{100}[uniqueness-adjoint]{Exercise}%
Let~$x$ and~$x'$ be elements of a pre-Hilbert space~$\scrH$
with $\left<y,x\right>=\left<y,x'\right>$
for all~$y\in\scrH$.
Show that~$x=x'$ (by taking $y=x-x'$).
Conclude that every operator between pre-Hilbert spaces
has at most one adjoint.
\begin{point}{110}{Remark}%
Note that we did not require that
an adjointable operator $T\colon \scrH\to\scrK$
between pre-Hilbert spaces be bounded,
and in fact, it might not be.
Take for example
the operator
$T\colon c_{00}\to c_{00}$
given by~$(T x)_n = nx_n$,
which is adjoint to itself,
and not bounded.
On the other hand,
if either~$\scrH$ or~$\scrK$ is complete,
then both~$T$ and~$T^*$ are automatically bounded
as we'll see in~\sref{hellinger-toeplitz}.
\end{point}
\end{point}
\begin{point}{120}{Exercise}%
Let~$S$ and~$T$ be adjointable operators on a pre-Hilbert space.
\begin{enumerate}
\item
Show that~$T^*$ is adjoint to~$T$ (and so $T^{**}=T$).
\item
Show that~$(T+S)^*=T^*+S^*$
and $(\lambda S)^*=\overline{\lambda}S^*$
for every~$\lambda\in \C$.
\item
Show that~$ST$ is adjoint to $T^*S^*$ (and so $(ST)^*=T^*S^*$).
\end{enumerate}
We will, of course, show
that every bounded operator on a Hilbert space is adjointable,
see~\sref{bounded-operator-adjoinable}.
But let us first show that~$\|\,\cdot\,\|$
defined in~\sref{hilb-def} is a norm,
which boils down to the following fact
about  $2\times 2$-matrices.
\end{point}
\begin{point}{130}[positive-2x2matrix]{Lemma}%
For a positive matrix $A\equiv 
\left(\begin{smallmatrix}p & \overline{c} \\ c & q\end{smallmatrix}\right)$
(i.e.~$\left(
\begin{smallmatrix}\overline{u}&\overline{v}\end{smallmatrix}\right)
A
\left(\begin{smallmatrix}u \\ v \end{smallmatrix}\right) \,\geq \, 0$
for all~$u,v\in \C$),
we have
$p,q\geq 0$, and $\left|c\right|^2 \leq pq$.
\begin{point}{140}{Proof}%
Let~$u,v\in\C$ be given.
We have
\begin{equation*}
0\ \leq\ 
\left(\begin{smallmatrix}\overline{u}&\overline{v}\end{smallmatrix}\right)
A
\left(\begin{smallmatrix}u \\ v \end{smallmatrix}\right)
\ = \ 
\left|u\right|^2 p\,+\, 
\overline{u}v\,\overline{c} \,+\,
u\overline{v}\,c \,+\,
\left|v\right|^2 q.
\end{equation*}
By taking~$u=1$ and $v=0$, we see that~$p\geq 0$,
and similarly $q\geq 0$.

The trick to see that~$\left|c\right|^2\leq pq$
is to
take~$v=1$ and $u=t\overline{c}$ with~$t\in \R$:
\begin{equation*}
0 \ \leq\ p\left|c\right|^2t^2
\,+\,2\left|c\right|^2t 
\,+\, q.
\end{equation*}
If~$p=0$, then~$-2\left|c\right|^2t \leq q $
for all~$t\in \R$,
which implies that~$\left|c\right|^2=0=pq$.

Suppose that~$p>0$.
Then taking~$t=-p^{-1}$ we see that
\begin{equation*}
0 \ \leq\ \left|c\right|^2p^{-1}
\,-\,2\left|c\right|^2p^{-1} 
\,+\, q \ = \ -\left|c\right|^2p^{-1}\,+\,q.
\end{equation*}
Rewriting gives us
 $\left|c\right|^2\leq pq$.\qed
\end{point}
\end{point}
\begin{point}{150}[inner-product-basic]{Exercise}%
Let~$\left<\,\cdot\,,\,\cdot\,\right>$
be an inner product on a vector space~$V$.
Show that
the formula~$\Define{\|x\|}=\smash{\sqrt{\left<x,x\right>}}$%
\index{$"\"|\,\cdot\,"\"|$, norm!on a pre-Hilbert space}
defines a seminorm on~$V$,
that is,
$\|x\|\geq 0$,
$\|\lambda x\|=\left|\lambda\right|\|x\|$,
and---the \Define{triangle inequality}---$\|x+y\|\leq \|x\|+\|y\|$
for all~$\lambda\in \C$ and~$x,y\in V$.

Moreover, prove that~$\|\,\cdot\,\|$
is a norm when~$\left<\,\cdot\,,\,\cdot\,\right>$
is definite;
and for~$x,y\in V$:
\begin{enumerate}
\item
The \Define{Cauchy--Schwarz inequality}:%
\index{Cauchy--Schwarz inequality!for $\C$-valued inner products}
$\left|\left<x,y\right>\right|^2\,\leq\, \left<x,x\right>
\,\left<y,y\right>$;
\item
\Define{Pythagoras' theorem}:%
\index{Pythagoras' theorem}
$\|x\|^2+\|y\|^2\,=\,\|x+y\|^2$ when~$\left<x,y\right>=0$;
\item
The \Define{parallelogram law}:%
\index{parallelogram law}
$\|x\|^2\,+\,
\|y\|^2
\,= \,
\frac{1}{2}(\,\|x+y\|^2\,+\,\|x-y\|^2\,)$;
\item
The \Define{polarization identity}:%
\index{polarization identity!for an inner product}
$\left<x,y\right> \,=\, \frac{1}{4}\sum_{n=0}^3i^n\|i^nx+y\|^2$.
\end{enumerate}

(Hint: prove the Cauchy--Schwarz inequality
before the triangle inequality
by applying~\sref{positive-2x2matrix} to the matrix
$\smash{\bigl(\begin{smallmatrix}
\smash{\left<x,x\right>} & \smash{\left<x,y\right>} \\
\smash{\left<y,x\right>} & \smash{\left<y,y\right>}
\end{smallmatrix}\bigr)}$.
Then prove $\|x+y\|^2\leq (\|x\|+\|y\|)^2$
using the inequalities~$\left<x,y\right>+\left<y,x\right>
\leq 2\left|\left<x,y\right>\right| \leq 2\|x\|\|y\|$.)
\end{point}
\begin{point}{160}[operators-cstar-identity]{Lemma}%
For an adjointable operator~$T$ on a pre-Hilbert space~$\scrH$
\begin{equation*}
\|T^*T\|\ =\ \|T\|^2\qquad\text{and}\qquad\|T^*\|\ =\ \|T\|.
\end{equation*}
\begin{point}{170}{Proof}%
If~$T=0$, then~$T^*=0$, and the statements are surely true.

Suppose~$T\neq 0$ (and so~$T^*\neq 0$).
Since $\|Tx\|^2=\left<Tx,Tx\right>=\left<x,T^*Tx\right>
\leq \|x\|\,\|T^*Tx\|\leq \|x\|^2\|T^*T\|$
for every~$x\in \scrH$
by Cauchy--Schwarz,
we have $\|T\|^2\leq \|T^*T\|$.
Since~$\|T^*T\|\leq \|T^*\|\|T\|$
and $\|T\|\neq 0$,
it follows that~$\|T\|\leq \|T^*\|$.
Since by a similar reasoning $\|T^*\|\leq \|T\|$,
we get~$\|T\|=\|T^*\|$.
But then $\|T\|^2\leq \|T^*T\|\leq \|T^*\|\|T\|=\|T\|^2$,
and so $\|T\|^2=\|T^*T\|$.\qed
\end{point}
\end{point}
\begin{point}{180}{Exercise}%
Given a Hilbert space~$\scrH$
show that the adjointable operators
form a closed subspace of~$\scrB(\scrH)$.
\end{point}
\begin{point}{190}[ketbra]{Exercise}%
Let~$x$ and~$y$ be vectors from a Hilbert space~$\scrH$.
\begin{enumerate}
\item
Show that $\Define{\ketbra{x}{y}}\colon\, z\mapsto\left<y,z\right>x$
\index{*ketbra@$\ketbra{x}{y}$, with $x,y\in\scrH$}
defines a bounded operator
$\scrH\to\scrH$,
and, moreover, that~$\|\,\ketbra{x}{y}\,\|=\|x\|\|y\|$.
\item
Show that~$\ketbra{x}{y}$
is adjointable,
and~$(\ketbra{x}{y})^*=\ketbra{y}{x}$.
\end{enumerate}
\end{point}
\end{parsec}
\begin{parsec}{50}[hilb-adjoint]%
\begin{point}{10}[adjoinables-cstar-algebra]%
At this point
it is clear that the vector space of adjointable operators
on a Hilbert space forms a $C^*$-algebra.
So to prove that $\scrB(\scrH)$
is a $C^*$-algebra,
it remains to be shown that every bounded operator
is adjointable (which we'll do in~\sref{bounded-operator-adjoinable}).
We first show that each bounded functional $f\colon \scrH\to \C$
has an adjoint, see~\sref{riesz-representation-theorem},
for which we need the (existence and) properties of ``projections''
on (closed) linear subspaces:
\end{point}
\begin{point}{20}[projection-on-closed-linear-subspace]{Definition}
Let~$x$ be an element of a pre-Hilbert space~$\scrH$.
We say that an element~$y$ of a linear subspace~$C$
of~$\scrH$ is a \Define{projection of~$x$ on~$C$}%
\index{projection!of~$x$ on~$C$}
if
\begin{equation*}
\|x-y\|\,=\,\min\{\,\|x-y'\|\colon \,y'\in C\,\}.
\end{equation*}
(In other words,~$y$ is one of the elements of~$C$ closest to~$x$.)
\end{point}
\begin{point}{30}{Exercise}%
We'll see in~\sref{projection-theorem}
that on a \emph{closed}
linear subspace
every vector has a projection.
For arbitrary linear subspaces this
isn't so:
show that the only vectors in~$\ell_2$
having 
a projection on the linear subspace~$c_{00}$
(from \sref{hilb-basic-examples})
are the vectors in~$c_{00}$ themselves.
\end{point}
\begin{point}{40}{Lemma}%
Let~$\scrH$ be a pre-Hilbert space,
and let $x,e\in\scrH$ with
$\|e\|=1$.

Then~$y=\left<e,x\right>e$ is the unique projection of~$x$ on~$e\C$.
\begin{point}{50}{Proof}%
Let~$y'\in e\C$
with~$y'\neq y$
be given.
To prove that~$y$
is the unique projection of~$x$ on $e\C$
it suffices to show that $\|x-y\|<\|x-y'\|$.
Since~$y'\neq y\equiv \left<e,x\right>e$,
there is~$\lambda\in \C$, $\lambda\neq 0$ 
with $y'=(\lambda+\left<e,x\right>)e$.

Note that $\left<e,y\right>=\left<e,\left<e,x\right>e\right>=
\left<e,x\right>\left<e,e\right>
= \left<e,x\right>$,
and so~$\left<e,x-y\right>=0$.
Then~$y'-y\equiv \lambda e$ and~$x-y$ are orthogonal too,
and thus, by Pythagoras'~theorem (see~\sref{inner-product-basic}),
we have $\|y'-x\|^2
=\|y'-y\|^2+\|y-x\|^2\equiv \left|\lambda\right|^2+\|x-y\|^2
>\|x-y\|^2$, because~$\lambda\neq 0$.
Hence~$\|y'-x\|>\|y-x\|$.\qed
\end{point}
\end{point}
\begin{point}{60}[hilb-projection-basic]{Exercise}%
Let~$y$ be a projection of an element~$x$ of a pre-Hilbert space~$\scrH$
on a linear subspace~$C$.
Show that~$y$ is a projection of~$x$ on $y\C$.
Conclude that~$y$ is the unique projection of~$x$ on~$C$,
and that~$\left<y,x-y\right>=0$.
Show that~$y+c$ is the projection of~$x+c$ on~$C$
for every~$c\in C$.
Conclude that~$\left<y',x-y\right>\equiv\left<y',(x+y'-y)-y'\right>=0$ 
for every~$y'\in C$.
\end{point}
\begin{point}{70}[projection-theorem]{Projection Theorem}%
\index{Projection Theorem}%
Let~$C$ be a closed linear subspace
of a Hilbert space~$\scrH$.
Each~$x\in \scrH$
has a unique projection~$y$ on~$C$,
and $\left<y',y\right>=\left<y',x\right>$ for~$y'\in C$.
\begin{point}{80}{Proof}%
We only need to show that there is a projection~$y$
of~$x$ on~$C$,
because~\sref{hilb-projection-basic}
gives us that such~$y$ is unique and satisfies
$\left<y',y\right> = \left<y',x\right>$ for all~$y'\in C$.

Write~$r:=\inf\{\,\|x-y'\|\colon\, y'\in C\,\}$,
and pick a sequence $y_1,y_2,\dotsc \in C$
such that $\|x-y_n\|\rightarrow r$.
We will show that~$y_1,y_2,\dotsc$ is Cauchy.
Let~$\varepsilon >0$
be given,
and pick~$N$ such that $\|y_n-x\|^2\leq r^2+\frac{1}{4}\varepsilon$
for all~$n\geq N$.
Let~$n,m\geq N$ be given.
Then since $\frac{1}{2}(y_n+y_m)$
is in~$C$, we have
$\|y_n+y_m-2x\|\equiv 
2\|\frac{1}{2}(y_n+y_m)-x\|\geq 2r$,
and so by the parallelogram law (see \sref{inner-product-basic}),
\begin{alignat*}{3}
\|y_n-y_m\|^2
\ &\equiv\ 
\|(y_n-x)-(y_m-x)\|^2\\
\ &=\ 
2\|y_n-x\|^2 + 2\|y_m-x\|^2
- \|y_n+y_m-2x\|^2\\
\ &\leq\ 
4r^2 + \varepsilon - 4r^2 \ \leq \ \varepsilon.
\end{alignat*}
Hence~$y_1,y_2,\dotsc$ is Cauchy,
and converges to some~$y\in C$,
because~$\scrH$ is complete and~$C$ is closed.
It follows easily that~$\|x-y\|=r$,
and thus~$y$ is the projection of~$x$ on~$C$.\qed
\end{point}
\end{point}
\begin{point}{90}[riesz-representation-theorem]{Riesz'~Representation Theorem}%
\index{Riesz' Representation Theorem}%
Let~$\scrH$ be a Hilbert space.
For every bounded linear map~$f\colon \scrH\to\C$
there is a unique vector~$x\in \scrH$
with $\left<x,\,\cdot\,\right>=f$.
\begin{point}{100}{Proof}%
If~$f=0$, then $x=0$ does the job.
Suppose that~$f\neq 0$.
There is an~$x'\in\scrH$ with~$f(x')=1$.
Note that~$\ker(f)$ is closed, because~$f$
is bounded.
So by~\sref{projection-theorem},
we know that~$x'$
has a projection~$y$ on~$\ker(f)$,
and $\left<x',z\right>=\left<y,z\right>$
for all~$z\in \ker(f)$.
Then for~$x'':=x'-y$,
we have $f(x'')=1$ and~$\left<x'',y'\right>=0$
for all~$y'\in \ker(f)$.
Given $z\in \scrH$,
we have $f(\,z-f(z)x''\,)=0$,
so~$z-f(z)x''\in \ker(f)$,
and thus~$0=\left<x'',z-f(z)x''\right>\equiv \left<x'',z\right>-f(z)\|x''\|^2$.
Hence writing $x:=x''\|x''\|^{-2}$
we have~$f(z)=\left<x,z\right>$
for all~$z\in \scrH$.

Finally, uniqueness of~$x$ follows from~\sref{uniqueness-adjoint}.\qed
\end{point}
\end{point}
\begin{point}{110}[bounded-operator-adjoinable]{Exercise}%
Prove that every bounded operator~$T$ on a Hilbert space~$\scrH$
is adjointable, as follows.
Let~$x\in \scrH$ be given.
Prove that~$\left<x,T(\,\cdot\,)\right>\colon \scrH\to \C$
is a bounded linear map.
Let~$Sx$ be the unique vector with $\left<Sx,\,\cdot\,\right>
=\left<x,T(\,\cdot\,)\right>$,
which exists by~\sref{riesz-representation-theorem}.
Show that~$x\mapsto Sx$
gives a bounded linear map $S$, which is adjoint to~$T$.
\end{point}
\begin{point}{120}%
Thus the bounded operators
on a Hilbert space~$\scrH$
form a $C^*$-algebra~$\scrB(\scrH)$%
\index{BH@$\scrB(\scrH)$!as a $C^*$-algebra}
as described in~\sref{example-hilb}.
We will return to Hilbert spaces
in~\sref{gelfand-naimark},
where we show that every $C^*$-algebra
is isomorphic to a $C^*$-subalgebra of
a $\scrB(\scrH)$.
\end{point}
\end{parsec}
\begin{parsec}{60}%
\begin{point}{10}%
Here is a non-trivial
example of a Hilbert space
that will be used later on.
\end{point}
\begin{point}{20}[hilb-sum]{Proposition}%
Given a family  $(\scrH_i)_{i\in I}$ 
of Hilbert spaces,
the vector space
\begin{equation*}
	\textstyle
	\Define{\bigoplus_i \scrH_i} \ =\ \{\ 
		x\in \prod_i \scrH_i\colon\ 
	\sum_i \|x_i\|^2 <\infty \ \}.
\end{equation*}%
\index{direct sum!of Hilbert spaces}%
\index{$\bigoplus$, direct sum!$\bigoplus_i\scrH_i$, of Hilbert spaces}
is a Hilbert space
when endowed with the inner product
$\left<x,y\right>=\sum_i \left<x_i,y_i\right>$.
\begin{point}{30}{Proof}%
To begin with
we must show that~$\sum_i \left<x_i,y_i\right>$
converges for~$x,y\in\bigoplus_i \scrH_i$.
Given~$\varepsilon>0$
we must
find a finite subset~$G$ of~$I$
such that~$ \left|\sum_{i \in F} \left<x_i,y_i\right>\right|
\leq \varepsilon$ for all finite $F\subseteq I\backslash G$.
Since an obvious application
of the Cauchy--Schwarz inequality
gives
us that for every finite subset~$F$ of~$I$
\begin{equation*}
	\Bigl|\sum_{i\in F}
	\left<x_i,y_i\right>\Bigr|^2
	\ \leq\ 
	\sum_{i\in F}\|x_i\|^2
	\, \sum_{i\in F}\|y_i\|^2,
\end{equation*}
any~$G\subseteq I$
with $\sum_{i\in I\backslash G} \|x_i\|^2 \leq \sqrt{\varepsilon}$
	and~$\sum_{i\in I\backslash G} \|y_i\|^2 \leq \sqrt{\varepsilon}$
will do.

It is easily seen that
$\left<x,y\right>:=\sum_i \left<x_i,y_i\right>$
gives a definite inner product on~$\bigoplus_i \scrH_i$; the
remaining difficulty
lies in showing that the resulting norm is complete.
To this end, let $x_1,x_2,\dotsc$ be a Cauchy sequence 
in~$\bigoplus_{i\in I}\scrH_i$;
we must show that it converges to some~$x_\infty\in \bigoplus_i \scrH_i$.
We do the obvious thing:
since for every~$i\in I$
the sequence $(x_1)_i, (x_2)_i,\dotsc$
is Cauchy in~$\scrH_i$
we may define $(x_\infty)_i:=\lim_n (x_n)_i$,
and thereby get an element $x_\infty$ of~$\prod_i \scrH_i$.
Since for each finite subset~$F$ of~$I$
we have $\sum_{i\in F} \|(x_\infty)_i\|^2
=\lim_n \sum_{i\in F} \|(x_n)_i\|^2
\leq \lim_n \|x_n\|^2$,
we have $\sum_{i\in I} \|(x_\infty)_i\|^2 
\leq \lim_n \|x_n\|^2 <\infty$,
and so~$x_\infty\in\bigoplus_i \scrH_i$.

It remains to be shown that~$x_1,x_2,\dotsc$
converges to~$x_\infty$
(not only coordinatewise but also)
with respect to the inner product on~$\bigoplus_i \scrH_i$.
Given~$\varepsilon >0$
pick $N$ such that $\|x_n - x_m\|\leq \frac{1}{2\sqrt{2}}\varepsilon$
for all~$n,m\geq N$.
We claim that for such~$n$
we have $\|x_\infty -x\|\leq \varepsilon$.
Indeed, first note that since the sum
\begin{equation*}
	\sum_{i\in I} \|(x_\infty)_i - (x_n)_i \|^2
\ \equiv\ 
\sum_{i\in F} \|(x_\infty)_i - (x_n)_i \|^2
\ +\ 
\sum_{i\in I\backslash F} \|(x_\infty)_i - (x_n)_i \|^2
\end{equation*}
converges (to $\|x_\infty - x_n\|^2$),
we can find
a finite subset~$F$ (depending on~$n$)
such that second term in the right-hand side above
is smaller than~$\frac{1}{2}\varepsilon^2$.
To see that the first term is also below~$\frac{1}{2}\varepsilon^2$,
begin by noting that for every~$m$,
\begin{equation*}
\Bigl(\,\sum_{i\in F} \|(x_\infty)_i - (x_n)_i \|^2\,\Bigr)^{\nicefrac{1}{2}}
\ \leq \ 
\Bigl(\,\sum_{i\in F} \|(x_\infty)_i - (x_m)_i \|^2\,\Bigr)^{\nicefrac{1}{2}}
\ +\ 
\Bigl(\,\sum_{i\in F} \|(x_m)_i - (x_n)_i \|^2\,\Bigr)^{\nicefrac{1}{2}}.
\end{equation*}
Since~$F$ is finite,
and~$(x_m)$ converges
to~$x_\infty$ coordinatewise
we can find an~$m$ large enough
that the first term on the right-hand side above
is below~$\smash{\frac{1}{2\sqrt{2}}\varepsilon}$.
If we choose~$m\geq N$
we see that the second term is below $\smash{\frac{1}{2\sqrt{2}}\varepsilon}$
as well,
and we conclude that~$\|x_\infty-x_n\|\leq \varepsilon$.\qed
\end{point}
\end{point}

\end{parsec}
\section{The Basics}

\begin{parsec}{70}%
\begin{point}{10}%
Now that we have seen the most important examples
of $C^*$-algebras,
we can begin developing the theory.
We'll start easy with the self-adjoint elements:
\end{point}
\begin{point}{20}{Definition}%
Given an element $a$ of a $C^*$-algebra $\scrA$, 
\begin{enumerate}
\item we say that $a$ is \Define{self-adjoint}%
\index{self adjoint} if $a^* =a$, and
\item we write $\Define{\Real{a}}:= \frac{1}{2}(a+a^*)$
and $\Define{\Imag{a}}:=\frac{1}{2i}(a-a^*)$
for the \Define{real} and \Define{imaginary part}%
\index{real part!of an element of a $C^*$-algebra}%
\index{$\Real{(\,\cdot\,)}$, real part!operation on a $C^*$-algebra}%
\index{imaginary part!of an element of a $C^*$-algebra}%
\index{$\Imag{(\,\cdot\,)}$, imaginary part!operation on a $C^*$-algebra}
of~$a$, respectively.
\end{enumerate}
The set of self-adjoint elements of~$\scrA$
is denoted by~\Define{$\sa{\scrA}$}.%
\index{$\Real{(\,\cdot\,)}$, real part!$\Real{\scrA}$, of a $C^*$-algebra}
\end{point}
\begin{point}{30}[cstar-involution-basic]{Exercise}%
Let~$a$ be an element of a $C^*$-algebra~$\scrA$.
\begin{enumerate}
\item 
Show that $\Real{a}$ and $\Imag{a}$ are self-adjoint,
and  $a= \Real{a}+i\Imag{a}$.
\item
Show that if $a\equiv b+ic$ for self-adjoint elements $b$, $c$ of~$\scrA$,
then $b=\Real{a}$ and~$c=\Imag{a}$.
\item
Show that $\Real{(a^*)}=\Real{a}$ and $\Imag{(a^*)}=-\Imag{a}$.
\item 
Show that~$a$ is self-adjoint iff $\Real{a}=a$ iff $\Imag{a}=0$.
\item
Show that $a\mapsto \Real{a}$ and $a\mapsto \Imag{a}$
give $\R$-linear maps $\scrA\to\scrA$.
\item
Show that $\Imag{a} = -\Real{(ia)}$ and $\Real{a}=\Imag{(ia)}$.
\item
Show that $a^*a$ is self-adjoint,
and  $a^*a=\Real{a}^2+\Imag{a}^2+i(\Real{a}\Imag{a}-\Imag{a}\Real{a})$.
\item
Give an example of~$\scrA$ and~$a$ 
with  $\Real{a}\Imag{a} \neq \Imag{a}\Real{a}$.

(This inequality is a source of many technical difficulties.)
\item
Show that $a^*a+aa^* = 2(\Real{a}^2+\Imag{a}^2)$.
\item
The product of self-adjoint elements $b$, $c$ need not be self-adjoint;
show that, in fact, $bc$ is self-adjoint iff $bc=cb$.
\item
Show that $\|a^*\| = \|a\|$. (Hint:  $\|a\|^2=\|a^*a\|\leq \|a^*\|\|a\|$.)

\item
Show that $\|\Real{a}\|\leq \|a\|$ and $\|\Imag{a}\|\leq \|a\|$.
\item
Show that $\|a^2\|=\|a\|^2$ when~$a$ is self-adjoint.

However,
show that $\|a^2\|\neq \|a\|^2$ might occur
when~$a$ is not self-adjoint.
(Hint: $\bigl(
\begin{smallmatrix}
	0&1\\
	0&0
\end{smallmatrix}
\bigr)$.)

\end{enumerate}
\end{point}
\end{parsec}
\begin{parsec}{80}%
\begin{point}{10}{Notation}%
Recall that (in this text) every $C^*$-algebra~$\scrA$ has a unit, $1$.
Thus, for every scalar $\lambda\in \C$,
we have an element $\lambda\cdot 1$ of~$\scrA$,
which we will simply denote by~$\lambda$.
This should hardly cause any confusion,
for while an expression of an element of~$\scrA$
such as $i+2+5a$ (where $a\in \scrA$) 
may be interpreted in several ways,
the result is always the same.
\end{point}
\begin{point}{20}{Exercise}%
There is a subtle point regarding
the norm~$\|\lambda\|$ of a
scalar~$\lambda\in \C$ inside a $C^*$-algebra~$\scrA$:
	we do not always have~$\|\lambda\|=\left|\lambda\right|$
	on the nose.
\begin{enumerate}
\item
Indeed, show 
that $\|1\|=0\neq 1$ when~$\scrA=\{0\}$ is the trivial $C^*$-algebra.
\item 
Show that $\|\lambda\|\leq \left| \lambda\right|$ (in~$\C$).
\item
Show that~$\|\lambda\|=\left|\lambda\right|$
when~$\|\lambda\|$ and~$\left|\lambda\right|$
are interpreted as elements of~$\scrA$.
\end{enumerate}
\end{point}
\end{parsec}
\begin{parsec}{90}%
\begin{point}{10}%
Let us now generalize the notion of a positive function
in~$C(X)$
to a positive element of a $C^*$-algebra.
There are several descriptions of
positive functions in~$C(X)$ in terms of the $C^*$-algebra structure
(see~\sref{cx-positive}) on which we can base such a  generalization,
and while we will eventually see that these all yield the same notion
of positive element of a $C^*$-algebra (see~\sref{cstar-positive-final})
we base our definition of positive element (\sref{cstar-positive-def})
on the description that is perhaps
not most familiar,
but does give us the richest structure at this stage.
\end{point}
\begin{point}{20}[cx-positive]{Exercise}%
Let~$X$ be a compact Hausdorff space.
Show that for self-adjoint $f\in C(X)$, the following are equivalent.
\begin{enumerate}
\item \label{cx-positive-1}
$f(X)\subseteq [0,\infty)$;
\item
$f\equiv g^2$ for some $g\in \sa{C(X)}$;
\item
$f\equiv g^* g$ for some~$g\in C(X)$;
\item
$\|f-t\|\leq t$ for some $t\in \R$;
\item
$\|f-t\|\leq t$ for all $t\geq \frac{1}{2}\|f\|$.
\end{enumerate}
(Hint: $\|f-t\|\leq t$ iff $-t\leq f-t\leq t$
iff $0\leq f\leq 2t$.)
\begin{point}{30}{Exercise}%
To see how condition~\ref{cx-positive-1}
can be expressed in terms of the $C^*$-algebra structure of~$C(X)$,
prove that  $\lambda\in f(X)$ iff $f-\lambda$
is not invertible.
\end{point}
\end{point}
\begin{point}{40}[cstar-positive-def]{Definition}%
A self-adjoint element~$a$ of a $C^*$-algebra~$\scrA$ is called
\Define{positive}%
\index{positive!element of a $C^*$-algebra}
if $\|a-t\|\leq t$
for some~$t\in \R$.
We write $\Define{a\leq b}$%
\index{*leq@$\leq$, order!on a $C^*$-algebra}
for $a,b\in\scrA$ when $b-a$ is positive,
and we denote the set of positive elements of~$\scrA$
by~$\Define{\pos{\scrA}}$.%
\index{$(\,\cdot\,)_+$, positive part!$\scrA_+$,
of a $C^*$-algebra}
\begin{point}{41}
Given elements $a$ and~$b$ of a $C^*$-algebra~$\scrA$
we denote by $\Define{[a,b]_\scrA}$,
or sometimes simply $\Define{[a,b]}$,%
\index{$[a,b]$, interval!$[a,b]_\scrA$, in a $C^*$-algebra}
the set of elements~$c$ of~$\scrA$
with $a\leq c\leq b$.
\end{point}
\begin{point}{50}{Remark}%
One advantage of this
definition
over, say, taking the elements of the form~$a^*a$
to be positive,
is that it is immediately clear
that an element~$b$ of a $C^*$-subalgebra~$\scrB$
of a $C^*$-algebra~$\scrA$ is positive in~$\scrB$
iff~$b$ is positive in~$\scrA$---that is,
`positive permanence' comes for free (cf.~\sref{spectral-permanence}).
Another advantage is
that it's also pretty easy to see 
that the sum of such positive elements
is again positive, see~\sref{cstar-positive-sum}.
\end{point}
\begin{point}{51}{Remark}%
Note that when an element~$a$
of a $C^*$-algebra
is positive on the grounds that $\|a-t\|\leq t$
for some~$t\in \R$,
then this number~$t$ must be positive,
and we even have $t\geq \frac{1}{2}\|a\|$,
since $\|a\|-\|t\|\leq \|a-t\|\leq t$.
There's nothing special about this~$t$:
we'll see in~\sref{cstar-positive-1}
that $\|a-s\|\leq s$
    for all~$s\geq \frac{1}{2} \|a\|$ and positive~$a$.
\end{point}
\end{point}
\begin{point}{60}{Example}%
We'll see in~\sref{hilb-positive-operators},
that a bounded operator~$T$ on a Hilbert space~$\scrH$
is positive iff~$\left<x,Tx\right>\geq 0$ for all~$x\in\scrH$.
\end{point}
\begin{point}{70}[cstar-positive-sum]{Lemma}%
Let~$a,b$ be positive elements of a $C^*$-algebra.
Then $a+b$ is positive.
\begin{point}{80}{Proof}
Since~$a\geq 0$,
there is~$t\in \R$
    with $\|a-t\|\leq t$.
Similarly, there is~$s\in \R$
with $\|b-s\|\leq s$.
Then $\|a+b-(t+s)\|\leq \|a-t\|+\|b-s\|\leq t+s$.\qed
\end{point}
\end{point}
\begin{point}{90}{Exercise}%
Given an element~$a$
of a $C^*$-algebra~$\scrA$
with~$0\leq a\leq 1$
(which is called an \Define{effect})%
\index{effect!in a $C^*$-algebra}
show that 
the \Define{orthosupplement} $\Define{a^\perp} :=1-a$%
\index{orthosupplement!operation in a $C^*$-algebra}%
\index{$(\,\cdot\,)^\perp$, orthosupplement}
is an effect too.
\end{point}
\begin{point}{100}[cstar-positive]{Exercise}%
Let~$\scrA$ be a $C^*$-algebra.
\begin{enumerate}
\item
Show that~$\pos{\scrA}$ is a \emph{cone}:
$0\in \pos{\scrA}$,
$a+b\in \pos{\scrA}$ for all $a,b\in\pos{\scrA}$,
and
$\lambda a\in \pos{\scrA}$  
for all $a\in \pos{\scrA}$ and $\lambda\in [0,\infty)$.
Conclude that~$\leq$ is a preorder.
\item
Show that~$1$ is positive, and  $-\|a\|\leq a \leq \|a\|$
for every self-adjoint element~$a$ of~$\scrA$.
(Thus $1$ is an \emph{order unit} of~$\sa{\scrA}$.)
\item
The behavior of positive elements may be surprising:
give an example of positive elements $a$ and~$b$
from a $C^*$-algebra
such that $ab$ is not positive.
\item
Given a self-adjoint element~$a$ of~$\scrA$ define
\begin{equation*}
\|a\|_o \ = \ \inf\{\ \lambda\in[0,\infty)\colon \ 
-\lambda\leq a\leq \lambda\ \}.
\end{equation*}
Show that $\|-\|_o$ is a seminorm on~$\sa{\scrA}$,
and that~$\|a\|_o\leq \|a\|$
for all~$a\in\sa{\scrA}$.

Prove that $0\leq a\leq b$ implies that~$\|a\|_o\leq\|b\|_o$
for $a,b\in\sa{\scrA}$.

\item
There is not much more that can easily be
proven about positive elements, at this point,
but don't take my word for it:
try to prove the following facts
about a self-adjoint element~$a$ of~$\scrA$ directly.
\begin{enumerate}
\item $a^2$ is positive;
\item if $a$ is the limit of positive $a_n\in\scrA$,
then $a$ is positive;
\item if $a\geq -\frac{1}{n}$ for all~$n\in \N$, then $a\geq 0$;
\item  $\|a\|=\|a\|_o$;
\item $a=0$ when~$0\leq a\leq 0$.
\end{enumerate}
We will prove these facts
when we return to the positive elements in~\sref{cstar-positive-2}.
\end{enumerate}
\end{point}
\end{parsec}

\begin{parsec}{100}%
\begin{point}{10}%
Let us spend some words
on the morphisms between $C^*$-algebras.
\end{point}
\begin{point}{20}[maps]{Definition}
A linear map $f\colon \scrA \to \scrB$
between $C^*$-algebras
is called
\begin{enumerate}
\item
\Define{\textbf{m}ultiplicative}%
\index{multiplicative!map between $C^*$-algebras}
if $f(ab)=f(a)f(b)$ for all $a,b\in\scrA$;
\item
\Define{\textbf{i}nvolution preserving}%
\index{involution preserving!map between $C^*$-algebras}
if $f(a^*)=f(a)^*$ for all~$a\in\scrA$;
\item
\Define{\textbf{u}nital}%
\index{unital!map between $C^*$-algebras}
if $f(1)=1$;
\item
\Define{\textbf{s}ub\textbf{u}nital}%
\index{subunital map between $C^*$-algebras}
if $f(1)\leq 1$;
\item
\Define{\textbf{p}ositive}%
\index{positive!map between $C^*$-algebras}
if $f(a)$ is positive
for every positive $a\in\scrA$, and
\item
\Define{\textbf{c}ompletely \textbf{p}ositive}%
\index{completely positive!map between $C^*$-algebras}%
\index{positive!completely~$\sim$ map between $C^*$-algebras}
if $\sum_{i,j} b_i^*\,f(\,a_i^*a_j\,)\,b_j$ is positive
for all~$a_1,\dotsc,a_n\in \scrA$, and $b_1,\dotsc,b_n\in\scrB$
(see Remark~5.1 of~\cite{paschke}).
\end{enumerate}
\begin{point}{30}%
We use the bold letters as abbreviations,
so for instance,
$f$ is \Define{pu}%
\index{pu-map} if it is positive and unital,
and a \Define{miu-map}%
\index{miu-map}
is a multiplicative, involution preserving,
unital linear map between $C^*$-algebras
(which is usually called a \Define{unital $*$-homomorphism}%
\index{homomorphism@$*$-homomorphism}).

We'll denote the category of $C^*$-algebras
and miu-maps by~$\Define{\Cstar{miu}}$,%
\index{Cstar@$\Cstar{}$: $\Cstar{miu}$, $\Cstar{cpu}$, \dots}
and
the subcategory of commutative $C^*$-algebras
by~$\Define{\cCstar{miu}}$.%
\index{cCstar@$\cCstar{}$: $\cCstar{miu}$, $\cCstar{pu}$,\dots}
We'll use similar notation
for the other classes of maps,
but
will,
naturally, only mention $\Cstar{cpu}$
after having established that cp-maps are closed under composition.

The advantages of completely positive maps
become apparent
only later on 
when we start dealing with matrices (see~\sref{n-pos})
and the tensor product (see~\sref{tensor-functorial}).
\end{point}
\end{point}
\begin{point}{40}[cstar-p-implies-i]{Lemma (``p$\Rightarrow$i'')}
A positive map $f\colon \scrA\to\scrB$ between
$C^*$-algebras is involution preserving.
\begin{point}{50}{Proof}%
Let~$a\in \scrA$ be given. We must show that~$f(a^*)=f(a)^*$.

But first we'll show that if~$a$ is self adjoint,
then so is~$f(a)$.
Indeed, since $\|a\|$ and $\|a\|-a$ are positive (see~\sref{cstar-positive}),
we see that $f(\|a\|)$ and $f(\|a\|-a)$ are positive,
and so~$f(a)=f(\|a\|)-f(\|a\|-a)$ being positive is self adjoint.

It follows that $\Real{f(a)}=f(\Real{a})$
and $\Imag{f(a)}=f(\Imag{a})$ (for~$a\in\scrA$),
because $f(a)\equiv f(\Real{a})+if(\Imag{a})$,
and~$f(\Real{a})$ and~$f(\Imag{a})$
are self adjoint
(see~\sref{cstar-involution-basic}).

Hence $f(a^*)\equiv f(\Real{a}-i\Imag{a})
=\Real{f(a)}-i\Imag{f(a)}\equiv f(a)^*$.\qed
\end{point}
\end{point}
\begin{point}{60}{Remark}%
Other important relations between these types of morphisms
can only be established later on
once we have a firmer grasp on the positive elements.
We will then see  
that every mi-map 
is completely positive (in~\sref{cp}),
and that every completely positive map is positive 
(in~\sref{astara-pos-basic-consequences}).
\begin{point}{61}%
Note that we didn't bother to include
an abbreviation for bounded linear maps in our list, \sref{maps}.
That's because we'll see in~\sref{weak-russo-dye} that any positive
map between $C^*$-algebras is automatically bounded.
\end{point}
\end{point}
\begin{point}{70}{[Moved to~\sref{cstar-product-2}.]}%
\end{point}
\begin{point}{80}{[Moved to~\sref{cstar-equaliser-1}.]}%
\begin{point}{90}{[Moved to~\sref{cstar-no-pu-equalisers}.]}
\end{point}
\end{point}
\end{parsec}

%
% geometric series 
%
\begin{parsec}{110}%
\begin{point}{10}%
After having visited the positive elements,
let us explore our second landmark,
the  invertible elements
of a $C^*$-algebra,
whose role 
is as important as it is technical.
This paragraph culminates in what is essentially
 \emph{spectral permanence} (\sref{spectral-permanence}):
the fact that if an element $a$ of a $C^*$-subalgebra $\scrB$
of a $C^*$-algebra~$\scrA$
is invertible in~$\scrA$,
then~$a$ is already invertible in~$\scrB$,
see~\sref{inverse-permanence}.
\end{point}
\begin{point}{20}[geometric]{Lemma}%
\index{geometric series}%
Let~$a$ be an element of a $C^*$-algebra~$\scrA$ with~$\|a\|<1$.
Then~$a^\perp\equiv 1-a$ has an inverse,
namely~$(a^\perp)^{-1}= \sum_{n=0}^\infty\, a^n$.
Moreover, this series converges absolutely,
that is,
$\sum_{n=0}^\infty \|a^n\|<\infty$.
\begin{point}{30}{Proof}%
Note that
$(1-\|a\|)\,(1+\|a\|+\|a\|^2+\dotsb+\|a\|^N) \,=\, 1-\|a\|^{N+1}$,
and so 
\begin{equation*}
\sum_{n=0}^N \|a\|^n \ =\  \frac{1-\|a\|^{N+1}}{1-\|a\|}
\end{equation*}
for every~$N$.
Thus,
since $\|a\|^N$ converges to~$0$
(because\footnote{
In case you've never seen the argument:
the limit $b:=\lim_N \|a\|^N$
exists, because $\|a\|\geq \|a\|^2\geq \dotsb\geq 0$,
and is zero
because $\|a\|b=\lim_N \|a\|^{N+1}=b$
and~$\|a\| < 1$.}
 $\|a\|<1$),
we  get $\sum_{n=0}^\infty \|a\|^n = (1-\|a\|)^{-1}$.
Note that since~$\|a^n\|\leq \|a\|^n$ for every~$n$,
    this entails that $\sum_{n=0}^\infty \|a^n\|\leq (1-\|a\|)^{-1}
    <\infty$.
\begin{point}{40}%
Note that $a^N$ norm converges to~$0$,
because $\|a\|^N$ converges to~$0$.
Also (but slightly less obvious),
$\sum_n a^n$ norm converges,
because~$\sum_n \|a\|^n$ converges.
\end{point}
\begin{point}{50}%
Thus, taking the norm limit
on both sides of $(1-a)(1+a+a^2+\dotsb a^N) = 1-a^{N+1}$,
gives us $(1-a)(\sum_n a^n) = 1$.
Since we can derive $(\sum_n a^n)(1-a) = 1$
in a similar manner, 
we see that $\sum_n a^n$ is the inverse of~$1-a$.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{60}[spectrum-bounded]{Exercise}%
\index{invertible!element of a $C^*$-algebra}%
Let~$a$ be an element of a $C^*$-algebra~$\scrA$.
\begin{enumerate}
\item
Show that $a-\lambda$ is invertible
for every~$\lambda\in\C$ with~$\|a\|< \left|\lambda\right|$.
\item
Show that $a-b$ is invertible
when~$b\in\scrA$ is invertible and $\|a\| < \|b\|$.
\item
Show that $U:=\{\ b\in\scrA\colon\ \text{$b$ is invertible}\ \}$
is an open subset of~$\scrA$.
\end{enumerate}
\end{point}
\begin{point}{70}[geometric-convergence]{Lemma}%
\index{geometric series}%
For a self-adjoint element~$a$ of~$\scrA$
the series $\sum_n a^n$ 
converges iff~$\|a\|<1$;
and in that case converges absolutely.
\begin{point}{80}{Proof}%
We have already seen in~\sref{geometric}
that~$\sum_n a^n$
converges absolutely when~$\|a\|<1$.
Now, if $\sum_n a^n$ converges,
then~$\|a^n\|$
(being the norm of the difference
between consecutive partial sums of~$\sum_n a^n$)
converges to~$0$.
    In particular, $\smash{\|a\|^{2^n}}$
    (being equal to $\|\smash{a^{2^n}}\|$
    by the $C^*$-identity)
converges to~$0$ too,
which only happens when~$\|a\|<1$.\qed
\end{point}
\begin{point}{90}[geometric-non-self-adjoint]{Remark}%
For non-self-adjoint elements~$a$ of~$\scrA$,
the convergence of~$\sum_n a^n$
is a more delicate matter.
Take for example
the matrix $A:=\bigl(\begin{smallmatrix}0&2\\0&0\end{smallmatrix}\bigr)$
for which the series $\sum_n A^n$ converges (to~$1+A$),
while~$\|A\|=2$ --- the problem being that
$\|A^2\|^{\nicefrac{1}{2}}$ differs from $\|A\|$.
In fact,
we'll see from~\sref{hadamard}
(although we won't need it)
that~$\sum_n a^n$
converges absolutely when $1>\limsup_n \|a^n\|^{\nicefrac{1}{n}}$,
and diverges when $1<\limsup_n \|a^n\|^{\nicefrac{1}{n}}$.
This begs the question
what happens when $1=\limsup_n \|a^n\|^{\nicefrac{1}{n}}$
--- which I do not know.
\end{point}
\end{point}
\begin{point}{100}[cstar-inv-continuous]{Lemma}%
Let~$\scrA$ be a $C^*$-algebra.
The assignment $a\mapsto a^{-1}$
gives a  continuous map
(from the set $\{\,b\in \scrA\colon\, \text{$b$ is invertible}\,\}$
to~$\scrA$.)
\begin{point}{110}[cstar-inv-continuous-1]{Proof}
(Based on Proposition 3.1.6 of~\cite{kr}.)

First we establish continuity at~$1$:
let~$a\in\scrA$ with $\|1-a\|\leq \frac{1}{2}$ be given;
we claim that~$a$ is invertible,
and~$\|1-a^{-1}\| \leq 2\|1-a\|$.

Indeed, since~$\|1-a\|\leq \frac{1}{2}<1$,
$a$ is invertible by~\sref{geometric},
and $a^{-1}=\sum_{n=0}^\infty (1-a)^n$.
Then~$\|1-a^{-1}\|=\|\sum_{n=1}^\infty (1-a)^n\|\leq \sum_{n=1}^\infty \|1-a\|^n
= \|1-a\|\, (1-\|1-a\|)^{-1}$.
Thus, as $\|1-a\|\leq\frac{1}{2}$,
we get $(1-\|1-a\|)^{-1}\leq 2$,
and so $\|1-a^{-1}\|\leq 2\|1-a\|$.
\begin{point}{120}%
Let~$a$ be an invertible element of~$\scrA$,
and let~$b\in\scrA$ with~$\|a-b\|\leq\frac{1}{2}\|a^{-1}\|$.
We claim that~$b$ is invertible,
and~$\|a^{-1}-b^{-1}\|\leq 2\|a-b\|\,\|a^{-1}\|^2$.
Since $\|a-b\|\leq \frac{1}{2}\|a^{-1}\|$
we have
$\|1-a^{-1}b\|\leq \|a^{-1}\|\,\|a-b\|\leq \frac{1}{2}$.
By~\sref{cstar-inv-continuous-1}, $a^{-1}b$ is invertible,
and $\|1-(a^{-1}b)^{-1}\|\leq 2\|1-a^{-1}b\|\leq 2\|a-b\|\,\|a^{-1}\|$.
Hence $\|a^{-1}-b^{-1}\| = \|(1-(a^{-1}b)^{-1})a^{-1}\|
\leq \|1-(a^{-1}b)^{-1}\|\,\|a^{-1}\|\leq 2 \|a-b\|\,\|a^{-1}\|^2$.\qed
\end{point}
\end{point}
\end{point}
%
%	Towards spectral permanence
%
\begin{point}{130}{Lemma}%
For a self-adjoint element~$a$ from a $C^*$-algebra,
$a-i$ is invertible.
\begin{point}{140}{Proof}%
(Based on Proposition 4.1.1(ii) of~\cite{kr}.)

The trick
is to 
write~$a-i\equiv (a+ni)\,-\,(n+1)i$
for sufficiently large~$n$,
because  
then
by~\sref{spectrum-bounded}
$a-i$
is invertible provided that~$n+1 > \|a+ni\|$.
Indeed, for~$n$ such that~$\|a\|<2n+1$,
we have $\|a+ni\|^2 = \|(a+ni)^*(a+ni)\|
= \|a^2+n^2\|
\leq \|a\|^2+n^2 < 2n+1+n^2 = (n+1)^2$,
and so $\|a+ni\| < n+1$.\qed
\end{point}
\end{point}
\begin{point}{150}[spectrum-self-adjoint-real]{Exercise}%
Let~$a$ be a self-adjoint element of a $C^*$-algebra.
\begin{enumerate}
\item
Show that~$a-\lambda$ is invertible for all $\lambda\in \C\backslash \R$.
\item
Show that $a^2-\lambda$ is invertible for all 
$\lambda\in \C\backslash[0,\infty)$.\\
(Hint: first prove that
 $a^2+1 \equiv (a+i)(a-i)$ is invertible.)

Conclude that $a^n-\lambda$ is invertible for all 
$\lambda\in\C\backslash[0,\infty)$ and \emph{even} $n\in\N$.
\item
Let~$n\in \N$ be \emph{odd}.
Show that $a^n-\lambda$ is invertible
for all~$\lambda\in \C\backslash[0,\infty)$
if and only if $a-\lambda$ is invertible
for all~$\lambda\in \C\backslash[0,\infty)$.\\
(Hint: show that
$a^n+1= \prod_{k=1}^n a+\zeta^{2k+1}$
where $\zeta=e^{\frac{\pi i}{n}}$.)
\end{enumerate}
\end{point}
\begin{point}{160}[inverse-permanence]{Proposition}%
Let~$\scrA$ be a $C^*$-subalgebra
of a $C^*$-algebra $\scrB$.
Let~$a$ be a self-adjoint element of~$\scrA$,
which has an inverse, $a^{-1}$, in~$\scrB$.
Then~$a^{-1}\in\scrA$.
\begin{point}{170}{Proof}%
While we do not know yet that~$a$ is invertible in~$\scrA$,
we do know that~$a+\nicefrac{i}{n}$ 
has an inverse $(a+\nicefrac{i}{n})^{-1}$ in~$\scrA$
by~\sref{spectrum-self-adjoint-real}
for each~$n$
(using that $a$ is self-adjoint.)
Since~$a+\nicefrac{i}{n}$ converges to~$a$ in~$\scrB$ as~$n$ increases,
we see that $(a+\nicefrac{i}{n})^{-1}$ converges to~$a^{-1}$
in~$\scrB$ by~\sref{cstar-inv-continuous}.
Thus, as all~$(a+\nicefrac{i}{n})^{-1}$ are in~$\scrA$,
and~$\scrA$ is closed in~$\scrB$,
we see that~$a^{-1}$ is in~$\scrA$.\qed
\end{point}
\end{point}
\begin{point}{180}[improved-inverse-permanence]{Exercise}%
	Show that the assumption in~\sref{inverse-permanence} 
	that~$a$ is self-adjoint
may be dropped. 

(Hint: consider $a^*a$, see Proposition VIII.1.14 of~\cite{conway2013}.)
\end{point}
\begin{point}{190}[spectrum-of-element]{Definition}%
The \Define{spectrum},
\Define{$\spec(a)$},%
\index{sp@$\spec$, spectrum!$\spec(a)$, of an element of a $C^*$-algebra}
of an element $a$
of a $C^*$-algebra
is the set 
of complex numbers~$\lambda$
for which~$a-\lambda$ is not invertible.
\end{point}
\begin{point}{200}{Exercise}%
Verify the following examples.
\begin{enumerate}
\item
The spectrum of a continuous function~$f\colon X\to \R$
on a compact Hausdorff space~$X$
being an element of the $C^*$-algebra $C(X)$
is the image of~$f$, that is,
$\spec(f) = \{f(x)\colon x\in X\}$.
\item
The spectrum of a square matrix~$A$
from the $C^*$-algebra $M_n$
is the set of eigenvalues of~$A$.
\end{enumerate}
\end{point}
\begin{point}{210}[spectrum-basic]{Exercise}%
Let~$a$ be an element of a $C^*$-algebra $\scrA$.
\begin{enumerate}
\item
Prove that $\spec(a)\subseteq \R$ when $a$ is self-adjoint
(see~\sref{spectrum-self-adjoint-real}).

The reverse implication does not hold:
show that~$\spec(\bigl(
\begin{smallmatrix}0&2\\0&0\end{smallmatrix}\bigr))=\{0\}$.

\item
Show that $\spec(a^2)\subseteq [0,\infty)$ when $a$ is self-adjoint
(see~\sref{spectrum-self-adjoint-real}).

\item
Show that $|\lambda|\leq \|a\|$ for all~$\lambda\in\spec(a)$
using~\sref{spectrum-bounded}.

In fact, we will see in~\sref{norm-spectrum},
that $\|a\|=\sup\{\left|\lambda\right|\colon \lambda\in \spec(a)\}$.
\item
Show that $\spec(a)$ is closed (using~\sref{spectrum-bounded}).\\
Conclude that~$\spec(a)$ is compact.
\item
Show that $\spec(a+z)=\{\lambda+z\colon \lambda\in\spec(a)\}$
for all~$z\in \C$.
\item
Prove that~$\spec(a^{-1})=\{\lambda^{-1}\colon \lambda\in \spec(a)\}$
if~$a$ is invertible (and~$0\notin \spec(a)$).
\end{enumerate}
\end{point}
\begin{point}{220}%
On first sight,
the spectrum $\spec(a)$
of an element~$a$ of a $C^*$-algebra~$\scrA$ 
depends not only on~$a$,
but also on the surrounding $C^*$-algebra~$\scrA$ for it determines
for which~$\lambda\in\C$ the operator $a-\lambda$ is invertible.
Thus we should perhaps write $\spec_\scrA(a)$ instead
of~$\spec(a)$.
However, such careful bookkeeping turns out 
be unnecessary
by the following result.
\end{point}
\begin{point}{230}[spectral-permanence]{Theorem (Spectral Permanence)}%
\index{Spectral Permanence}%
Let~$\scrB$ be a $C^*$-subalgebra of a $C^*$-algebra $\scrA$.
Then~$\spec_{\scrA}(a)=\spec_\scrB(a)$
for every element~$a$ of~$\scrB$.
\begin{point}{240}{Proof}%
Let~$a$ be an element of~$\scrB$,
and let~$\lambda\in \C$.
We must show that $a-\lambda$ is invertible in~$\scrA$
iff $a-\lambda$ is invertible in~$\scrB$.
Surely,
if $a-\lambda$ has an inverse $(a-\lambda)^{-1}$ in~$\scrB$,
then~$(a-\lambda)^{-1}$ is also an inverse of~$a-\lambda$ in~$\scrA$,
since~$\scrB\subseteq \scrA$.
The other, non-trivial, direction follows
    directly from~\sref{inverse-permanence} 
    (and~\sref{improved-inverse-permanence}.)\qed%
\end{point}
\end{point}
\end{parsec}
\section{Positive Elements}
\subsection{Holomorphic Functions}
\begin{parsec}{120}%
\begin{point}{10}%
The next order of business
is to show that the spectrum~$\spec(a)$ of an element~$a$
of a $C^*$-algebra contains enough points, so to speak.
One incarnation of this idea 
is that~$\spec(a)$ is non-empty
(see~\sref{spectrum-non-empty}), but
we will need more,
and prove that  $\|a\|=\left|\lambda\right|$
for some~$\lambda\in\spec(a)$
(provided that~$a$ is self-adjoint).
Somewhat bafflingly,
the canonical and apparently
easiest way to derive this fact is by considering the power series
expansion of a cleverly chosen $\scrA$-valued function
(see~\sref{norm-spectrum}).
To this end,
we'll first quickly redevelop some complex analysis
for~$\scrA$-valued functions
(instead of $\C$-valued functions),
which will only be needed to prove this fact.
\end{point}
\begin{point}{20}{Setting}%
Fix a $C^*$-algebra~$\scrA$ for the remainder of this paragraph.
For brevity,
we'll say that a \Define{function}%
\index{function!$\scrA$-valued \& partial}
is a partially defined map $f\colon \C\to \mathscr{A}$
whose domain of definition $\dom(f)$%
\index{dom@$\dom(f)$, domain of an $\scrA$-valued partial function}
is an open subset of~$\C$.
Such a function is called \Define{holomorphic} at a point~$x\in \C$%
\index{function!holomorphic (at~$x$)}%
\index{holomorphic function}
if $f$ is defined on~$x$,
and 
\begin{equation*}
\frac{f(x)-f(y)}{x-y}
\end{equation*}
converges (with respect to the norm on~$\scrA$)
to some element~$f'(x)$ of~$\scrA$
as $y\in \dom(f)\backslash\{x\}$
converges to~$x$.

We say that~$f$ is \Define{holomorphic}
if~$f$ is holomorphic at~$x$ for all~$x\in \dom(f)$,
and the function $z\mapsto f'(z)$
with $\dom(f')=\dom(f)$
is called its \Define{derivative}.%
\index{derivative of a holomorphic function}%
\index{$(\,\cdot\,)'$!$f'$, derivative of a holomorphic function}
\end{point}
\begin{point}{30}{Exercise}%
Verify the following examples of holomorphic functions.
\begin{enumerate}
\item
If~$f$ and $g$ are holomorphic functions with $\dom(f)=\dom(g)$,
then $f+g$ and $f\cdot g$ are holomorphic,
and $(f+g)'=f'+g'$ and $(f\cdot g)' = f'g+g'f$.

\item
The function~$f$ given by $f(z)=z$ and~$\dom(f)=\C$
is holomorphic, and $f'(z)=1$ for all $z\in\C$.

\item
Let~$a\in \scrA$. The constant function $f$ given by $f(z)=a$
for all~$z\in \C$ is holomorphic, and $f'(z)=0$ for all~$z\in \scrA$.

\item
Any polynomial,
that is, function~$f$ of the form $f(z)\equiv a_n z^n+\dotsb+a_1 z+a_0$
with~$a_i\in \scrA$
is holomorphic with $f'(z)=na_nz^{n-1}+\dotsb+2a_2z+a_1$.
\end{enumerate}
\end{point}
\end{parsec}

\begin{parsec}{130}%
\begin{point}{10}%
We now turn
to perhaps the most important example
of a holomorphic $\scrA$-valued function ---
or at the very least the very source from
which (as we'll see) all holomorphic functions
draw their interesting and pleasant
properties:
the holomorphic $\scrA$-valued function
given by a power series  $\sum_n a_n z^n$.
\end{point}
\begin{point}{20}[hadamard]{Theorem}%
\index{power series}%
Let~$a_0,a_1,a_2,\dotsc\in\scrA$
be given,
and write~$R:=(\limsup_n \|a_n\|^{\nicefrac{1}{n}})^{-1}$.
Then for every~$z\in\C$,
\begin{enumerate}
\item
$\sum_n a_n z^n$
converges absolutely
when~$\left|z\right| < R$, and 
\item
if~$\sum_n a_n z^n$ converges,
then~$\left|z\right|\leq R$.
\end{enumerate}
(The number~$R\in[0,\infty]$ is called the \Define{radius of convergence}%
\index{radius of convergence}
of the series $\sum_n a_n z^n$.)
\begin{point}{30}{Proof}%
Suppose that $\left|z\right|<R$.
To show that the series 
$\sum_n a_nz^n$ converges absolutely,
we must show that $\sum_n \left\|a_n\right\|\left|z\right|^n
\equiv \sum_n (\,\left\|a_n\right\|^{\nicefrac{1}{n}}\left|z\right|\,)^n
<\infty$.
If~$z=0$, this is obvious,
so we'll assume that~$\left|z\right| > 0$.
Then, since~$\left|z\right|<R$,
we have~$R^{-1}\left|z\right|<1$
(and $R^{-1}<\infty$).
Note that
there is $\varepsilon>0$ with $(R^{-1}+\varepsilon)\left|z\right|<1$.
The point of this~$\varepsilon$
is that~$\limsup_n \|a_n\|^{\nicefrac{1}{n}} 
< R^{-1}+\varepsilon$,
so that we can find~$N$ 
with $\|a_n\|^{\nicefrac{1}{n}} \leq R^{-1}+\varepsilon$
for all~$n\geq N$.
Then $\|a_n\|^{\nicefrac{1}{n}}\left|z\right|
\leq (R^{-1}+\varepsilon)\left|z\right|<1$
for all~$n\geq N$,
and so
$\sum_n \|a_n\|\left|z\right|^n 
\leq\sum_{n=0}^{N-1} \|a_n\|\left|z\right|^n+ \sum_{n=N}^\infty 
(\,(R^{-1}+\varepsilon)\left|z\right|\,)^n < \infty$
by  convergence
of the geometric series (c.f.~\sref{geometric}).

Suppose now instead that $\sum_n a_n z^n$ converges.
Then~$\|a_n\|\left|z\right|^n$
converges to~$0$.
In particular,
there is~$N$ with $\|a_n\|\left|z\right|^n \leq  1$
for all~$n\geq N$.
Then~$\|a_n\|^{\nicefrac{1}{n}} \left|z\right| \leq 1$,
and $\|a_n\|^{\nicefrac{1}{n}} \leq \left|z\right|^{-1}$
for all~$n\geq N$,
so that $R^{-1}\equiv \limsup_n \|a_n\|^{\nicefrac{1}{n}}
\leq \left|z\right|^{-1}$,
giving $\left|z\right|\leq R$.\qed
\end{point}
\end{point}
\begin{point}{40}{Proposition}%
The $\scrA$-valued function~$f$
given by a series $\sum_n a_n z^n$
with radius of convergence~$\smash{R:=(\,\limsup_n \|a_n\|^{\nicefrac{1}{n}}\,)^{-1}}$
is holomorphic
when defined
on the disk $\dom(f)=\{z\in\C\colon \left|z\right|<R\}$,
and $f'(z)=\sum_{n=1}^\infty n a_n z^{n-1}$
for all~$z\in \dom(f)$.
\begin{point}{50}{Proof}%
If~$R=0$,
the statement is rather dull, but clearly true,
so we assume that~$R\neq 0$,
that is, $\smash{\limsup_n \|a_n\|^{\nicefrac{1}{n}}}<\infty$.

Note
that the radius of convergence
of~$\sum_{n=1}^\infty na_n z^{n-1}
\equiv \sum_{n=0}^\infty (n+1)a_{n+1}z^n$
is also~$R$,
because
\begin{equation*}
	\smash{\bigl\|\,(n+1)\, a_{n+1}\,\bigr\|^{\nicefrac{1}{n}}}
\ =\  \smash{(n+1)^{\nicefrac{1}{n}}}
\ \smash{\|a_{n+1}\|^{\frac{1}{n+1}}}
\ \smash{\bigl(\|a_{n+1}\|^{\frac{1}{n+1}}\bigr)^{\nicefrac{1}{n}}},
\end{equation*}
and
$R^{-1}=\limsup_n \|a_{n+1}\|^{\frac{1}{n+1}}$,
and both 
$(n+1)^{\nicefrac{1}{n}}$
and  $\bigl(\|a_{n+1}\|^{\frac{1}{n+1}}\bigr)^{\nicefrac{1}{n}}$
converge to~$1$ as~$n\to\infty$
(using here that $\smash{\limsup_n\|a_n\|^{\nicefrac{1}{n}}}<\infty$).

Hence~$\sum_{n=1}^\infty n a_n z^{n-1}$
converges absolutely for every~$z\in\C$ with $\left|z\right|<R$.
Let~$z\in \C$ with $\left|z\right|<R$
be given. We must show that~$f$
is holomorphic at~$z$ with~$f'(z)=\sum_n na_nz^{n-1}$.
For this it suffices to show that
\begin{equation}
\label{power-series-derivative-0}
\sum_{n=0}^\infty
\|a_n\|\left|\,\frac{(z+h)^n-z^n}{h}-nz^{n-1}\,\right|
\end{equation}
converges to~$0$
as $h\in\C$ (with $h\neq 0$ and $\left|z+h\right|<R$)
tends to~$0$.

Pick~$r>0$ with $\left|z\right| < r < R$.
With the appropriate algebraic gymnastics
(involving the identity
$a^n-b^n=(a-b)\sum_{k=1}^n a^{n-k}b^{k-1}$
and the inequalities $\left|z+h\right| \leq r$
and $\left|z\right|\leq r$)
we get, for every~$n$
and~$h\in \C$ with
$h\neq 0$ and~$\left|z+h\right|<r$,
\begin{align}
\left|\  \frac{(z+h)^n-z^n}{h}-nz^{n-1}\ \right|
\ &= \ 
\biggl|\ \sum_{k=1}^n \bigl(\,(z+h)^{n-k}-z^{n-k}\,\bigr)z^{k-1} \ \biggr|
\label{power-series-derivative-1}
\\ 
\label{power-series-derivative-2}
\ &\leq\ 
2nr^{n-1}.
\end{align}
On the one hand,
we see from~\eqref{power-series-derivative-1}
that any term
--- and thus any partial sum ---
of the series from~\eqref{power-series-derivative-0}
converges to~$0$ as~$h$ tends to~$0$.
On the other hand,
we see from~\eqref{power-series-derivative-2}
that the series 
from~\eqref{power-series-derivative-0}
is dominated by~$2\sum_n \|a_n\|nr^{n-1}$
(which converges
because the radius of convergence of $\sum_n a_n nz^{n-1}$
is $R>r$),
so that the tails of the series in~\eqref{power-series-derivative-0}
vanish uniformly in~$h$.
All in all, the sum of the infinite series
from~\eqref{power-series-derivative-0}
converges to~$0$ as~$h$ tends to~$0$.\qed
\end{point}
\end{point}
\begin{point}{60}[powerseries-uniqueness-coeffients]{Exercise}%
Let~$\sum_n a_n z^n$
be a power series over~$\scrA$
with radius of convergence~$R>0$
such that~$\sum_n a_n z^n=0$
for all~$z$ from
some disk around~$0$ with radius~$r<R$.
Show that~$0=a_0=a_1=a_2=\dotsb$.

(Hint: clearly~$a_0=0$.  Show that the derivative
of the power series also vanishes on the disk around~$0$
with radius~$r$.)
\end{point}
\end{parsec}%
\begin{parsec}{140}%
\begin{point}{10}%
All holomorphic functions
are power series
in the sense
that any $\scrA$-valued holomorphic
function~$f$ defined on~$0$
is given by some power series $\sum_n a_n z^n$
on the largest disk around~$0$
that fits in~$\dom(f)$.
This fact,
which follows from~\sref{taylor}
and~\sref{rigid-expansion} below,
is all the more remarkable,
because here the pointwise (``local'') property
of being holomorphic
entails
the uniform (``global'') property
of being equal to a power
series (on some disk).
The device
that bridges
this
gap
is integration of $\scrA$-valued
holomorphic functions along
line segments.
\end{point}
\begin{point}{20}{Exercise}%
We're going to define as quickly as possible
an integral~$\int f $
for every continuous map $f\colon [0,1]\to\scrA$.%
\index{S@$\int$, integral!$\int f$, of continuous $f\colon [0,1]\to \scrA$}
Any interval~$I$
in~$[0,1]$
is of one of the following forms
\begin{equation*}
	[s,t]\qquad[s,t)\qquad(s,t]\qquad(s,t)
\end{equation*}
where~$0\leq s\leq t\leq 1$;
we'll denote the length of an interval~$I$ --- being~$t-s$ 
in the four cases above --- by $\Define{\left|I\right|}$.
An \Define{$\scrA$-valued step function}
is a function $f\colon [0,1]\to\scrA$
of the form
$f\equiv \sum_n a_n \mathbf{1}_{I_n}$
for some~$a_1,\dotsc,a_N\in\scrA$
and intervals $I_1,\dotsc,I_N$
(where~$\mathbf{1}_{I_n}$ is~$1$ is the 
\emph{indicator function}%
\index{$\mathbf{1}_A$, indicator function!on $[0,1]$} of~$I_n$
which is~$1$
on~$I_n$
and~$0$ elsewhere);
and the set of $\scrA$-valued step functions
is denoted by~$\Define{S_\scrA}$,%
\index{SA@$S_\scrA$}
which is a subset
of the space of all bounded functions
$f\colon [0,1]\to\scrA$
which we'll denote by~$\Define{B_\scrA}$.%
    \index{BA@$B_\scrA$}
\begin{enumerate}
\item
Show that there is a unique
linear map $\int\colon S_\scrA\to \scrA$
with~$\int a \mathbf{1}_{I}=\left| I \right|a$
for every interval~$I$ in~$[0,1]$
and~$a\in\scrA$.

(Hint:  the difficulty
here is to show that no contradiction
arises in the sense that 
$\sum_n a_n\left|I_n\right| = \sum_m a_m' \left|I_m'\right|$
when 
$\sum_n a_n \mathbf{1}_{I_n}=\sum_m a_m' \mathbf{1}_{I_n'}$
for intervals $I_1,\dotsc,I_N,I_1',\dotsc,I_M'$ in~$[0,1]$
and $a_1,\dotsc,a_N,a_1',\dotsc,a_M'\in\scrA$.)

\item
We endow~$B_\scrA$
with the supremum norm,
viz.~$\|f\|=\sup_{t\in[0,1]} \|f(t)\|$
for all~$f\in B_\scrA$.

Show that every $\scrA$-valued step function~$f$
may be written as
$f\equiv \sum_n a_n \mathbf{1}_{I_n}$
where $I_1,\dotsc,I_N$
are \emph{disjoint and non-empty}
intervals in~$[0,1]$.

Show that 
for such a representation
$\|f\|=\sup_n \|a_n\|$, and
$\sum_n \left|I_n\right|\leq 1$.
Deduce that
$\|\int f\| \leq \sum_n \|a_n\|\left|I_n\right|
\leq \|f\|$.

Conclude that~$\int\colon S_\scrA\to\scrA$
is a bounded linear map
and can therefore
be uniquely extended to a bounded linear map
$\int\colon \overline{S}_\scrA\to\scrA$
on the closure~$\overline{S}_\scrA$
of~$S_\scrA$.

\item
Show that every continuous function $f\colon [0,1]\to\scrA$
is the supremum norm limit
of a sequence $g_1,g_2,\dotsc$
of $\scrA$-valued step functions
 (i.e.~$f\in\overline{S}_\scrA$).

\item
Show that~$\int af = a\int f$
when~$f\colon [0,1]\to\C$
is continuous and~$a\in \scrA$.
\end{enumerate}
\end{point}
\begin{point}{30}{Definition}%
The integral of
a holomorphic $\scrA$-valued function~$f$
along a line segment $[w,w']\subseteq\dom(f)$
	(where~$w$ and~$w'$ are thus \emph{complex numbers})
is now defined as
\begin{equation*}
\Define{\int_{w}^{w'}f}
\ = \ 
(w'-w)\int_0^1f(\,w+t(w'-w)\,)\,dt.
\end{equation*}
\index{S@$\int$, integral!of continuous $f\colon \C\to\scrA$!$\int_w^{w'} f$, over an interval}%
We'll also need integration along a triangle~$T$,%
\index{triangle, for our purposes}
which is for this  purpose a triple of 
complex numbers~$w_0,w_1,w_2$
(of which the order \emph{does} matter)
called the
\Define{vertices} of~$T$.
The \Define{boundary} of
such a triangle~$T$
is $\Define{\partial T}
:=[w_0,w_1]\cup [w_1,w_2]\cup [w_2,w_0]$,
and given any $\scrA$-valued
holomorphic function~$f$
with $\partial T\subseteq \dom(f)$
we define
\begin{equation*}
	\Define{\int_T f}\ = \ \int_{w_0}^{w_1} f
\,+\, \int_{w_1}^{w_2} f
\,+\, \int_{w_2}^{w_0} f.
\end{equation*}
\index{S@$\int$, integral!of continuous $f\colon \C\to\scrA$!$\int_T f$, over a triangle}%
We'll need some more terminology
relating to our triangle~$T$.
Its \Define{closure},
written $\Define{\mathrm{cl}(T)}$,
is the convex hull of~$w_0,w_1,w_2$,
and its \Define{interior}
is simply
$\Define{\mathrm{in}(T)}=\mathrm{cl}(T)\backslash \partial T$.
The length
of~$T$ is given by
$\Define{\mathrm{length}(T)}:=
\left|w_1-w_0\right|\,+\,
\left|w_2-w_1\right|\,+\,
\left|w_0-w_2\right|$.

The number of times the triangle~$T$ winds
around a point $z\in \C\backslash \partial T$
in the counterclockwise direction
is
called the 
\Define{winding number}, and
is
written $\Define{\mathrm{wn}_T(z)}$,%
\index{wn@$\mathrm{wn}_T$, winding number}
is either $1$ or~$-1$ when~$z\in\mathrm{in}(T)$
(depending on the order of the vertices),
is~$0$ when~$z\notin\mathrm{cl}(T)$,
and undefined on~$\partial T$.
It is defined formally for $z\in \C\backslash \partial T$ by
\begin{equation*}
	\textstyle
2\pi \wn_T(z)\ = \ 
\measuredangle(w_0,z,w_1)
\,+\,\measuredangle(w_1,z,w_2)
\,+\, \measuredangle(w_2,z,w_0),
\end{equation*}
where~$\Define{\measuredangle(w_0,z,w_1)}$%
\index{$\measuredangle(w_0,z,w_1)$, angle between complex numbers}
denotes the number of radians in~$(-\pi,\pi)$
needed to rotate the line through~$z$ and~$w_0$ counterclockwise
around~$z$ to hit~$w_1$,
that is, the angle of the corner on the right when travelling
from~$w_0$ to~$w_1$ via~$z$.

The winding number~$\mathrm{wn}_T$
pops
up in the value of the integral $\int_T (z-z_0)^{-1}dz$
later on,
see~\sref{invint}.
\end{point}
\begin{point}{40}[goursat]{Goursat's Theorem}%
\index{Goursat's Theorem}%
Let~$f$ be a holomorphic function,
and let~$T$ be a triangle whose closure
is entirely contained in~$\dom(f)$.
Then~$\int_T f = 0$.
\begin{point}{50}[goursat-1]{Proof}%
(Based on~\cite{moore1900}.)
If two vertices of~$T$ coincide
the result is obviously true,
so we may assume that they're all distinct,
that is, $\mathrm{in}(T)\neq \varnothing$.

Note that if~$f$ has an antiderivative,
that is, $f\equiv g'$ for some holomorphic function~$g$,
then one can show that~$\int_T f=0$
(after deriving the fundamental theorem of calculus).
Although it is true that every holomorphic function
with simply connected domain has a antiderivative,
this result is not yet available 
(and in fact usually depends on this very theorem).
Instead we will approximate~$f$
by an affine function
(which does have an antiderivative)
using the derivative of~$f$.
But since such an approximation only
concerns a single point,
we first need to zoom in.
\begin{point}{60}[goursat-2]%
If we split~$T$ into four similar triangles
$T^\text{i}$, $T^\text{ii}$,
$T^\text{iii}$, $T^\text{iv}$
\begin{equation*}
\begin{tikzpicture}
\coordinate (A) at (0,0){};
\coordinate (B) at (4,0){};
\coordinate (C) at (3,2){};
\coordinate (AB) at ($(A)!0.5!(B)$) {};
\coordinate (BC) at ($(B)!0.5!(C)$) {};
\coordinate (CA) at ($(C)!0.5!(A)$) {};
\node  at ($(AB)!0.5!(BC)!0.33!(CA)$) {$\circlearrowright$};
\node  at ($(A)!0.5!(AB)!0.33!(CA)$) {$\circlearrowleft$};
\node  at ($(B)!0.5!(BC)!0.33!(AB)$) {$\circlearrowleft$};
\node  at ($(C)!0.5!(BC)!0.33!(CA)$) {$\circlearrowleft$};
\draw
	(A) -- (AB) 
	(AB) -- (B)
	(B)  -- (BC)
	(BC) -- (C) 
	(C) -- (CA)
	(CA) -- (A)
	(CA) -- (BC)
	(BC) -- (AB)
	(AB) -- (CA);
\end{tikzpicture}
\end{equation*}
we have $\smash{\int_Tf = \sum_{n={\text{i}}}^{\text{iv}} \int_{T^n}f}$.
There is $T'$ among
$T^\text{i}$, $T^\text{ii}$,
$T^\text{iii}$, $T^\text{iv}$
with 
 $\|\int_Tf\|\leq 4 \|\int_{T'} f\|$.
Clearly, $\length(T)=2\length(T')$.
Write~$T_0 := T$ and $T_1 := T'$. 

From this it is clear how to
 get a sequence of similar triangles $T_0, T_1, T_2, \dotsc$
with $\|\int_Tf\|\leq 4^n \|\int_{T_n} f\|$,
and $\length(T)=2^n\length(T_n)$.
\end{point}
\begin{point}{70}%
If we pick a point on the closure $\mathrm{cl}(T_n)$
of each triangle~$T_n$ 
we get a Cauchy sequence
that converges to some point~$z_0\in\C$
which lies in~$\bigcap_n \mathrm{cl}(T_n)$.
We can approximate $f$ by an affine
function at~$z_0$ as follows.
For $z\in \dom(f)$,
\begin{equation*}
f(z)\ = \ f(z_0)\,+\,f'(z_0)\,(z-z_0)\,-\,r(z)\,(z-z_0),
\end{equation*}
where~$r\colon \dom(f)\to \C$
is given by $r(z)=f'(z_0)-(f(z)-f(z_0))(z-z_0)^{-1}$ for $z\neq z_0$
and $r(z_0)=0$.
We see that~$r(z)$ converges to~$0$ as~$z\to z_0$.

Let~$\varepsilon >0$ be given.
There is~$\delta>0$
such that $z\in\dom(f)$
and $\|r(z)\|\leq \varepsilon$
for all~$z\in \C$ with $\|z-z_0\|<\delta$.
There is~$n$ such that the triangle~$T_n$ is contained
in the ball around~$z_0$ of radius~$\delta$.
Note that $\int_{T_n} f(z_0)+f'(z_0)(z-z_0)\,dz=0$
by the discussion in~\sref{goursat-1}, because
the integrated function is affine.
Thus
\begin{equation*}
\textstyle
\int_{T_n} f \  = \ -\int_{T_n}r(z)\,(z-z_0)\,dz.
\end{equation*}
Note that for $z\in T_n$,
we have  $\|z-z_0\|\leq \length(T_n)$,
and $\|r(z)\|\leq \varepsilon$ (because $\|z-z_0\|< \delta$),
and so $\|r(z)(z-z_0)\|\leq \varepsilon\,\length(T_n)$.
Thus:
\begin{equation*}
\textstyle
\|\int_{T_n} f\| \  = \ \|\int_{T_n}r(z)\,(z-z_0)\,dz\|
\ \leq\ \varepsilon\length(T_n)^2.
\end{equation*}
Using the inequalities from~\sref{goursat-2},
we get
\begin{equation*}
\textstyle
\|\int_T f\|\ \leq\ 4^n\, \|\int_{T_n} f\|
\ \leq\ \varepsilon \,4^n\,\length(T_n)^2 
\ \equiv\ \varepsilon \length(T)^2.
\end{equation*}
Since~$\varepsilon>0$ was arbitrary,
we see that~$\int_T f=0$.\qed
\end{point}
\end{point}
\end{point}%
\begin{point}{80}[invint]{Exercise}%
	The assumption in Goursat's Theorem (\sref{goursat})
that the holomorphic function~$f$
is defined not only on the boundary~$\partial T$
of the triangle~$T$
but also on the interior $\mathrm{in}(T)$
is essential,
for if only a single hole in~$\dom(f)$ is allowed within $\mathrm{in}(T)$
the integral~$\int_T f$ can become non-zero---which we will demonstrate
here by computing
$\int_T (z-z_0)^{-1}dz$.
\begin{enumerate}%
\item
Show that for a non-zero
complex number~$z$ we have
\begin{equation*}
	z^{-1}\ =\  \frac{\Real{z}-i\Imag{z}}{\Real{z}^2+\Imag{z}^2}.
\end{equation*}
\item
Given real numbers~$a\neq 0$ and~$b$,
show that
\begin{alignat*}{3}
	\int_{a}^{a+ib}\ 
	z^{-1}\,dz
	\ &=\ 
	i\int_{0}^t \frac{a-it}{a^2+t^2}dt
	\\
	\ &=\ 
	i\int_0^b \frac{a}{a^2+t^2}\,dt 
	\ +\ 
	\int_0^b \frac{t}{a^2+t^2}\,dt\\
	\ &=\ 
	\textstyle
	i\,\arctan(\,b/a\,)
	\,+\, \log\left|a+ib\right| - \log\left|ia\right|,
\end{alignat*}
and similarly, show that for real numbers~$a$ and $b\neq 0$
\begin{equation*}
	\int_{a+ib}^{ib} z^{-1}\,dz
	\ = \ 
	i\arctan(\,a/b\,) \ +\ 
	\log\left|ib\right| \,-\,
	\log\left|a+ib\right|.
\end{equation*}
	\item
Show that for complex numbers~$w$, $w'$ and~$z_0$
with~$z_0\notin [w,w']$
\begin{equation*}
	\int_{w}^{w'}\,(z-z_0)^{-1}\,dz
\ = \ 
i\, \measuredangle(w,z_0,w')\ +\ 
\log\,\frac{\left|w'-z_0\right|}{\left|w-z_0\right|},
\end{equation*}
where~$\measuredangle(w,z_0,w')$
denotes
the number of radians
in~$(-\pi,\pi)$
needed
to rotate the line through~$z_0$ and~$w$
counterclockwise around~$z_0$ to hit~$w'$.

(Hint: 
using Goursat's Theorem, \sref{goursat},
one may reduce the problem
to integration along horizontal and vertical line segments.)
\item
Given a triangle~$T$ and~$z_0\in\C\backslash \partial T$,
show that
\begin{equation*}
	\frac{1}{2\pi i}\int_T (z-z_0)^{-1}\,dz
	\ =  \mathrm{wn}_T(z_0).
\end{equation*}
\end{enumerate}
\end{point}
\begin{point}{90}%
Thus integration of 
$z\mapsto (z-z_0)^{-1}$
along a triangle~$T$ detects
the number of times~$T$ winds
around~$z_0$.
There is nothing special about a triangle:
a similar result---not needed here---holds
for a broad class of curves
(c.f.~Thm~2.9 of~\cite{conway2013}).

Integration along a curve can also be used
to probe the value of a holomorphic function at a point~$z_0$.
On this occasion
we restrict ourselves
to regular $N$-gons.
\end{point}
\end{parsec}%
\begin{parsec}{150}%
\begin{point}{10}[cauchy-formula]{Theorem (Cauchy's Integral Formula)}%
\index{Cauchy's Integral Formula}%
Let~$f$ be a holomorphic $\scrA$-valued function
which is defined on the interior and boundary
of some regular $N$-gon 
with centre~$c\in\C$,
circumradius~$r$
and vertices $w_n := c+r\cos(2\pi/n)+ir\sin(2\pi/n)$.
Then for any complex number~$z_0$ in
the interior of the~$N$-gon
we have
\begin{equation*}
	f(z_0)\ = \ \frac{1}{2\pi i}\,\sum_{n=0}^{N-1}\int_{w_n}^{w_{n+1}}
	\frac{f(z)}{z-z_0}\,dz
\end{equation*}
\begin{point}{20}{Proof}%
	Since~$\sum_{n=0}^{N-1} \int_{w_n}^{w_{n+1}} \frac{f(z_0)}{z-z_0}\,dz
	= 2\pi i f(z_0)$ by~\sref{invint}
it suffices to show that
\begin{equation}
\label{eq:cauchy-formula-1}
\sum_{n=0}^{N-1}\int_{w_n}^{w_{n+1}} \frac{f(z)-f(z_0)}{z-z_0}\,dz \ = \ 0.
\end{equation}
\begin{point}{30}[cauchy-formula-1]%
Let~$\varepsilon>0$ be given.
Since~$f$ is holomorphic at~$z_0$
we can find $\delta>0$ with
\begin{equation*}
\left\|\frac{f(z)-f(z_0)}{z-z_0}\right\|
\ \leq \ \,\|f'(z_0)\|\,+\,37
\end{equation*}
for all~$z\in\dom(f)$ with $\left|z-z_0\right|\leq \delta$. 
\end{point}
\begin{point}{40}%
To use~\sref{cauchy-formula-1},
we must restrict our attention to a smaller polygon.
Let~$T$ be a triangle 
that is entirely inside the~$N$-gon
such that  $\mathrm{wn}_T(z_0)=-1$,
$\length(T)\leq \varepsilon$,
and $\|z_0-z\|\leq \delta$ for all~$z\in \partial T$.
By partitioning the area
between~$T$ and the~$N$-gon
in the obvious manner
into triangles~$T_1,\dotsc,T_M$
(for which~$\int_{T_m}f=0$ for all~$m$ by~\sref{goursat})
we see that
\begin{equation}
\label{eq:cauchy-formula-2}
\sum_{n=0}^{N-1}\int_{w_n}^{w_{n+1}} \frac{f(z)-f(z_0)}{z-z_0}\,dz
\ = \ 
\int_T \frac{f(z)-f(z_0)}{z-z_0}\,dz.
\end{equation}
Hence by~\sref{cauchy-formula-1}
we have
\begin{alignat*}{3}
	\left\|\,\sum_{n=0}^{N-1} 
	\int_{w_n}^{w_{n+1}} \frac{f(z)-f(z_0)}{z-z_0}\,dz\,\right\|
	\ &\leq \ \length(T)\,\cdot\,
\sup_{z\in\partial T} \,\left\|\,\frac{f(z)-f(z_0)}{z-z_0}\,\right\|
\\
	\ &\leq \ \|f'(z_0)\|\varepsilon\,+\,37\varepsilon.
\end{alignat*}
Since~$\varepsilon>0$ was arbitrary,
 \eqref{eq:cauchy-formula-1}
follows from \eqref{eq:cauchy-formula-2}.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{50}[taylor]{Proposition}%
Let~$f$ be a holomorphic $\scrA$-valued function
defined on the boundary and interior
of a regular $K$-gon
with vertices $w_0,\dotsc,w_{K-1},w_K=w_0$
as in~\sref{cauchy-formula}.
Then for every element~$z$ of an open disk in the interior of the $K$-gon
with centre~$w$,
\begin{equation*}
f(z)\ = \ 
	\sum_{n=0}^\infty \ 
	\left(\frac{1}{2\pi i}\sum_{k=0}^{K-1}\int_{w_k}^{w_{k+1}} 
	\frac{f(u)}{(u-w)^{n+1}}\,du\right)
\ (z-w)^n.
\end{equation*} 
\begin{point}{60}{Proof}%
By~\sref{cauchy-formula} and some easy algebra we have
\begin{alignat*}{3}
2\pi if(z)\ &=\  
	\sum_{k=0}^{K-1}\int_{w_k}^{w_{k+1}}
	\frac{f(u)}{u-z}\,du
\ =\ 
\sum_{k=0}^{K-1}\int_{w_k}^{w_{k+1}}
   \frac{f(u)}{u-w}\,\frac{1}{1-\frac{z-w}{u-w}}\,du
\end{alignat*}
Note that~$\left|z-w\right|<\left|u-w\right|$
for all~$u\in [w_k,w_{k+1}]$ and~$k$,
because the open disk with centre~$w$
from which~$z$ came lies entirely in the~$K$-gon.
Hence,
by~\sref{geometric},
\begin{alignat*}{3}
	2\pi if(z) \ &= \ 
\sum_{k=0}^{K-1}\int_{w_k}^{w_{k+1}}
   \frac{f(u)}{u-w}\, \sum_{n=0}^\infty 
\frac{(z-w)^n}{(u-w)^n}
 \,du\\
 \ &= \ 
  \sum_{n=0}^\infty \ 
\sum_{k=0}^{K-1}\int_{w_k}^{w_{k+1}}
  \frac{f(u)}{(u-w)^{n+1}}du \ (z-w)^n,
\end{alignat*}
where the interchange of ``$\sum$'' and ``$\int$''
is allowed
because the partial sum 
$\sum_{n=0}^Nf(u)\frac{(z-w)^n}{(u-w)^{n+1}}$
converges uniformly in~$u$ as~$N\to\infty$.\qed
\end{point}
\end{point}
\begin{point}{70}[rigid-expansion]{Proposition}%
Let~$f$ be an $\scrA$-valued holomorphic
function
that 
can be written as a power
series $f(z)=\sum_n a_n (z-w)^n$
where~$a_0,a_1,\dotsc\in\scrA$
for all~$z$ from some disk in~$\dom(f)$ around~$w$
with radius~$r>0$.

Then the formula $f(z)=\sum_n a_n (z-w)^n$
holds also for any $z$ from a larger disk 
with radius~$R>r$
around~$w$ that still fits in~$\dom(f)$.
\begin{point}{80}{Proof}%
Let~$z$ with $\left|z-w\right|<R$ be given.
By choosing~$K$ large enough
we can fit the boundary of a regular $K$-gon
centered around~$w$
with vertices $w_0,\dotsc,w_{K-1},w_{K}\equiv w_0$
inside the difference between the two disks,
and we can moreover, by~\sref{taylor},
choose the polygon
in such a way that
$f(z')=\sum_n b_n (z'-w)^n$
for all~$z'\in\C$
with $\left|z'-w\right|\leq \left|z-w\right|$
where $b_n = \sum_{k=0}^{K-1}
\int_{w_k}^{w_{k+1}}
\frac{f(u)}{(u-w)^{n+1}}\,du$.

Thus to show that $f(z)=\sum_n a_n (z-w)^n$
it suffices to show that~$a_n=b_n$ for all~$n$.
This in turn
follows by~\sref{powerseries-uniqueness-coeffients} from the fact
that $\sum_n a_n (z'-w)^n
= \sum_n b_n (z'-w)^n$
for all~$z'\in \C$ with $\left|z'-w\right|<r$.\qed
\end{point}
\end{point}
\end{parsec}
\subsection{Spectral Radius}
\begin{parsec}{160}%
\begin{point}{10}%
Our analysis of $\scrA$-valued
holomorphic functions
allows us to expose
the following connection
between the norm
and the invertible elements
in a $C^*$-algebra.
\end{point}
\begin{point}{20}[norm-spectrum]{Proposition}%
For a self-adjoint element~$a$ of a $C^*$-algebra~$\scrA$,
we have
\begin{equation*}
\|a\|\,=\,\sup\{\,\left|\lambda\right|\colon 
\,\lambda\in \spec(a)\,\}.
\end{equation*}
(The quantity on the right hand-side above
is called the \Define{spectral radius} of~$a$.)%
\index{spectral radius}
\begin{point}{30}{Proof}%
Write~$r=
\sup\{\left|\,\lambda\right|\colon\, \lambda\in \spec(a)\backslash\{0\}\,\}$
where the supremum is computed
in~$[0,\infty]$ so that~$\sup\varnothing=0$.
Since~$\left|\lambda\right| \leq \|a\|$
for all~$\lambda\in\spec(a)$
(\sref{spectrum-bounded})
we see that~$r\leq \|a\|$,
and so we only need to show that~$\|a\|\leq r$. 
Note that this is clearly true if~$\|a\|=0$,
so we may assume that~$\|a\|\neq 0$.

The trick is to consider
the power series expansion
around~$0$ of the holomorphic function~$f$ defined
on~$G:=\{\,z\in \C\colon 1-az\text{ is invertible}\,\}$ 
by  $f(z)=z(1-az)^{-1}$.
More specifically,
we are interested in the distance~$R$
of~$0$ to the complement of~$G$,
viz.~$R= \inf\{\left|\lambda\right|\colon \lambda\in \C\backslash G\}$
(where the infimum is computed in~$[0,\infty]$
so that~$\inf\varnothing=\infty$)
because since $0\in G$
and $z\notin G\iff z^{-1}\in \spec(a)$,
we have~$R=r^{-1}$
(using the convention $0^{-1}=\infty$).

Note that $f$ has the power series expansion
$f(z) = \sum_n a^nz^{n+1}$
for all~$z\in \C$ with $\|z\|<\|a\|^{-1}$,
because for such~$z$
we have $\sum_n (az)^n=(1-az)^{-1}$
by~\sref{geometric},
and thus~$f(z)=z(1-az)^{-1}=z\sum_n (az)^n = \sum_n a^nz^{n+1}$.

By~\sref{rigid-expansion}
we know that~$f(z)=\sum_n a^nz^{n+1}$
is valid not only for~$z\in \C$ with $\left|z\right|< \|a\|^{-1}$,
but for all~$z$ with $\left|z\right|< R$.
However, $R$ cannot be strictly larger than~$\|a\|^{-1}$,
because for every $z\in\C$ with $\left|z\right|>\|a\|^{-1}$
the series $\sum_n(az)^n$ 
and thus $\sum_n a^n{z}^{n+1}$ diverges (see~\sref{geometric-convergence})
--- using here that~$a$ is self-adjoint.
Hence~$R=\|a\|^{-1}$, and so~$r=\|a\|$.\qed
\end{point}
\end{point}
\begin{point}{40}{Remark}%
For an arbitrary (possibly non-self-adjoint)
element~$a$ of a $C^*$-algebra~$\scrA$
the formula in~\sref{norm-spectrum}
might be incorrect, e.g.~$\bigl\|\,\bigl(\begin{smallmatrix}
0& 1\\
0 & 0
\end{smallmatrix}\bigr)\,\|=1$
while 
$\spec(\,\bigl(\begin{smallmatrix}
0& 1\\
0 & 0
\end{smallmatrix}\bigr)\,)=\{0\}$
cf.~\sref{geometric-non-self-adjoint}.
For such~$a$
the formula
$\sup\{\,\left|\lambda\right|\colon\, \lambda\in \spec(a)\,\}
\,=\, \limsup_n \|a^n\|^{\nicefrac{1}{n}}$
	can be derived (see e.g.~Theorem~3.3.3 of~\cite{kr}) --- 
	which we won't need here.
\end{point}
\begin{point}{50}[spectrum-non-empty]{Exercise}%
Given a self-adjoint element~$a$ of a $C^*$-algebra show that
$\spec(a)\neq \varnothing$.
\end{point}
\begin{point}{60}{Exercise}%
Given a self-adjoint element~$a$ of a $C^*$-algebra
and~$\lambda\in\R$
show that $\spec(a) =\{\lambda\}$ iff $a=\lambda$.
\end{point}
\begin{point}{60}{Exercis}%
Use the previous exercise to prove the following theorem.
\end{point}
\begin{point}{70}{Theorem (Gelfand--Mazur for $C^*$-algebras)}%
\index{Gelfand--Mazur's Theorem}%
If every non-zero element of a $C^*$-algebra~$\scrA$
is invertible, then~$\scrA=\C$ or~$\scrA=\{0\}$.
\end{point}
\begin{point}{80}[gelfand-mazur-predicament]{Remark}%
A logical next step
towards Gelfand's representation theorem
is to show that if~$\lambda\in\spec(a)$
for some element~$a$ of a \emph{commutative} $C^*$-algebra~$\scrA$,
then there is a miu-map $f\colon \scrA\to \C$
with~$f(a)=\lambda$.
Here we have moved ourselves into a tight spot
by evading Banach algebras,
because the mentioned result is usually obtained
by finding a maximal ideal~$I$ of~$\scrA$
(by Zorn's Lemma) that contains~$\lambda-a$,
and then forming the \emph{Banach algebra} quotient~$\scrA/I$.
One then applies Gelfand--Mazur's Theorem for \emph{Banach algebras}, 
to see that
$\scrA/I= \C$,
and thereby obtain a miu-map~$f\colon \scrA\to \C$ with~$f(a-\lambda)=0$.
The problem here is that while $\scrA/I$
will turn out to be a $C^*$-algebra (indeed, be $\C$)
the formation of the $C^*$-algebra quotient
is non-trivial and depends on Gelfand's representation theorem
(see e.g. \S{}VIII.4 of~\cite{conway2013}) 
which is the very theorem we are working towards.
The way out of this predicament
is to avoid ideals and quotients of $C^*$- and Banach algebras
altogether,
and instead work 
with order ideals (and what are essentially
 quotients of Riesz and order unit spaces).
To this end,
we develop the theory
of the positive elements of a $C^*$-algebra
farther than is usually done
for Gelfand's representation theorem.
\end{point}
\end{parsec}
\begin{parsec}{170}[cstar-positive-2]%
\begin{point}{10}%
We return to the positive elements 
in a $C^*$-algebra (see~\sref{cstar-positive-def}).
We'll see that the connection we have established
between the norm and invertible elements
of a $C^*$-algebra
via the spectral radius (\sref{norm-spectrum})
affects the positive elements as well, see~\sref{cstar-positive-1}.
\end{point}
\begin{point}{20}[real-pos-ineq]{Exercise}%
Show that 
$\left|\,\lambda-t\,\right| \,\leq\, t$ iff  $\lambda \in[0,2t]$,
where $\lambda,t\in\R$.
\end{point}
\begin{point}{30}[pos-spectrum]{Proposition}%
For a self-adjoint element $a$ from a $C^*$-algebra,
and $t\in [0,\infty]$, 
\begin{equation*}
\|a-t\|\,\leq\, t\qquad\iff\qquad \spec(a)\subseteq [0,2t].
\end{equation*}%
\begin{point}{40}{Proof}%
To begin, note that~$\spec(a-t)=\spec(a)-t\subseteq \R$ 
by~\sref{spectrum-basic},
because~$a$ is self-adjoint.
Thus $\|a-t\|=\sup\{\,\left|\lambda-t\right|\colon \lambda\in \spec(a)\,\}$
by~\sref{norm-spectrum}.
Hence $\|a-t\|\leq t$
iff $\left|\lambda-t\right|\leq t$ for all~$\lambda\in\spec(a)$
iff $\spec(a)\subseteq [0,2t]$ (by \sref{real-pos-ineq}).\qed
\end{point}
\end{point}
\begin{point}{50}[cstar-positive-1]{Exercise}%
\index{positive!element of a $C^*$-algebra}%
Show
(using~\sref{pos-spectrum} and~\sref{spectrum-basic})
that
for any self-adjoint element $a$ of a $C^*$-algebra~$\scrA$,
the following are equivalent.
\begin{enumerate}
\item 
\label{cstar-pos-1}
$\|a-t\|\leq t$
for some $t\geq \frac{1}{2}\|a\|$;
\item 
\label{cstar-pos-2}
$\|a-t\|\leq t$
for all $t\geq \frac{1}{2}\|a\|$;
\item 
\label{cstar-pos-3}
$\spec(a)\subseteq[0,\infty)$;
\item
$a$ is positive.
\end{enumerate}
We will complete this list in~\sref{cstar-positive-final}.
\end{point}
\begin{point}{60}[positive-basic-2]{Exercise}%
Let~$\scrA$ be a $C^*$-algebra.
\begin{enumerate}
\item
Show that $0\leq a\leq 0$ entails that~$a=0$
for all~$a\in\scrA$.
\item
Show that~$\pos{\scrA}$ is closed.
\item
Let~$a$ be a self-adjoint element of~$\scrA$.
Show that
 $-\lambda \leq a\leq \lambda$
iff $\|a\|\leq \lambda$,
for $\lambda\in [0,\infty)$.
Conclude that $\|a\| = \inf\{ \lambda \in \R\colon 
-\lambda \leq a\leq \lambda\}$.

(In other words
$\sa{\scrA}$ is a \emph{complete Archimedean order unit space},
see Definition~1.12 of~\cite{alfsen2012}---a 
type of structure first studied in~\cite{kadison1951}.)


Show that $0\leq a \leq b$ entails $\|a\|\leq \|b\|$
for $a,b\in\sa{\scrA}$.

\item 
Recall that $ab$ need not be positive if~$a,b\geq 0$. However:

Show that $a^2$ is positive for every self-adjoint element~$a$ of~$\scrA$.

Show that $a^n$ is positive for \emph{even} $n\in \N$ and~$a\in\sa{\scrA}$.

Show that $a^n$ is positive iff $a$ is positive for \emph{odd} $n\in \N$
and $a\in\sa{\scrA}$.

Show that $a^n$ is positive
for every positive $a$ from~$\scrA$ and~$n\in \N$.
\item
Let~$a$ be an invertible element of~$\scrA$.
Show that $a\geq 0$ iff $a^{-1}\geq 0$.

\item
Show that a positive element~$a$ of~$\scrA$ is invertible
iff $a\geq \frac{1}{n}$ for some~$n>0$.
(Hint: show that $\spec(a)\subseteq [\frac{1}{n},\infty)$
		when~$a\geq \frac{1}{n}$.)
\end{enumerate}
\end{point}
\end{parsec}
\begin{parsec}{180}%
\begin{point}{10}{[Moved to \sref{cstar-product-2}.]}
\end{point}
\end{parsec}
\begin{parsec}{190}
\begin{point}{10}%
Although we can't quite yet see that~$a^*a$
is positive---for this we need the existence of the square root, 
    \sref{sqrt},---we
    can already prove that~$a^*a$ can't be negative,
    see~\sref{astara-non-negative}.
\end{point}
\begin{point}{11}[prod-spec]{Lemma}%
For elements $a$ and $b$ from a $C^*$-algebra,
we have
\begin{equation*}
\spec(ab)\backslash\{0\}\ =\ \spec(ba)\backslash\{0\}.
\end{equation*}
\begin{point}{20}{Proof}%
Let~$\lambda\in \C$ with $\lambda\neq 0$ be given.
We must show that $\lambda - ab$ is invertible
iff $\lambda - ba$ is invertible.
Suppose that $\lambda-ab$ is invertible.
Then using the equality $a(\lambda-ba)=(\lambda-ab)a$
one sees that $(1+b(\lambda-ab)^{-1}a)(\lambda-ba)=\lambda$.
Since similarly $(\lambda-ba)(1+b(\lambda-ab)^{-1}a)=\lambda$,
we see that $\lambda^{-1}(1+b(\lambda-ab)a)$
is the inverse of~$\lambda-ba$.\qed
\end{point}
\end{point}
\begin{point}{30}[astara-non-negative]{Lemma}%
We have $a^*a  \leq 0\implies a=0$
for every element~$a$ of a $C^*$-algebra.
\begin{point}{40}{Proof}%
Suppose that $a^*a\leq 0$.
Then~$\spec(a^*a)\subseteq (-\infty,0]$, almost by definition,
and so $\spec(aa^*)\subseteq (-\infty,0]$ by~\sref{prod-spec},
giving $aa^*\leq 0$.
Thus $a^*a+aa^*\leq 0$.

But on the other hand, 
$a^*a+aa^* = 2(\Real{a}^2 + \Imag{a}^2) \geq 0$,
and so~$a^*a+aa^*=0$.
Then $0\geq a^*a=-aa^*\geq 0$ gives $a^*a=0$,
and $a=0$.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{200}%
\begin{point}{10}%
Observe that the norm and order on 
(the self-adjoint elements of a) $C^*$-algebra~$\scrA$
completely determine one another (using the unit):
on the one hand
$\|a\|=\inf\{\lambda\geq 0\colon -\lambda\leq a\leq \lambda\}$
by~\sref{positive-basic-2},
and on the other hand
$a\geq 0$ iff $\|a-s\|\leq s$ for some $s\in\R$
by definition (\sref{cstar-positive-def}).
This has some useful consequences.
\end{point}
\begin{point}{20}[weak-russo-dye]{Lemma}%
A positive map~$f\colon \scrA\to\scrB$
between $C^*$-algebras
is bounded.
More specifically,
we have
$\|f(a)\|\leq \|f(1)\|\,\|a\|$
for all self-adjoint~$a\in\sa{\scrA}$,
and we have $\|f(a)\|\leq 2\|f(1)\|\,\|a\|$
for arbitrary $a\in \scrA$.
\begin{point}{30}{Proof}%
Given~$a\in\sa{\scrA}$
we have~$-\|a\|\leq a \leq \|a\|$,
and $-\|a\|\,f(1)\leq f(a)\leq \|a\|\,f(1)$
(because $f$ is positive),
and thus~$\|f(a)\|\leq f(1)\,\|a\|\leq \|f(1)\|\,\|a\|$ 
by~\sref{positive-basic-2}.

For an arbitrary element $a\equiv \Real{a}+i\Imag{a}$
of~$\scrA$
we have 
$\|f(a)\|\leq \|f(\Real{a})\|+\|f(\Imag{a})\|\leq 2\|f(1)\|\,\|a\|$.\qed
\end{point}
\begin{point}{40}[russo-dye-remark]{Remark}%
It is a non-trivial theorem (see~\sref{russo-dye})
that the factor ``2'' in the statement above
can be dropped, i.e.~$\|f\|=\|f(1)\|$
(c.f.~Corollary~1 of~\cite{russodye}).
We'll be using this improved bound mostly
for completely positive maps,
for which it's much easier to obtain (see~\sref{cp-russo-dye}).

For miu-maps we can already obtain the improved bound here:
\end{point}
\end{point}
\begin{point}{50}[norm-mi-map]{Lemma}%
Any miu-map $\varrho\colon\scrA\to\scrB$ 
    between $C^*$-algebras $\scrA$ and~$\scrB$
is positive, bounded,
    and, in fact, $\|\varrho\|\leq 1$.
\begin{point}{51}{Proof}%
Let~$a$ be a positive element of~$\scrA$,
    so~$\spec(a)\subseteq[0,\infty)$ by~\sref{cstar-positive-1},
To show that~$\varrho$ is positive,
we must prove that~$\varrho(a)\geq 0$,
that is, $\spec(\varrho(a))\subseteq[0,\infty)$.
This follows immediately
from the observation that~$\spec(\varrho(a))\subseteq \spec(a)$:
 when $a-\lambda$ is invertible,
so is~$\varrho(\,a-\lambda\,)\equiv\varrho(a)-\lambda$,
for any~$\lambda\in \C$.
Hence~$\varrho$ is positive.

It follows by~\sref{weak-russo-dye}
that~$\varrho$ is bounded, and~$\|\varrho(b)\|\leq \|b\|$
for \emph{self-adjoint} $b\in \scrA$.
It remains to be shown that~$\|\varrho(a)\|\leq \|a\|$
for arbitrary~$a\in\scrA$.
But since~$a^*a$ is self-adjoint for such~$a$,
we have $\|\varrho(a)\|^2\equiv\|\varrho(a^*a)\|\leq \|a^*a\|=\|a\|^2$
by the $C^*$-identity and using that~$\varrho$
    is a miu-map. Whence $\|\varrho(a)\|\leq\|a\|$
    for all~$a\in\scrA$, and so~$\|\varrho\|\leq 1$.\qed
\end{point}
\end{point}
\begin{point}{60}[cstar-isometry]{Lemma}%
For a pu-map $f\colon \scrA\to\scrB$
the following are equivalent.
\begin{enumerate}
\item\label{cstar-isometry-1}
$f$ is \Define{bipositive}%
\index{bipositive!map between $C^*$-algebras}%
, that is, $f(a)\geq 0$ iff $a\geq 0$
for all~$a\in\scrA$;
\item\label{cstar-isometry-2}%
$f$ is an isometry on~$\sa{\scrA}$, 
that is, $\|f(a)\|=\|a\|$ for all~$\in \sa{\scrA}$;
\item\label{cstar-isometry-3}
	$f$ is an isometry on~$\pos{\scrA}$.
\end{enumerate}
\begin{point}{70}{Proof}%
It is clear that \ref{cstar-isometry-2} implies~\ref{cstar-isometry-3}.
\begin{point}{80}{\ref{cstar-isometry-1}$\Longrightarrow$\ref{cstar-isometry-2}}%
Let~$a\in \sa{\scrA}$ be given.
Note that $-\lambda \leq a\leq \lambda$
iff $-\lambda \leq f(a) \leq \lambda$
for all~$\lambda \geq 0$,
because~$f$ is bipositive and unital.
In particular,
since~$-\|a\|\leq a\leq \|a\|$,
we have $-\|a\|\leq f(a)\leq \|a\|$,
and so~$\|f(a)\|\leq \|a\|$.
On the other hand,
$-\|f(a)\|\leq f(a)\leq \|f(a)\|$
implies $-\|f(a)\|\leq a\leq \|f(a)\|$,
and so $\|a\|\leq \|f(a)\|$.
Thus $\|a\|=\|f(a)\|$,
and $f$ is an isometry on~$\sa{\scrA}$.
\end{point}
\begin{point}{90}{\ref{cstar-isometry-3}$\Longrightarrow$\ref{cstar-isometry-1}}%
Let~$a\in \scrA$ be given.
We must show that~$f(a)\geq 0$ iff $a\geq 0$.
Since~$f$ is involution preserving (\sref{cstar-p-implies-i})
$a$ is self-adjoint iff $f(a)$ is self-adjoint,
and so we might as well assume that~$a$ is self-adjoint
to start with.
Since~$f$ is an isometry on~$\scrA_+$,
$\|a\|-a$ is positive,
and $f$ is unital,
we have $\|\,\|a\|-a\,\|=\|f(\|a\|-a)\|=\|\,\|a\|-f(a)\,\|$.
Now,
observe that
$0\leq a$
iff
$ \|\,\|a\|-a\,\|\leq \|a\|$,
and that
$\|\,\|a\|-f(a)\,\|\leq \|a\|$ 
iff $0\leq f(a)$,
by~\sref{positive-basic-2},
because $\frac{1}{2}\|a\|\leq \|a\|$
and $\frac{1}{2}\|f(a)\|\leq  \|a\|$
(by~\sref{weak-russo-dye}).\qed
\end{point}
\end{point}
\begin{point}{100}{Warning}%
Such a map~$f$ need not preserve the norm of arbitrary elements:
the map $A\mapsto \frac{1}{2}A+\frac{1}{2}A^T\colon M_2\to M_2$
is bipositive and unital,
but
\begin{equation*}
\left\|\left(\begin{matrix}0&1\\0&0\end{matrix}
\right)\right\|
\ = \ 1 \ \neq \ \frac{1}{2}\ = \ 
\left\|\,\left(\begin{matrix}0 & \nicefrac{1}{2} \\ 0 & 0
\end{matrix}\right)
\,+\,\left(\begin{matrix}0 & 0\\ \nicefrac{1}{2} & 0
\end{matrix}\right)\,\right\|.
\end{equation*}
(Even if~$f$ is completely positive, \sref{cp},
it might still only preserve the norm of self-adjoint elements
cf.~\sref{warning-norm-states}.)
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{201}
\begin{point}{10}[cstar-product-2]{Exercise}%
\index{product!in $\Cstar{miu}$ and $\cCstar{miu}$}
\index{product!in $\Cstar{pu}$}%
Show that
the product~$\bigoplus_{i\in I}\scrA_i$
of
a family $(\scrA_i)_{j\in I}$
of $C^*$-algebras
defined in~\sref{cstar-product}
is also the categorical product 
of these $C^*$-algebras
    in~$\Cstar{miu}$ and~$\cCstar{miu}$
with as projections
the maps~$\Define{\pi_j}\colon \bigoplus_{i \in I}\scrA_i\to\scrA_j$%
\index{pi@$\pi_j$, projection!in $\Cstar{miu}$}
given by~$\pi_j(a)=a(j)$.

(Hint: use here that
the projections $\pi_j$ are bounded by~\sref{norm-mi-map}.)

Show that the same description applies to~$\cCstar{pu}$
and $\cCstar{pu}$.
(Hint: first show that an element~$a$
of $\bigoplus_{i\in I}\scrA_i$ is positive
    iff $a(i)$ is positive for every~$i\in I$.)

We'll return to the product of $C^*$-algebras
a final time in~\sref{cstar-product-4}.
\end{point}
\begin{point}{20}[cstar-equaliser-1]{Exercise}%
\index{equaliser!in~$\Cstar{miu}$ and $\Cstar{pu}$}
Show that given miu-maps $f,g\colon \scrA\to\scrB$
between $C^*$-algebras
the collection $\scrE:=\{a\in\scrA\colon f(a)=g(a)\}$
is a $C^*$-subalgebra
    of~$\scrA$ (using the fact that~$f$ and~$g$
    are bounded by~\sref{norm-mi-map} to show that~$\scrE$ is closed.)
Show that the inclusion $e\colon \scrE\to\scrA$
is a (positive) miu-map
that is in fact the equaliser of~$f$ and~$g$
in~$\Cstar{miu}$ and~$\Cstar{pu}$.
Show that the same description applies
to~$\cCstar{miu}$ and~$\cCstar{pu}$.
\begin{point}{30}[cstar-no-pu-equalisers]{Remark}%
The assumption here that~$f$ and~$g$ are miu-maps
is essential:
the pair of pu-maps $f,g\colon \C^4\to\C$
given by
\begin{equation*}
    \textstyle
f(a,b,c,d)\,=\, \frac{1}{2}(a+b),
    \quad \text{and}\quad
g(a,b,c,d)\,=\, \frac{1}{2}(c+d),
\end{equation*}
for example,
has no equaliser
in~$\Cstar{pu}$,
as we'll show in~\sref{cstar-no-pu-equalisers-example}.
\end{point}
\end{point}
\end{parsec}

\begin{parsec}{210}%
\begin{point}{10}%
We just saw in~\sref{cstar-isometry} that a
map on a $C^*$-algebra~$\scrA$
that preserves and reflects the order
determines the norm of the self-adjoint
--- but not all --- elements of~$\scrA$.
This theme, to what extent a linear map
(or a collection of linear maps)
on a $C^*$-algebra
determines its structure,
while tangential at the moment,
will
grow ever more important 
until it is essential for the theory 
of von Neumann algebras.
That's why we introduce
the four levels
of discernment
that a collection of maps on a $C^*$-algebra
might have already here.
\end{point}
\begin{point}{20}[separating]{Definition}%
A collection~$\Omega$ of linear maps on a $C^*$-algebra~$\scrA$
will be called
\begin{enumerate}
\item
\label{separating-1}
\Define{order separating}%
\index{order separating collection!of maps on a $C^*$-algebras}
if an element~$a$ of~$\scrA$
is positive iff $0\leq \omega(a)$
for all~$\omega\in \Omega$;
\item
    \label{separating-2}
\Define{separating}%
\index{separating collection!of maps on a $C^*$-algebra} 
if an element~$a$ of~$\scrA$
is zero iff $\omega(a)=0$ for all~$\omega\in\Omega$;
\item
    \label{separating-3}
\Define{faithful} if an element~$a$ of~$\scrA_+$%
\index{faithful collection!of maps on a $C^*$-algebra}
is zero iff~$\omega(a)=0$ for all~$\omega\in\Omega$; and
\item
    \label{separating-4}
\Define{centre separating}%
\index{centre separating collection!of maps on a $C^*$-algebra}
if $a\in\scrA_+$
is zero iff $\omega(b^*ab)=0$ for all~$\omega\in\Omega$
and~$b\in \scrA$.

(The ``centre'' in ``centre separating''
will be explained in~\sref{vn-center-separating}.)
\end{enumerate}
(Note that 
    $\text{\eqref{separating-1}}
\implies
    \text{\eqref{separating-2}}
\implies
    \text{\eqref{separating-3}}
\implies
    \eqref{separating-4}$.)
\end{point}
\begin{point}{30}{Examples}%
We'll see later on that the following
collections
are order separating.
\begin{enumerate}
\item
The set of all pu-maps $\omega\colon \scrA\to\C$
on a $C^*$-algebra
(see~\sref{states-order-separating}).
\item
The set of all miu-maps $\omega\colon \scrA\to\C$
on a commutative $C^*$-algebra
(see~\sref{gelfand-representation-isometry}).
\item
The set of functionals on~$\scrB(\scrH)$,
where~$\scrH$ is a Hilbert space,
of the form 
$\left<x,(\,\cdot\,)x\right>
\colon \scrB(\scrH)\to\C$ where $x\in \scrH$
(see~\sref{hilb-vector-states-order-separating}).

We'll call these functionals
\Define{vector functionals}%
\index{vector functional!for a Hilbert space}%
\index{functional!vector}.
(They are clearly bounded
and involution preserving linear maps,
and once we know that each positive element of a $C^*$-algebra
is a square, in~\sref{sqrt},
it'll be obvious that vector functionals
are positive too.)
\end{enumerate}
\begin{point}{40}
None of the four levels of separation
coincide.
This follows from the following examples,
that we'll just mention here,
but can't verify yet.
\begin{enumerate}
\item
A single non-zero vector~$x$ from a Hilbert space~$\scrH$
gives a vector functional $\left<x,(\,\cdot\,)x\right>$
on~$\scrB(\scrH)$ that is centre separating
on its own, but is not faithful when~$\scrH$ has dimension~$\geq 2$.
\item
Given an orthonormal basis~$\scrE$
of a Hilbert space~$\scrH$
		the collection 
\begin{equation*}
	\{\,\left<e,(\,\cdot\,)e\right>\colon\,e\in\scrE\,\}
\end{equation*}
		of vector functionals on~$\scrB(\scrH)$
		is faithful, but not separating when~$\scrE$
		has more than one element.
\item
Given Hilbert spaces~$\scrH$
and~$\scrK$
the set of vector functionals
\begin{equation*}
	\{\,\left<\,x\otimes y,\,(\,\cdot\,)\,x\otimes y\,\right>\colon\,
x\in\scrH,\,y\in\scrK\,\}
\end{equation*}
on~$\scrB(\scrH\otimes\scrK)$
is separating,
but not order separating
when both~$\scrH$ and~$\scrK$
are at least two dimensional.
\end{enumerate}
\end{point}
\end{point}
\begin{point}{50}[separating-self-adjoint]{Exercise}%
One use for 
a separating collection~$\Omega$
of involution preserving maps
on a $C^*$-algebra~$\scrA$
is checking whether an element~$a\in\scrA$
is self-adjoint:
show that $a\in\scrA$ is self-adjoint
iff $\omega(a)$ is self-adjoint for all~$\omega\in\Omega$.
\end{point}
\begin{point}{60}%
An order separating
collection senses the norm 
of a self-adjoint element:
\end{point}
\begin{point}{70}[order-separating-norm]{Proposition}%
For a collection~$\Omega$ of pu-maps on a $C^*$-algebra~$\scrA$
the following are equivalent.
\index{order separating collection!of pu-maps on a $C^*$-algebra}
\begin{enumerate}
\item 
	$\Omega$ is order separating;
\item
	$\|a\|= \sup_{\omega\in\Omega} \left\|\omega(a)\right\|$
	for all $a\in \sa{\scrA}$;
\item
	$\|a\| = \sup_{\omega\in\Omega} \left\|\omega(a)\right\|$
	for all~$a\in \pos{\scrA}$.
\end{enumerate}
\begin{point}{80}{Proof}%
Denoting the codomain of~$\omega\in\Omega$
by~$\scrB_\omega$
(so that $\omega\colon \scrA\to\scrB_\omega$),
apply~\sref{cstar-isometry}
to the pu-map $\left<\omega\right>_{\omega\in\Omega}\colon 
\scrA\to\bigoplus_{\omega\in\Omega}
\scrB_\omega$ (see~\sref{cstar-product-2}).\qed
\end{point}
\begin{point}{90}[warning-norm-states]{Warning}%
The formula
$\|a\|=\sup_{\omega\in\Omega} \|\omega(a)\|$
need not be correct
for an arbitrary (not necessarily self-adjoint)
element~$a$.
Indeed,
consider the matrix $A:=\smash{%
\bigl(\begin{smallmatrix}0&1\\0&0\end{smallmatrix}\bigr)}$,
and the collection $\Omega=\{\,\left<x,(\,\cdot\,)x\right>\colon 
x\in \C^2,\,\|x\|=1\,\}$,
which will turn out to be order separating.
We have $\|A\|=1$,
while $\left| \left<x,\omega(A)x\right>\right|
 =\left|x_1\right|\left|x_2\right|$
 never exceeds~$\nicefrac{1}{2}$
for $x\equiv (x_1,x_2)\in \scrH$ with $1=\|x\|$.
\end{point}
\end{point}
\begin{point}{100}[order-separating-dense-subset]{Exercise}%
Show that any operator norm dense subset~$\Omega'$
of an order separating collection~$\Omega$
of positive functionals
on a $C^*$-algebra~$\scrA$
is order separating too.
\end{point}
\end{parsec}
\begin{parsec}{220}%
\begin{point}{10}%
We'll use~\sref{order-separating-norm}
to show 
that the pu-maps $\omega\colon \scrA\to\C$
on a $C^*$-algebra~$\scrA$
(called \Define{states}%
\index{state of a $C^*$-algebra}
of~$\scrA$ for short)
are order separating
by showing that
for every self-adjoint element~$a\in \scrA$
there is a state~$\omega$ of~$\scrA$ with $\omega(a)=\|a\|$ or 
$\omega(a)=-\|a\|$.
To obtain such a state
we first find its kernel,
which leads us to the following definitions.
\end{point}
\begin{point}{20}{Definition}%
An \Define{order ideal}%
\index{order ideal of a $C^*$-algebra}
of a $C^*$-algebra~$\scrA$
is a linear subspace~$I$ of~$\scrA$
with $b\in I\implies b^*\in I$
and $b\in I\cap\pos{\scrA}\implies [-b,b]
    \equiv \{\,a\in\scrA\colon \,-b\leq a\leq b\, \} \ \subseteq\, I$.

The order ideal~$I$ is called \Define{proper}%
\index{order ideal of a $C^*$-algebra!proper}
if~$1\notin I$,
and \Define{maximal} 
\index{order ideal of a $C^*$-algebra!maximal}
if it is maximal among all proper order ideals.
\begin{point}{21}{Warning}%
``Order ideals'' like ``subspaces'' appear in relation
to other structures as well,
with appropriately varying meanings.
Our definition for $C^*$-algebras is based on
to the order ideals for order unit spaces
from Definition~2.2 of~\cite{kadison1951}.
\end{point}
\end{point}
\begin{point}{30}[order-ideal-basic]{Exercise}%
Let~$\scrA$ be a $C^*$-algebra.
\begin{enumerate}
\item
Show that the kernel of a state is a maximal order ideal.

(Hint: the kernel of a state is already maximal as linear subspace.)
\item
Let~$I$ be a proper order ideal of~$\scrA$.
Show that there is a maximal 
order ideal~$J$ of~$\scrA$ with $I\subseteq J$.
(Hint: Zorn's Lemma may be useful.)
\item
Let~$a\in \sa{\scrA}$.
Show that there is a least order ideal~\Define{$(a)$}%
\index{$(a)$, order ideal generated by $a$}
that contains~$a$,
and that given~$b\in\Real{\scrA}$
we have $b\in (a)$
iff there are $\lambda,\mu\in \R$
with~$\lambda a\leq b\leq \mu a$.

Show that~$(a)=\C a$
when~$0\nleq a\nleq 0$.

Show that~$1\in (a)$ if and only if $a$ is invertible
and either $0\leq a$ or $a\leq 0$.

\item
Let~$a$ be a self-adjoint element of~$\scrA$ which
is not invertible.
Show that there is a maximal order ideal~$J$
of~$\scrA$
with $a\in J$.

\item
Let~$a$ be a self-adjoint element of~$\scrA$.
Show that  $\|a\|-a$
or $\|a\|+a$ is not invertible
(perhaps by considering the spectrum of~$a$.)
\end{enumerate}
\end{point}
\begin{point}{40}[maximal-ideal-state]{Lemma}%
For every maximal order ideal~$I$ of a $C^*$-algebra~$\scrA$,
 there is a state $\omega \colon \scrA\to \C$
with $\ker(\omega)=I$.
\begin{point}{50}{Proof}%
Form the quotient vector space $\scrA/I$
with quotient map $q\colon \scrA\to \scrA/I$.
Note that since~$1\notin I$
we have $q(1)\neq 0$
and so we may regard~$\C$ 
to be a linear subspace of~$\scrA/I$
via $\lambda\mapsto q(\lambda)$.
We will, in fact, show that~$\C=\scrA/I$.

But let us first put an order on~$\scrA/I$:
we say that $\mathfrak{a}\in \scrA/I$ is positive
if $\mathfrak{a}\equiv q(a)$ for some~$a\in\pos{\scrA}$,
and write $\mathfrak{a}\leq \mathfrak{b}$ 
if $\mathfrak{b}-\mathfrak{a}$ is positive
for $\mathfrak{a},\mathfrak{b}\in\scrA/I$.
Note that the definition of ``order ideal'' is such
that if both~$\mathfrak{a}$ and $-\mathfrak{a}$ are positive,
then~$\mathfrak{a}=0$.
We leave it to the reader to verify 
that~$\scrA/I$ becomes a partially ordered vector space
with the order defined above.
There is, however,
one detail we'd like to draw attention to,
namely that a scalar $\lambda$ is positive in~$\scrA/I$
iff $\lambda$ is positive in~$\C$.
Indeed, if~$\lambda\geq 0$ in~$\C$,
then $\lambda\geq 0$ in~$\scrA$, and so~$\lambda \geq 0$ in~$\scrA/I$.
On the other hand,
if~$\lambda\geq 0$ in~$\scrA/I$, but~$\lambda\leq 0$ in~$\C$,
then $\lambda\leq 0$ in~$\scrA/I$,
and so $\lambda=0$.
This detail
has the pleasant consequence
that once we have shown that~$\scrA/I=\C$,
we automatically get that~$q\colon \scrA\to\C$ is positive.

\begin{point}{60}[pos-hahn-banach-1]%
Let~$a\in \sa{\scrA}$ be given.
Define~$\alpha := \inf\{\,\lambda\in\R\colon\, q(a)\leq \lambda\,\}$.
Note that $-\|a\| \leq \alpha\leq \|a\|$.
We will prove that~$q(a)=\alpha$
by considering the order ideal
\begin{alignat*}{3}
	J\ := \ \{\,b\in \scrA\colon\, 
		& \exists\lambda,\mu\in\R\,[\ 
	\lambda (\alpha-q(a))\,\leq\, \Real{b}\,\leq\, \mu 
(\alpha-q(a))\ ]\,\wedge\,
		\\
		&\exists\lambda,\mu\in \R\,[\ 
\lambda (\alpha-q(a))\,\leq\, \Imag{b}\,\leq\, \mu (\alpha-q(a)) \ ] \,\}.
\end{alignat*}
We claim that $1\notin J$.
Indeed, suppose not---towards a contradiction.
Then there is~$\mu\in \R$
with $1\leq \mu (\alpha-q(a))$.
What can we say about~$\mu$?
If~$\mu <0$,
then $0\geq \nicefrac{1}{\mu}\geq \alpha-q(a)$,
so~$\alpha-\nicefrac{1}{\mu} \leq q(a)$,
but $q(a)\leq \alpha+\varepsilon$
for every~$\varepsilon>0$,
and so~$\alpha-\nicefrac{1}{\mu}\leq q(a)\leq \alpha-\nicefrac{1}{2\mu}$,
which is absurd.
If $\mu=0$,
then we get $1\leq \mu (\alpha-q(a))\equiv 0$, which is absurd.
If $\mu> 0$,
then $\nicefrac{1}{\mu}\leq \alpha-q(a)$,
or in other words,
 $q(a) \leq \alpha - \nicefrac{1}{\mu}$,
 giving $\alpha \leq \alpha-\nicefrac{1}{\mu}$
by definition of~$\alpha$,
which is absurd.
Hence~$1\notin J$.

But then since~$I\subseteq J$,
we get~$I=J$, by maximality of~$I$.
Thus, as $\alpha-a\in J$, we have $\alpha-a\in I$,
and so $q(a)=\alpha$, as desired.
\end{point}
\begin{point}{70}%
Let~$a\in \scrA$ be given.
Then~$a=\Real{a}+i\Imag{a}$.
By~\sref{pos-hahn-banach-1},
there are $\alpha,\beta\in \R$ with $q(\Real{a})=\alpha$,
and $q(\Imag{a})=\beta$.
Thus~$q(a)=\alpha+i\beta$.
Hence~$\scrA/I=\C$.
Since the quotient map $q\colon \scrA\to \scrA/I\equiv \C$
is pu, and $\ker(q)=I$, we are done.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{80}[states-order-separating]{Exercise}%
\index{state of a $C^*$-algebra!order separating}%
Show using~\sref{maximal-ideal-state} that given a self-adjoint element~$a$
of a $C^*$-algebra~$\scrA$
there is a state~$\omega$ with $\left|\omega (a)\right| = \|a\|$.
Conclude that the set of states of a $C^*$-algebra
is order separating (see~\sref{separating}).
\end{point}
\end{parsec}
\subsection{The Square Root}
\begin{parsec}{230}%
\begin{point}{10}%
The key that unlocks the remaining basic facts 
about the (positive) elements of a  $C^*$-algebra
is the existence of the square root~$\sqrt{a}$ of a positive element~$a$,
and its properties.
For technical reasons,
we will assume $\|a\|\leq 1$,
and construct
 $1-\sqrt{1-a}$ instead of~$\sqrt{a}$.
\end{point}
\begin{point}{20}{Lemma}%
Let $a$ be an element of a $C^*$-algebra $\scrA$
with $0\leq a\leq 1$.
Then there is a unique element~$b\in\scrA$ 
with, $0\leq b\leq 1$,
$ab=ba$,
and~$(1-b)^2 = 1-a$.
To be more specific,
$b$ is the norm limit of
the sequence $b_0\leq b_1\leq \dotsb$
given by $b_0=0$ and $b_{n+1} = \frac{1}{2}(a+b_n^2)$.
Moreover,
if~$c\in\scrA$ commutes with~$a$, then~$c$ commutes with~$b$,
and if in addition $a\leq 1-c^2$ and $c^*=c$,
we have $b\leq 1-c$.
\begin{point}{30}{Proof}%
When discussing $b_n$ it 
is convenient to write~$b_n \equiv q_n(a)$
where~$q_0,q_1,\dotsc$ are the polynomials over~$\R$ given by
$q_0=0$ and $q_{n+1}=\frac{1}{2}(x + q_n^2)$.
For example,
we have~$b_n\geq 0$, 
because all coefficients of~$q_n$ are all positive,
and $a,a^2,a^3,\dotsc$ are positive by~\sref{positive-basic-2}.
With a similar argument we can see that
 $b_0 \leq b_1\leq b_2\leq \dotsb$.
Indeed, 
the coefficients of~$q_{n+1}-q_n$
are positive,
by induction,
because
\begin{alignat*}{3}
q_{n+2}-q_{n+1} \ &=\ \textstyle \frac{1}{2}(x+ q_{n+1}^2)
\,-\, \textstyle\frac{1}{2}(x+q_n^2) \\
&=\ \textstyle\frac{1}{2}(q_{n+1}^2- q_n^2) \\
&=\ \textstyle\frac{1}{2}(q_{n+1}+q_n)(q_{n+1}-q_n) \\
&=\ (q_n+\textstyle\frac{1}{2}(q_{n+1}-q_n))(q_{n+1}-q_n),
\end{alignat*}
has positive coefficients
if~$q_{n+1}-q_n$ has positive coefficients,
and $q_1-q_0\equiv \frac{1}{2}x$ clearly has positive coefficients.
Hence~$b_{n+1}-b_{n} = q_{n+1}(a)- q_n(a)$ is positive.
(Note that we have carefully avoided
using the fact here that the product of positive 
commuting elements is positive,
which is not available to us until~\sref{ineq-square-root}.)

Let us now show that~$b_0\leq b_1\leq \dotsb$ converges.
Let~$n\geq N$ from~$\N$ be given.
Since the coefficients of $q_n-q_N$ are positive,
and $\|a\|\leq 1$,
the triangle inequality gives us
$\|b_n-b_N\|\equiv \|(q_n-q_N)(a)\|\leq q_n(1)-q_N(1)$,
and
so it suffices to 
show that the ascending sequence
 $q_0(1)\leq q_1(1)\leq \dotsb$
of real numbers
converges,
i.e.~is bounded.
Indeed,
we have $q_n(1)\leq 1$,
by induction,
because $q_{n+1}(1)\equiv \frac{1}{2}(1+q_n(1)^2)
\leq 1$ if $q_n(1)\leq 1$,
and clearly $0\equiv q_0(1)\leq 1$.

Let~$b$ be the limit of $b_0\leq b_1\leq\dotsb$.
Then~$b$ being the limit of positive elements
is positive
(see~\sref{positive-basic-2}),
and if $c\in \scrA$ commutes with~$a$,
then $c$ commutes with all powers of~$a$,
and therefore with all~$b_n$,
and thus with~$b$.
Further, 
from the recurrence relation $q_{n+1} = \frac{1}{2}(a+q_n^2)$
we get $b=\frac{1}{2}(a+b^2)$,
and so $-a = -2b+b^2$, 
giving us  $(1-b)^2 = 1-2b+b^2 = 1-a$.

Let us prove that~$b\leq 1$.
To begin, note that~$\|b_n\|\leq 1$ for all~$n$, by induction,
because $0\equiv \|b_0\|\leq 1$,
and if $\|b_n\|\leq 1$, then $\|b_{n+1}\|\leq \frac{1}{2}(\|a\|+\|b_n\|^2)
\leq 1$, since $\|a\|\leq 1$.
Since~$b_n\geq 0$, we get $-1\leq b_n\leq 1$ for all~$n$,
and so $b\leq 1$.

\begin{point}{40}[square-commuting-monotone]%
Let us take a step back for the moment.
From what we have proven so far
we see that each positive $c\in\scrA$
is of the form $c\equiv d^2$ for some positive~$d\in\scrA$
which commutes with all~$e\in \scrA$ that commute with~$c$.

From this we can see that $c_1c_2\geq 0$
for  
 $c_1,c_2 \in\pos{\scrA}$
with $c_1c_2 = c_2c_1$.
Indeed, writing $c_i\equiv d_i^2$ with $d_i$ as above,
we have $d_1c_2=c_2d_1$ (because $c_1c_2=c_2c_1$), and thus 
$d_1d_2=d_2d_1$. It follows that $d_1d_2$ is self-adjoint,
and $c_1c_2 = (d_1d_2)^2$. Hence $c_1c_2\geq 0$.

We will also need the following corollary.
For~$c,d\in\pos{\scrA}$ with $c\leq d$ and $cd=dc$,
we have $c^2\leq d^2$.
Indeed, $d^2-c^2 \equiv d(d-c)+c(d-c)$
is positive by the previous paragraph.
\end{point}
\begin{point}{50}[ineq-square-root]%
Let~$c\in\sa{\scrA}$ be such that~$ca=ac$ and  $a\leq 1-c^2$.
We must show that $b\leq 1-c$.
Of course,
since~$b$ is the limit of $b_1,b_2,\dotsc$,
it suffices to show that~$b_n\leq 1-c$,
and we'll do this by induction.
Since $0\leq c^2 \leq 1-a$,
 we have $\|c\|^2\leq \|1-a\|\leq 1$,
and so $-1\leq c\leq 1$.
Thus $b_0\equiv 0\leq 1-c$.
Now, suppose that~$b_n\leq 1-c$ for some~$n$.
Then $b_{n+1} = \frac{1}{2}(a+b_n^2)
\leq \frac{1}{2}( (1-c^2)+(1-c)^2) = 1-c$,
where we have used that $b_n^2 \leq (1-c)^2$,
because $b_n\leq 1-c$
by~\sref{square-commuting-monotone}.
\begin{point}{60}%
We'll now show that~$b$ is unique
in the sense that $b=b'$
for any~$b'\in \scrA$ with $0\leq b'\leq 1$,
 $b'a=ab'$ and $(1-b')^2=1-a$.
Note that $b'\leq 1$,
because $\|1-b'\|^2=\|1-a\|\leq 1$,
From $a=1-(1-b')^2$,
we immediately get $b \leq 1-(1-b')=b'$ by~\sref{ineq-square-root}.
For the other direction,
note that
$(1-b')^2= (1-b)^2 \equiv (1-b'+(b'-b))^2 = (1-b')^2+2(1-b')(b'-b)+(b'-b)^2$,
which gives $0=2(1-b')(b'-b)+(b'-b)^2$.
Now, since~$1-b'$ and $b'-b$ are positive,
and commute, we see that $(1-b')(b'-b)$ is positive 
by~\sref{ineq-square-root}, and so 
 $0=2(1-b')(b'-b)+(b'-b)^2\geq (b'-b)^2 \geq 0$,
which entails $(b'-b)^2=0$, and so $\|(b'-b)^2\|=\|b'-b\|^2=0$,
yielding $b=b'$.\qed
\end{point}
\end{point}
\end{point}
\end{point}
\begin{point}{70}[sqrt]{Exercise}%
\index{*sqrt@$\sqrt{a}$, square root!in a $C^*$-algebra}%
Let~$a$ be a positive element of a $C^*$-algebra~$\scrA$.
Show that there is a unique 
positive element of~$\scrA$
denoted by $\Define{\sqrt{a}}$ 
(and by~$\Define{a^{\nicefrac{1}{2}}}$)
with $\smash{\sqrt{a}^2}=a$
and $a\sqrt{a}=\sqrt{a}a$.
Show that if~$c\in\scrA$ commutes with~$a$,
then $c\sqrt{a}=\sqrt{a}c$,
and if in addition $c^*=c$ and $c^2\leq a$,
then $c\leq \sqrt{a}$.
Using this, verify:
\begin{enumerate}
\item
If~$a,b\in \scrA$ are positive,
and~$ab=ba$,
then $ab\geq 0$.

\item
Let~$a\in\pos{\scrA}$.
If $b,c\in \sa{\scrA}$ commute with~$a$,
then $b\leq c$ implies $ab\leq ac$.

\item
If~$a,b\in\Real{\scrA}$ commute, and~$a\leq b$, then~$a^2\leq b^2$.

\item
The requirement in the previous item  that~$a$ and~$b$ commute is essential:
there are positive elements $a$, $b$ of a $C^*$-algebra~$\scrA$
with $a\leq b$, but $a^2 \nleq b^2$.

In other words, the square $a\mapsto a^2$
on the positive elements of a $C^*$-algebra
need not be monotone,
(but $a\mapsto \sqrt{a}$ \emph{is} monotone, see~\sref{sqrt-monotone}).

(Hint: take $a=(\begin{smallmatrix}1&0\\0&0\end{smallmatrix})$
and $b=a+\frac{1}{2}(\begin{smallmatrix}1&1\\1&1\end{smallmatrix})$
from~$M_2$.)
\end{enumerate}
\end{point}
\end{parsec}
\begin{parsec}{240}
\begin{point}{10}{Definition}
Given a self-adjoint element~$a$ of a $C^*$-algebra $\scrA$,
we write
\begin{equation*}
\textstyle
\Define{\left|a\right|}\ :=\ \sqrt{a^2}
\qquad
\Define{\pos{a}}\ :=\ \frac{1}{2}(\left|a\right| + a)
\qquad
\Define{a_{-}}\ :=\ \frac{1}{2}(\left|a\right| - a).
\end{equation*}%
\index{$(\,\cdot\,)_+$, positive part!$a_+$, of a self-adjoint element of a $C^*$-algebra}
We call $a_+$ the \Define{positive part} of~$a$,
and $a_-$ the \Define{negative part}.
\index{$(\,\cdot\,)_-$, negative part!$a_-$, of a self-adjoint element of a $C^*$-algebra}
\end{point}
\begin{point}{20}[cstar-pos-neg-part]{Exercise}%
Let~$a$ be a self-adjoint element of a
 $C^*$-algebra $\scrA$.
\begin{enumerate}
\item
Show that $-\left|a\right| \leq a \leq \left| a \right|$,
and $\|\,\left|a\right|\,\|= \|a\|$.
\item
Prove that $a_+$ and $a_-$ are positive,  $a=a_+-a_-$
and $a_+a_-=a_-a_+=0$.
\item
One should not read too much into the notation
$\left|\,\cdot\,\right|$
in the non-commutative case:
give an example of
self-adjoint elements~$a$ and~$b$ of a $C^*$-algebra with
 $\left|a+b\right|\nleq \left|a\right|+ \left|b\right|$.

(Hint: one may take  
$a=\frac{1}{2}\left(\begin{smallmatrix}1 & 1 \\ 1 & 1\end{smallmatrix}\right)$
and $b=-\left(\begin{smallmatrix}1 & 0 \\ 0 & 0 \end{smallmatrix}\right)$.)
\end{enumerate}
\end{point}
\begin{point}{30}%
The existence of positive and negative parts
in a $C^*$-algebra
has many pleasant and subtle consequences
of which we'll now show one.
\end{point}
\begin{point}{40}[astara-positive]{Lemma}%
Given an element $a$ of a $C^*$-algebra $\scrA$,
we have $a^*a\geq 0$.
\begin{point}{50}{Proof}%
Writing $b:=a((a^*a)_-)^{\nicefrac{1}{2}}$,
we have $b^*b=
((a^*a)_-)^{\nicefrac{1}{2}} a^*a
((a^*a)_-)^{\nicefrac{1}{2}}
= (a^*a)_- \,a^*a
=
-((a^*a)_-)^2\leq 0$,
and so
$b=0$
by~\sref{astara-non-negative}.
Hence~$(a^*a)_-=0$ giving us $a^*a=(a^*a)_+\geq 0$.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{250}
\begin{point}{10}[cstar-positive-final]{Exercise}%
\index{positive!element of a $C^*$-algebra}%
Round up our results regarding positive elements
to 
prove that
the following are equivalent
for a self-adjoint element $a$ of a $C^*$-algebra~$\scrA$.
\begin{enumerate}
\item 
$a$ is positive, that is,  $\|a-t\|\leq t$
for some $t\geq \frac{1}{2}\|a\|$;
\item
$\|a-t\|\leq t$ for all~$t\geq \frac{1}{2}\|a\|$;
\item
$a\equiv b^2$ for some self-adjoint $b\in\scrA$;
\item
$a\equiv c^* c$ for some $c\in\scrA$;
\item
$\spec(a)\subseteq [0,\infty)$.
\end{enumerate}
\end{point}
\begin{point}{20}[astara-pos-basic-consequences]{Exercise}%
The fact that $a^*a$ is positive
for an element~$a$ of a $C^*$-algebra~$\scrA$
has some nice consequences
of its own needed later on.
\begin{enumerate}
\item
Show that $b\leq c\implies a^*ba \leq a^*ca$
for all~$b,c\in\sa{\scrA}$ and~$a\in\scrA$.
\item
Show that every mi-map and cp-map is positive.
\item
Show that~$a\leq b^{-1}$ 
iff $\sqrt{b}a\sqrt{b}\leq 1$
iff $\|\sqrt{a}\sqrt{b}\|\leq 1$
iff $b\leq a^{-1}$
for positive invertible elements $a$, $b$ of~$\scrA$
(and so $a\leq b$ entails $b^{-1}\leq a^{-1}$).
\item
Prove that $(1+a)^{-1}a\leq (1+b)^{-1}b$
for $0\leq a\leq b$ from~$\scrA$.\\
(Hint: add $(1+a)^{-1} + (1+b)^{-1}$
to both sides of the inequality.)
\end{enumerate}
\end{point}
\begin{point}{30}[hilb-vector-states-order-separating]{Proposition}%
The vector states
of~$\scrB(\scrH)$
are order separating (see~\sref{separating})
for every Hilbert space~$\scrH$.
\begin{point}{40}{Proof}%
By\sref{order-separating-norm}
it suffices to show
that~$\|T\|=  \sup_{x\in (\scrH)_1} 
\left|\left< x,Tx\right>\right|$
for given~$T\in\scrB(\scrH)_+$.
Since $\left|\left<x,Tx\right>\right|
=\left<T^{\nicefrac{1}{2}}x,T^{\nicefrac{1}{2}}x\right>
=\|T^{\nicefrac{1}{2}}x\|^2$
for all~$x\in \scrH$, 
we have $
\|T\| = \|T^{\nicefrac{1}{2}}\|^2
=(\,\sup_{x\in (\scrH)_1}\left\|T^{\nicefrac{1}{2}}x\right\|\,)^2
=\sup_{x\in (\scrH)_1} \left|\left<x,Tx\right>\right|$.\qed
\end{point}
\end{point}
\begin{point}{50}[hilb-positive-operators]{Corollary}%
For a bounded operator~$T$
on a Hilbert space~$\scrH$, we have
\begin{enumerate}
\item
$T$ is self-adjoint iff $\left<x,Tx\right>$
is real for all~$x\in (\scrH)_1$;
\item
$0\leq T$ iff $0\leq\left<x,Tx\right>$
for all~$x\in (\scrH)_1$;
\item
	$\|T\|=\sup_{x\in (\scrH)_1}\left|\left<x,Tx\right>\right|$
when~$T$ is self-adjoint.
\end{enumerate}
\begin{point}{60}{Proof}%
This follows from~\sref{separating-self-adjoint} 
and~\sref{order-separating-norm}
because the vector states on~$\scrB(\scrH)$
are order separating 
by~\sref{hilb-vector-states-order-separating}.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{260}%
\begin{point}{10}%
The interaction between the multiplication and order
on a $C^*$-algebra can be subtle,
but
when the $C^*$-algebra is commutative
almost all peculiarities disappear.
This is to be expected
as any commutative $C^*$-algebra
is isomorphic to a $C^*$-algebra
of continuous functions on a compact Hausdorff space
(as we'll see in~\sref{gelfand}).
\end{point}
\begin{point}{20}[commutative-cstar-basic]{Exercise}%
Let~$\scrA$ be a \emph{commutative} $C^*$-algebra.
Let~$a,b,c\in\sa{\scrA}$.
\begin{enumerate}
\item
Show that $\left| a\right|$ is the supremum of~$a$ and~$-a$
in~$\sa{\scrA}$.
\item
Show that if~$a$ and $b$ have a supremum, $a\vee b$, in $\sa{\scrA}$,
then~$c\,+\,a\vee b$ is the supremum of~$a+c$ and $b+c$.
\item
Show that~$\sa{\scrA}$ is a \Define{Riesz space},
that is,  a lattice ordered vector space.\\
(Hint: prove that $\frac{1}{2}(a+b+\left|a-b\right|)$
is the supremum of~$a$ and~$b$ in~$\sa{\scrA}$.)
\item
Show that a miu-map $f\colon \scrA\to\scrB$
between commutative $C^*$-algebras
preserves finite suprema and infima.
\end{enumerate}
\end{point}
\begin{point}{30}[riesz-decomposition-lemma]{Exercise}%
Prove the \Define{Riesz decomposition lemma}:%
\index{Riesz decomposition lemma}
For positive elements~$a,b,c$ of a commutative $C^*$-algebra~$\scrA$
with~$c\leq a+b$
we have $c\equiv a'+b'$
where  $0\leq a'\leq a$ and $0\leq b'\leq b$.
\end{point}
\end{parsec}
\section{Representation}
\subsection{\dots by Continuous Functions}
\begin{parsec}{270}%
\begin{point}{10}%
Now that we have have a firm grip
on the positive elements of a $C^*$-algebra
we turn to what is perhaps the most important
fact about commutative $C^*$-algebras:
that they are isomorphic to $C^*$-algebras
of continuous functions on a compact Hausdorff space,
via the \emph{Gelfand representation}.
\end{point}
\begin{point}{20}{Setting}%
$\scrA$ is a commutative $C^*$-algebra.
\end{point}
\begin{point}{30}[gelfand-representation]{Definition}%
The \Define{spectrum} of~$\scrA$,%
\index{sp@$\spec$, spectrum!$\spec(\scrA)$, of a $C^*$-algebra}
denoted by \Define{$\spec(\scrA)$},
is the set of all miu-maps $f\colon \scrA\to \C$.
We endow~$\spec(\scrA)$
with the topology of pointwise convergence.

The \Define{Gelfand representation}
of~$\scrA$
is the miu-map~$\gamma\colon \scrA\to C(\spec(\scrA))$%
\index{Gelfand representation@$\gamma$, Gelfand representation}
given by $\gamma(a)(f)=f(a)$.
\end{point}
\begin{point}{40}[gelfand-representation-basic]{Exercise}%
Verify that 
 the map $\spec(\scrA)\to \C,\ f\mapsto f(a)$ is indeed
continuous for every~$a\in\scrA$,
and that~$\gamma$ is miu.
\end{point}
\begin{point}{50}{Remark}
One might wonder if there is any connection between
the spectrum~$\spec(\scrA)$
of a commutative $C^*$-algebra,
and the spectrum~$\spec(a)$
of one of~$\scrA$'s elements (from~\sref{spectrum-of-element});
and indeed there is
as we'll see in~\sref{spectrum-miu}
(and~\sref{functional-calculus}).
\end{point}
\begin{point}{60}%
Our program for this paragraph is to show that
the Gelfand representation~$\gamma$ is 
a miu-isomorphism.
In fact,
we will show that it gives the unit
of an equivalence between the category of commutative $C^*$-algebras
(with miu-maps)
and the opposite of the category of compact Hausdorff spaces
(with continuous maps).
The first hurdle we take is the injectivity of~$\gamma$
--- that there are sufficiently many
points in the spectrum of a commutative $C^*$-algebra,
so to speak ---,
and involves
the following special type of order ideal.
\end{point}
\begin{point}{70}{Definition}%
A \Define{Riesz ideal}%
\index{Riesz ideal}
of~$\scrA$
is an order ideal~$I$
such that $a\in I\cap\sa{\scrA}\implies \left|a\right|\in I$.
A \Define{maximal Riesz ideal}%
	\index{Riesz ideal!maximal}
is a proper Riesz ideal which is maximal among
proper Riesz ideals.
\end{point}
\begin{point}{80}[riesz-ideal-ring-ideal]{Lemma}%
Let~$I$ be a Riesz ideal of~$\scrA$.
For all~$a\in \scrA$ and $x\in I$ we have $ax\in I$.
\begin{point}{90}{Proof}%
Since~$x=\Real{x}+i\Imag{x}$,
it suffices to show that~$a\Real{x}\in I$ and $a\Imag{x}\in I$.
Note that~$\Real{x},\Imag{x}\in I$,
so we might as well assume that~$x$ is self-adjoint to begin with.
Similarly, using that
 $\pos{x}\in I$ (because $\pos{x}=\frac{1}{2}(\left|x\right|+x)$
and~$\left|x\right|\in I$) and $x_-\in I$,
we can reduce the problem to the case that~$x$ is positive.
We may also assume that~$a$ is self-adjoint.
Now, since~$x\geq 0$ and $-\|a\|\leq a\leq \|a\|$,
we have $-\|a\|x \leq ax\leq \|a\|x$
by~\sref{sqrt},
and so~$ax\in I$,
because $\|a\|x\in I$.\qed
\end{point}
\end{point}
\begin{point}{100}[riesz-ideal-basic]{Exercise}%
Verify the following facts about Riesz ideals.
\begin{enumerate}
\item
The least Riesz ideal that contains a self-adjoint element~$a$
of~$\scrA$ is
\begin{equation*}
(a)_m\ :=\ \{\,b\in \scrA\colon\, 
\exists n\in \N\,[\ \left|\Real{b}\right|,\,\left|\Imag{b}\right|
\,\leq\, n\left|a\right| \ ]\,\}.
\end{equation*}
Moreover,  $(a)_m=\scrA$ iff $a$ is invertible,
and we have~$(a)=(a)_m$ when~$a\geq 0$
(where $(a)$ is the least order ideal that contains~$a$,
see~\sref{order-ideal-basic}).
For non-positive~$a$, however, we may have~$(a)\neq (a)_m$.
\item
$I+J$ is a Riesz ideal of~$\scrA$
when $I$ and~$J$ are Riesz ideals. (Hint: use~\sref{riesz-decomposition-lemma}.)
But~$I+J$ might not be an order ideal
when~$I$ and~$J$ are order ideals.

\item
Each proper Riesz ideal is contained in a maximal Riesz ideal.
\end{enumerate}
\end{point}
\begin{point}{110}[maximal-riesz-ideal-maximal-order-ideal]{Lemma}%
A maximal Riesz ideal~$I$ of~$\scrA$
is a maximal order ideal.
\begin{point}{120}{Proof}%
Let~$J$ be a proper order ideal with $I\subseteq J$.
We must show that $J=I$.
Let~$a\in J$ be given;
we must show that $a\in I$.
Since~$\Real{a},\Imag{a}\in J$,
it suffices to show that~$\Real{a},\Imag{a}\in I$,
and so we might as well assume that~$a$ is self-adjoint
to begin with.
Similarly,
since~$\left|a\right|\in J$,
and it suffices to show that~$\left|a\right|\in I$,
because then $-\left|a\right|\leq a\leq \left|a\right|$
entails $a\in I$,
we might as well assume that~$a$ is positive.

Note that the least ideal~$(a)$ that contains~$a$
is also a Riesz ideal by~\sref{riesz-ideal-basic}.
Hence  $I+(a)$ is a Riesz ideal by~\sref{riesz-ideal-basic}
Since~$a\in J$, we have $(a)\subseteq J$,
and so~$I+(a)\subseteq J$ is proper.
It follows that $a\in I+(a)=I$ by maximality of~$I$.\qed
\end{point}
\end{point}
\begin{point}{130}[riesz-ideal-miu-map]{Lemma}%
Let~$I$ be a maximal Riesz ideal of~$\scrA$.
Then there is a miu-map $f\colon \scrA\to \C$
with $\ker(f)=I$.
\begin{point}{140}{Proof}%
Since~$I$ is a maximal order ideal 
by~\sref{maximal-riesz-ideal-maximal-order-ideal},
there is a pu-map $f\colon \scrA\to \C$
with~$\ker(f)=I$ by~\sref{maximal-ideal-state}.
It remains to be shown that~$f$ is multiplicative.
Let~$a,b\in \scrA$ be given;
we must show that $f(ab)=f(a)f(b)$.
Surely, since~$f$ is unital,
we have $f(b-f(b))=f(b)-f(b)=0$,
an so $b-f(b)\in \ker(f)\equiv I$.
Now, since~$I$ is a Riesz ideal,
we have $a(b-f(b))\in I\equiv \ker(f)$ by~\sref{riesz-ideal-ring-ideal},
and so~$0=f(\,a(b-f(b))\,)=f(ab)-f(a)f(b)$.
Hence~$f$ is multiplicative.\qed
\end{point}
\end{point}
\begin{point}{150}[inv-mult-state]{Proposition}%
Let~$a$ be a self-adjoint element of a $C^*$-algebra.
Then~$a$ is not invertible
iff there is $f\in\spec(\scrA)$ 
with~$f(a)=0$.
\begin{point}{160}{Proof}%
Note that if~$a$ is invertible,
then~$f(a^{-1})$ is the inverse of~$f(a)$---and so~$f(a)\neq 0$---for 
every~$f\in\spec(\scrA)$.
For the other, non-trivial, direction,
assume that~$a$ is not invertible.
Then 
by~\sref{riesz-ideal-basic}
the least Riesz ideal $(a)_m$
that contains~$a$ is proper,
and can be extended to a maximal Riesz ideal~$I$.
By~\sref{riesz-ideal-miu-map}
there is a miu-map $f\colon \scrA\to\C$
with~$\ker(f)=I$.
Then~$f\in\spec(\scrA)$
and~$f(a)=0$.\qed
\end{point}
\end{point}
\begin{point}{170}[spectrum-miu]{Exercise}%
Show that $\spec(a)=\{f(a)\colon f\in\spec(\scrA)\}$
for each self-adjoint $a\in\scrA$.
\end{point}
\begin{point}{180}[gelfand-representation-isometry]{Exercise}%
Prove that $\|\gamma(a)\|=\|a\|$
for each~$a\in\scrA$
where~$\gamma$ is from~\sref{gelfand}.

(Hint: first assume that~$a$ is self-adjoint,
and use \sref{spectrum-miu} and~\sref{norm-spectrum}.
For the general case,
use the $C^*$-identity.)

Conclude that the Gelfand representation $\gamma\colon \scrA\to C(\spec(\scrA))$
is injective,
and that its range $\{\gamma(a)\colon a\in\scrA\}$
is a $C^*$-subalgebra of~$C(\spec(\scrA))$.
\end{point}
\begin{point}{190}%
To show that~$\gamma$ is surjective,
we use the following special case of
the Stone--Weierstra\ss{} theorem.%
\index{Stone--Weierstra\ss{}' Theorem}
\end{point}
\begin{point}{200}[stone-weierstrass]{Theorem}%
Let~$X$ be a compact Hausdorff space,
and let~$\scrS$ be a $C^*$-subalgebra of~$C(X)$
which `separates the points of~$X$',
that is, for all~$x,y\in X$ with~$x\neq y$
there is~$f\in \scrS$ with $f(x)\neq f(y)$.
Then~$\scrS=C(X)$.
\begin{point}{210}{Proof}%
Let~$g\in \pos{C(X)}$ and $\varepsilon >0$.
To prove that~$\scrS=C(X)$,
it suffices to show that~$g\in \scrS$,
and for this,
it suffices to find~$f\in \scrS$ with $\|f-g\|\leq \varepsilon$,
because~$\scrS$ is closed.
It is convenient to assume that~$g(x)> 0$ for all~$x\in X$,
which we may, without loss of generality,
by replacing~$g$ by~$1+g$.

\begin{point}{220}[stone-weierstrass-1]%
Let~$x,y\in X$ with~$x\neq y$
be given.
We know there is~$f\in \scrS$ with $f(x)\neq f(y)$.
Note that we can assume that~$f(x)=0$ (by replacing~$f$ by~$f-f(x)$),
and that~$f$ is self-adjoint (by replacing~$f$
by either~$\Real{f}$ or~$\Imag{f}$),
and that~$f$ is positive
(by replacing~$f$ by~$f_+$ or~$f_-$),
and that~$f(y)=g(y)>0$
(by replacing $f$ by $\frac{g(y)}{f(y)} f$),
and that~$f\leq g(y)$
(by replacing $f$ by $f\wedge g(y)$).
\end{point}
\begin{point}{230}%
Let~$y\in X$ be given.
We will show that there is~$f\in\scrS$
with $0\leq f\leq g+\varepsilon$
and~$f(y)=g(y)$.
Indeed,
since~$g$ is continuous
there is an open neighborhood~$V$ of~$y$
with~$g(y) \leq  g(x)+\varepsilon$
for all~$x\in V$.
For each~$x\in X\backslash V$ there is $f_x \in [0,f(y)]_{\scrS}$
with $f_x(x)=0$ and~$f_x(y)=g(y)$ by~\sref{stone-weierstrass-1}.
Since the open subsets
$U_x := \{\,z\in X\colon f_x(z)\leq \varepsilon\,\}$
with~$x\in X\backslash V$
form an open cover of the closed (and thus compact) subset $X\backslash V$,
there are $x_1,\dotsc,x_N\in X\backslash U$
with $U_{x_1}\cup\dotsb\cup U_{x_N}\supseteq X\backslash V$.
Define $f:=f_{x_1}\wedge \dotsb \wedge f_{x_N}$.
Then~$f\in \scrS$, $0\leq f\leq g(y)$, $f(y)=g(y)$,
and $f(x)\leq \varepsilon$
for every~$x\in X\backslash V$.

We claim that $f\leq g+\varepsilon$.
Indeed,
if~$x\in X\backslash V$,
then $f(x)\leq \varepsilon\leq g(x)+\varepsilon$.
If~$x\in V$,
then $f(x)\leq g(y)\leq g(x)+\varepsilon$
(by definition of~$V$).
Hence $f\leq g+\varepsilon$.
\end{point}
\begin{point}{240}%
Thus for each~$y\in X$
there is $f_y\in \scrS$ with $0\leq f_y \leq g+\varepsilon$
and~$f_y(y)=g(y)$.
Since~$f_y$ is continuous at~$y$,
and~$f_y(y)=g(y)$,
there is an open neighborhood~$U_y$ of~$y$
with $g(y)-\varepsilon\leq f_y(x)$
for all~$x\in U_y$.
Since these open neighborhoods cover~$X$,
and~$X$ is compact,
there are $y_1,\dotsc,y_N\in X$
with $U_{y_1}\cup\dotsb\cup U_{y_N} = X$.
Define $f:=f_{y_1}\vee \dotsb\vee f_{y_N}$.
Then~$f\in\scrS$,
and $g-\varepsilon \leq f\leq g+\varepsilon$,
giving $\|f-g\|\leq \varepsilon$.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{250}[spectrum-calg-compact-hausdorff]{Lemma}%
The spectrum $\spec(\scrA)$ of~$\scrA$ is a compact Hausdorff space.
\begin{point}{260}{Proof}%
Since for each~$a\in \scrA$
and~$f\in \spec(\scrA)$
we have  $\|f(a)\|\leq \|a\|$ 
by~\sref{norm-mi-map}
we see that~$f(a)$ is an element of the compact set
$\{\,z\in \C\colon\, \left|z\right|\leq\|a\|\,\}$,
and so~$\spec(\scrA)$ is a subset of
\begin{equation*}
\textstyle
\prod_{a\in \scrA}\, \{\,z\in \C\colon\, \left|z\right|\leq \|a\|\,\},
\end{equation*}
which is a compact Hausdorff space
(by Tychonoff's theorem, under the product topology
it inherits
from the space of all functions $\scrA\to \C$).
So to prove that~$\spec(\scrA)$
is a compact Hausdorff space,
it suffices to show that~$\spec(\scrA)$
is closed.
In other words,
we must show that if~$f\colon \scrA\to \C$
is the pointwise limit of a net of miu-maps $(f_i)_i$,
then~$f$ is a miu-map as well.
But this is easily achieved
using the continuity of addition, involution and multiplication on~$\C$,
because, for instance, 
for~$a,b\in\scrA$, we have $f(ab)
= \lim_i f_i(ab)=\lim_i f_i(a)f_i(b)
 = (\lim_i f_i(a))\,(\lim_i f_i(b))
= f(a) \,f(b)$.\qed
\end{point}
\end{point}
\begin{point}{270}[gelfand]{Gelfand's Representation Theorem}%
\index{Gelfand's Representation Theorem}%
\index{Cstar-algebra@$C^*$-algebra!commutative}
For a commutative $C^*$-algebra~$\scrA$,
the Gelfand representation, 
 $\gamma\colon \scrA\to C(\spec(\scrA))$
defined in~\sref{gelfand-representation}
is a miu-isomorphism.
\begin{point}{280}{Proof}%
We already know that~$\gamma$ is an injective miu-map
(see~\sref{gelfand-representation-basic} 
and~\sref{gelfand-representation-isometry}).
So to prove that~$\gamma$ is a miu-isomorphism,
it remains to be shown that~$\gamma$ is surjective.
Since~$\spec(\scrA)$ is a compact Hausdorff space 
(by~\sref{spectrum-calg-compact-hausdorff}),
and~$\gamma(\scrA)\equiv \{\gamma(a)\colon a\in \scrA\}$
is a $C^*$-subalgebra of~$C(\spec(\scrA))$
(by~\sref{gelfand-representation-isometry}),
it suffices to show that~$\gamma(\scrA)$
separates the points of~$\spec(X)$
by~\sref{stone-weierstrass}.
This is obvious,
because
for~$f,g\in \spec(\scrA)$ with~$f\neq g$
there is~$a\in \scrA$ with~$f(a)\equiv \gamma(a)(f)
\neq \gamma(a)(g)\equiv g(a)$.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{280}%
\begin{point}{10}%
While Gelfand's representation theorem
is a result about commutative $C^*$-algebras,
it tells us a lot about non-commutative $C^*$-algebras too,
via their commutative $C^*$-subalgebras.
\end{point}
\begin{point}{20}[functional-calculus]{Exercise}%
Let~$a$ be an element of a (not necessarily commutative)
$C^*$-algebra~$\scrA$.
We are going to use Gelfand's representation
theorem to define~$f(a)$
for every continuous map $f\colon \spec(a)\to\C$
whenever~$a$ is contained in some commutative $C^*$-algebra.
This idea is referred to as the \Define{continuous functional calculus}.%
\index{functional calculus@$f(a)$, continuous functional calculus}
\begin{enumerate}
\item
Show that there is a least $C^*$-subalgebra
$\Define{C^*(a)}$%
\index{Cstara@$C^*(a)$, $C^*$-subalgebra genererated by~$a$}
of~$\scrA$
that contains~$a$.

Given~$b\in C^*(a)$
show that~$bc=cb$ for all~$c\in\scrA$
with~$ac=ca$.
\item
We call~$a\in\scrA$ \Define{normal}%
\index{normal!element of a $C^*$-algebra}
when~$C^*(a)$ is commutative.

Show that~$a$ is normal iff
$aa^*=a^*a$
iff~$\Real{a}\Imag{a}=\Imag{a}\Real{a}$.
\item
From now on assume~$a$ is normal
so that~$C^*(a)$ is commutative.

Show that~$j\colon\, \varrho\mapsto \varrho(a),\, \spec(C^*(a))\to \spec(a)$
is a continuous map.

Denoting the composition of
the miu-maps
\begin{equation*}
\xymatrix@C=4em{
	C(\spec(a))
	\ar[r]^-{f\mapsto f\circ j}
	&
	C(\spec(C^*(a)))
	\ar[r]^-{\cong,\, \text{\sref{gelfand}}}
	&
	C^*(a)
	\ar[r]^-{\text{inclusion}}
	&
	\scrA.
}
\end{equation*}
by~$\Phi$,
we write $\Define{f(a)}:=\Phi(f)$
for all~$f\in C(\spec(a))$.

We have hereby defined, for example, $a^\alpha$ when $a\geq 0$
and~$\alpha\in (0,\infty)$.

From the fact that~$\Phi$ is miu some properties
of~$f(a)$ can be derived.
Show, for example,
that~$a^\alpha a^\beta = a^{\alpha+\beta}$
for all~$\alpha,\beta\in(0,\infty)$
when~$a\geq 0$.
\item
Given $f\in C(\spec(a))$,
show that~$f(a)$ is the unique element of~$C^*(a)$
with 
\begin{equation*}
	\varphi(f(a))
	 \ =\ f(\varphi(a))
\end{equation*}
for all~$\varphi\in\spec(C^*(a))$.
\item
(\Define{Spectral mapping thm.})%
\index{Spectral Mapping Theorem}
Show that~$\spec(f(a))=f(\spec(a))$
for~$f\in C(\spec(a))$.
\item
Show that~$\spec(\varrho(a))\subseteq
\spec(a)$
and $\varrho(f(a))=f(\varrho(a))$
for every $f\in C(\spec(a))$ and  miu-map
$\varrho\colon \scrA\to\scrB$
into a $C^*$-algebra~$\scrB$.
\item
Given~$f\in C(\spec(a))$ and~$g\in C(f(\spec(a)))$
show that $g(f(a))=(g\circ f)(a)$.

Show that $(a^\alpha)^\beta = a^{\alpha\beta}$
for~$\alpha,\beta\in (0,\infty)$
and~$a\in\scrA_+$.
\end{enumerate}
\end{point}
\begin{point}{30}[sqrt-monotone]{Theorem}%
We have $0\leq a\leq b \implies a^\alpha \leq b^\alpha$
for all positive elements $a$ and~$b$
of a $C^*$-algebra~$\scrA$,
and $\alpha\in (0,1]$.
\begin{point}{40}{Proof}%
(Based on~\cite{pedersen1972}.)
Note that the result is trivial if~$a$ and~$b$ commute.

It suffices to show that
$(a+\frac{1}{n})^\alpha \leq (b+\frac{1}{n})^\alpha$
for all~$n$,
because~$(a+\frac{1}{n})^\alpha$
norm converges to~$a^\alpha$
as~$n\to\infty$.
In other words,
it suffices to prove $a^\alpha \leq b^\alpha$
under the additional assumption that~$a$ and~$b$ are invertible.
Note that~$a^0$ and~$b^0$ are defined for such
invertible~$a$ and~$b$,
because the
function~$(\,\cdot\,)^0\colon [0,1]\to\C$
is only discontinuous at~$0$.
Writing~$E$ for the set of all~$\alpha\in[0,1]$
for which $b\mapsto b^\alpha$ is monotone
on positive,
\emph{invertible} elements of~$\scrA$
we must prove that~$E\supseteq(0,1]$,
and we will in fact show that~$E=[0,1]$.
Since clearly~$0,1\in E$
it suffices to show that~$E$ is convex.
We'll do this by showing that~$E$
is closed,
and $\alpha,\beta\in E\implies \frac{1}{2}\alpha 
+ \frac{1}{2}\beta \in E$.

\begin{point}{50}{$E$ is closed}
Let~$b$ be a positive and invertible element of~$\scrA$.
A moment's thought reveals it suffices to 
prove that $\alpha\mapsto b^\alpha, \,[0,1]\to\scrA$
is continuous.
And indeed it is
being
the composition of the map $\alpha \mapsto b^\alpha\colon\,[0,1]
\to C(\spec(b))$, 
	which is norm continuous,
and the functional
	calculus $f\mapsto f(b)\colon\,
	C(\spec(b))\to \scrA$, which being a miu-map is norm continous
	as well.
\end{point}
\begin{point}{60}{$\alpha,\beta\in E\implies \frac{1}{2}\alpha+\frac{1}{2}\beta
	\in E$}
Let~$\alpha,\beta\in E$. Let~$a,b\in\scrA$ be positive
and invertible with $a\leq b$.
We must show that $a^{\nicefrac{\alpha+\beta}{2}}\leq 
b^{\nicefrac{\alpha+\beta}{2}}$.
Since the map $b^{\nicefrac{\alpha+\beta}{4}}(\,\cdot\,)
b^{\nicefrac{\alpha+\beta}{4}}$
is positive (by~\sref{astara-pos-basic-consequences}),
it suffices to show that
$b^{-\frac{\alpha+\beta}{4}}\,a^{\frac{\alpha+\beta}{2}}\,
b^{-\frac{\alpha+\beta}{4}} \leq 1$,
that is, 
$\|b^{-\frac{\alpha+\beta}{4}}\,a^{\frac{\alpha+\beta}{2}}\,
b^{-\frac{\alpha+\beta}{4}} \| \leq 1$.

For this, it seems, we must take a look under the hood
of the theory of $C^*$-algebras:
writing $\varrho(c):=\sup_{\lambda\in \spec(c)}\left|\lambda \right|$
for~$c\in \scrA$,
we know that $\varrho(c)\leq \|c\|$ for any~$c$,
and $\varrho(c)=\|c\|$ for self-adjoint~$c$ by \sref{norm-spectrum}.
Moreover, recall from~\sref{prod-spec}
that $\spec(cd)\backslash\{0\}
=\spec(dc)\backslash\{0\}$,
and so~$\varrho(cd)=\varrho(dc)$
for all $c,d\in \scrA$.
Hence
\begin{alignat*}{3}
\|\,b^{-\frac{\alpha+\beta}{4}}\,a^{\frac{\alpha+\beta}{2}}\,
b^{-\frac{\alpha+\beta}{4}}\,\| 
\ &=\ 
\varrho(\,b^{-\frac{\alpha+\beta}{4}}\,a^{\frac{\alpha+\beta}{2}}\,
b^{-\frac{\alpha+\beta}{4}} \,) \\
&=\ 
\varrho(\,b^{-\frac{\alpha+\beta}{4}}\,a^{\frac{\alpha+\beta}{2}}\,
b^{-\frac{\alpha+\beta}{4}} \,b^{-\frac{\alpha-\beta}{4}}
\,b^{\frac{\alpha-\beta}{4}}\,) \\
&=\ 
\varrho(
\,b^{\frac{\alpha-\beta}{4}}\,
\,b^{-\frac{\alpha+\beta}{4}}\,a^{\frac{\alpha+\beta}{2}}\,
b^{-\frac{\alpha+\beta}{4}} \,b^{-\frac{\alpha-\beta}{4}}\,)\\
&=\ 
\varrho(
\,b^{-\nicefrac{\beta}{2}}\,a^{\nicefrac{\beta}{2}}\,
a^{\nicefrac{\alpha}{2}}\,
b^{-\nicefrac{\alpha}{2}} \,) \\
&\leq\ 
\|\,b^{-\nicefrac{\beta}{2}}\,a^{\nicefrac{\beta}{2}}\,\|
\,\|\,
a^{\nicefrac{\alpha}{2}}\,
b^{-\nicefrac{\alpha}{2}} \,\|\\
&=\ 
\|\,b^{-\nicefrac{\beta}{2}}\,a^{\beta}\,
b^{-\nicefrac{\beta}{2}}\,\|^{\nicefrac{1}{2}}
\,\|\,
b^{-\nicefrac{\alpha}{2}} \,a^\alpha\,
b^{-\nicefrac{\alpha}{2}} \,\|^{\nicefrac{1}{2}}\\
\ &\leq\ 
\|\,b^{-\nicefrac{\beta}{2}}\,b^{\beta}\,
b^{-\nicefrac{\beta}{2}}\,\|^{\nicefrac{1}{2}}
\,\|\,
b^{-\nicefrac{\alpha}{2}} \,b^\alpha\,
b^{-\nicefrac{\alpha}{2}} \,\|^{\nicefrac{1}{2}} \ = \ 1,
\end{alignat*}
and so we're done.\qed
\end{point}
\end{point}
\end{point}
\end{parsec}

\begin{parsec}{290}[gelfand-equivalence]%
\begin{point}{10}%
As a cherry on the cake,
we use Gelfand's representation theorem~\sref{gelfand}
to get an equivalence between the categories $\op{(\cCstar{miu})}$
and~$\Define{\CH}$%
\index{CH@$\CH$}
of continuous maps between compact Hausdorff spaces.

To set the stage,
we extend $X\mapsto C(X)$ to a functor
$\CH\to \op{(\cCstar{miu})}$
by sending a continuous function~$f\colon X\to Y$
to the miu-map $C(f)\colon C(Y)\to C(X)$
given by~$C(f)(g)=g\circ f$ for $g\in C(Y)$,
and we extend $\scrA\mapsto \spec(\scrA)$
to a functor $\spec\colon \op{(\cCstar{miu})}\to \CH$
by sending a miu-map $\varphi \colon \scrA\to\scrB$
to the continuous map~$\spec(\varphi)\colon \spec(\scrB)\to\spec(\scrA)$
given by~$\spec(\varphi)(f)=f\circ \varphi$.

The Gelfand representations $\gamma_\scrA\colon \scrA\to C(\spec(\scrA))$
form a natural isomorphism
from $C\circ \spec$ to the identity functor on~$\op{(\cCstar{miu})}$.
So to get an equivalence,
it suffices to find a natural isomorphism
from the identity on~$\CH$ to~$\spec\circ C$,
which is provided by the following lemma.
\end{point}
\begin{point}{20}{Lemma}%
Let~$X$ be a compact Hausdorff space,
and let~$\tau \colon C(X)\to \C$ be a miu-map.
Then there is~$x\in X$ with $\tau(f)=f (x)$
for all~$f\in C(X)$.
\begin{point}{30}{Proof}%
Define
$Z\,= \, \{\,x\in X\colon \ h(x)\neq 0\text{ for some~$h\in \pos{C(X)}$
with $\tau(h)=0$}\,\}$.
We'll prove~$X\backslash Z$ contains
exactly one point, $x_0$, and $\tau(f)=f(x_0)$ for all~$f$.
\begin{point}{40}%`
To see that~$X\backslash Z$ contains no more than one point,
let~$x,y\in X$ with $x\neq y$ be given;
we will show that either~$x\in Z$ or~$y\in Z$.
By the usual topological trickery,
we can find~$f,g\in \pos{C(X)}$
with $fg=0$, $f(x)=1$ and~$g(y)=1$.
Then~$0=\tau(fg)=\tau(f)\,\tau(g)$,
so either~$\tau(f)=0$ (and~$x\in Z$), or~$\tau(g)=0$
(and~$y\in Z$).

That~$X\backslash Z$ is non-empty
follows from the following result (by taking~$f=1$).
\end{point}
\begin{point}{50}[multiplicative-state-on-cx-1]%
For~$f\in \pos{C(X)}$
with~$f(x)> 0 \implies x\in Z$ for all~$x\in X$
we have~$\tau(f)=0$.
Indeed, for each~$x\in X$ with~$f(x)>0$
(and so~$x\in Z$)
we can find~$h\in \pos{C(X)}$
with $\tau(h)=0$ and~$h(x)\neq 0$.
Then~$f(x)< g(x)$
and~$\tau(g)=0$
for $g:=(\frac{f(x)}{h(x)}+1)h$.
By compactness,
we can find $g_1,\dotsc,g_N\in \pos{C(X)}$
with~$\tau(g_n)=0$,
such that for every~$x\in X$
there is~$n$ with $g(x)<f_n(x)$.
Writing $g:=g_1\vee \dotsb \vee g_N$,
we have $0\leq f\leq g$ and~$\tau(g)=0$
(because by~\sref{commutative-cstar-basic}
$\tau$ preserves finite infima).
It follows that~$\tau(f)=0$.
\end{point}
\begin{point}{60}%
We now know that~$X\backslash Z$ contains exactly
one point, say~$x_0$.
To see that~$\tau(f)=f(x_0)$
for~$f\in C(X)$,
write $g:=(f-f(x_0))^*(f-f(x_0))$
and note that $g(x)>0\implies x\neq x_0\implies  x\in Z$.
Thus by~\sref{multiplicative-state-on-cx-1},
we get $0=\tau(g)=\left|\tau(f)-f(x_0)\right|^2$,
and so $\tau(f)=f(x_0)$.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{70}{Exercise}%
Let~$X$ be a compact Hausdorff space.
Show that for every~$x\in X$
the map $\delta_x\colon C(X)\to \C,\ f\mapsto f(x)$
is miu,
and that the map $X\to \spec(C(X)),\ x\mapsto \delta_x$
is a continuous bijection
onto a compact Hausdorff space,
and thus a homeomorphism.
\end{point}
\begin{point}{80}[injective-miu-isometry]{Exercise}%
\index{miu-map!injective!is isometry}
As an application of the equivalence
between $\op{(\cCstar{MIU})}$
and~$\CH$,
we will show that every injective miu-map
between $C^*$-algebras
is an isometry.

Show that an arrow $f\colon X\to Y$
in~$\CH$ is mono iff injective, and epi iff surjective
(using complete regularity of~$Y$).
Conclude that~$f$ is both epi and mono in~$\CH$
only if~$f$ is an isomorphism (i.e.~homeomorphism).

Let~$\varrho\colon \scrA\to\scrB$
be an injective miu-map between $C^*$-algebras.
Let~$a$ be a self-adjoint element of~$\scrA$.
Show that~$\varrho$ can be restricted
to a miu-map $\sigma\colon C^*(a)\to C^*(\varrho(a))$,
which is both epi and mono in~$\cCstar{MIU}$.
Conclude that~$\sigma$ is an isomorphism,
and thus~$\|\varrho(a)\|=\|a\|$.
Use the $C^*$-identity
to extend the equality $\|\varrho(a)\|=\|a\|$ 
to (not necessarily self-adjoint) $a\in \scrA$.
\end{point}
\begin{point}{90}[injective-miu-iso-on-image]{Exercise}%
Let~$\varrho\colon \scrA\to\scrB$ 
be an injective miu-map.
Show that~$\varrho(\scrA)$
is closed (using~\sref{injective-miu-isometry}).
Conclude that~$\varrho(\scrA)$
is a
 $C^*$-subalgebra of~$\scrB$
isomorphic to~$\scrA$.
\end{point}
\end{parsec}
\subsection{Representation by Bounded Operators}
\begin{parsec}{300}%
\begin{point}{10}[completion-inner-product-space]%
Let us prove that every $C^*$-algebra~$\scrA$
is isomorphic
to a $C^*$-algebra
of bounded operators on some Hilbert space.
We proceed as follows.
To each p-map $\omega\colon \scrA\to\C$
(see~\sref{maps})
we assign a inner product $[\,\cdot\,,\,\cdot\,]_\omega$ on~$\scrA$,
which can be ``completed'' to a Hilbert space $\scrH_\omega$.
Every element~$a\in \scrA$ gives a bounded operator on~$\scrH_\omega$
via the action $b\mapsto ab$, which in turn gives a 
miu-map $\varrho_\omega\colon \scrA\to \scrB (\scrH_\omega)$.
In general $\varrho_\omega$ is not injective,
but if~$\Omega$ is a set of p-maps which separates the
points of~$\scrA$,
then the composition
\begin{equation*}
	\xymatrix@C=6em{
		\scrA\ar[r]^-{\left<\varrho_\omega\right>_{\omega\in \Omega}}
		&
		\bigoplus_{\omega\in\Omega} \scrB(\scrH_\omega)
		\ar[r]
		&
		\scrB(\,\bigoplus_{\omega\in\Omega}\scrH_\omega\,)
	}
\end{equation*}
does give an injective miu-map~$\varrho$,
which restricts to an isomorphism 
(\sref{injective-miu-iso-on-image})
from~$\scrA$
to the $C^*$-algebra~$\varrho(\scrA)$
of bounded operators
on $\bigoplus_{\omega\in \Omega} \scrH_\omega$,
see~\sref{hilb-sum}.

The creation of~$\varrho_\omega$ from~$\omega$
is known as the \emph{Gelfand--Naimark--Segal (GNS) construction}
and will make a reappearance in the theory of von Neumann algebras
(in~\sref{normal-functionals-lemma}).

We take a somewhat utilitarian stance towards the GNS construction here,
but there is much more that can be said about it:
in the first chapter of my twin brother's thesis, \cite{bas},
you'll see that the GNS construction has a certain universal property,
and that it can be generalized to apply
not only to maps of the form $\omega\colon \scrA\to\C$,
but also to maps of the form $\varphi\colon \scrA\to\scrB$.
\end{point}
\begin{point}{20}[state-inner-product]{Lemma}%
For every p-map~$\omega\colon \scrA\to \C$ on a
$C^*$-algebra~$\scrA$,
$\Define{[a,b]_\omega} = \omega(a^*b)$
defines an inner product~$\Define{[\,\cdot\,,\,\cdot\,]_\omega}$%
\index{*innerprodomega@$[\,\cdot\,,\,\cdot\,]_\omega$, given np-functional $\omega$}
on~$\scrA$
(see~\sref{hilb-def}).
\begin{point}{30}{Proof}%
Note that $[a,a]_\omega\equiv \omega(a^*a)\geq 0$ for each~$a\in\scrA$,
because $a^*a\geq 0$ (by~\sref{astara-positive});
and  $\smash{\overline{[a,b]}_\omega}=[b,a]_\omega$
for $a,b\in\scrA$,
because $\omega$ is involution preserving (by~\sref{cstar-p-implies-i}).
Finally, it is clear that $[a,\,\cdot\,]_\omega\equiv\omega(a^*\,\cdot\,)$
is linear for each~$a\in\scrA$.\qed
\end{point}
\end{point}
\begin{point}{40}[omega-norm-basic]{Exercise}%
Let~$\omega\colon \scrA\to\C$
be a p-map on a $C^*$-algebra.
Let us for a moment study
the 
semi-norm
$\Define{\|\,\cdot\,\|_\omega}$%
\index{*seminormomega@$\|\,\cdot\,\|_\omega$, given np-functional $\omega$}
	on~$\scrA$
induced by the inner product $[\,\cdot\,,\,\cdot\,]_\omega$
(so~$\smash{\|a\|_\omega = \omega(a^*a)^{\nicefrac{1}{2}}}$),
because it plays an important role
here,
and all throughout the next chapter.
\begin{enumerate}
\item
Use Cauchy--Schwarz
(\sref{inner-product-basic})
to prove \Define{Kadison's inequality}%
\index{Kadison's inequality}: 
for all~$a,b\in\scrA$,
\begin{equation*}
\left|\omega(a^*b)\right|^2\ \leq\ \omega(a^*a)\ \omega(b^*b).
\end{equation*}
\item
Show that $\|ab\|_\omega \leq \|\omega\| \,\|a\|\,\|b\|_\omega$
for all $a,b\in\scrA$
(using $a^*a\leq \|a\|^2$).

Show that we do \emph{not} always have 
$\|ab\|_\omega\leq \|\omega\|\|a\|_\omega \|b\|$.

(Hint:
take $a=(\begin{smallmatrix}0&0\\0&1\end{smallmatrix})$
and $b=\frac{1}{2}(\begin{smallmatrix}1&1\\1&1\end{smallmatrix})$
from~$\scrA=M_2$,
and $\omega(\,(\begin{smallmatrix}c & d\\e&f\end{smallmatrix})\,)=c$.)

Show that neither always
$\|ab\|_\omega \leq \|a\|_\omega \|b\|_\omega$,
or
$\|a^*a\|_\omega = \|a\|^2_\omega$.

(Hint: 
take~$a=b=\frac{1}{2}(\begin{smallmatrix}1 & 1 \\ 1 & 1\end{smallmatrix})$
from~$\scrA= M_2$,
and 
$\omega((\,(\begin{smallmatrix}c&d\\e&f\end{smallmatrix})\,)=c$.)

Give a counterexample to $\|a^*\|_\omega = \|a\|_\omega$.
\end{enumerate}
\end{point}
\begin{point}{50}[inner-product-completion]{Exercise}%
\index{inner product!$\C$-valued!completion}
Let us begin by showing how a complex vector space~$V$
with inner product
$[\,\cdot\,,\,\cdot\,]$ can be ``completed'' to a Hilbert space~$\scrH$.

We will take for~$\scrH$ the set of Cauchy sequences on~$V$
modulo the following equivalence relation.
Two Cauchy sequences $(a_n)_n$ and~$(b_n)_n$ in~$V$
are considered equivalent
iff $\lim_n \|a_n-b_n\|=0$.
We ``embed'' $V$ into~$\scrH$ via the map $\eta\colon V\to \scrH$
which sends~$a$ to
the constant sequence $a,a,a,\dotsc$.
Note, however, that $\eta$ need not be injective:
show that $\eta(a)=\eta(b)$ iff $\|a-b\|=0$ for all $a,b\in V$.

Show that $d(\,(a_n)_n,\,(b_n)_n\,) = \lim_n \|a_n-b_n\|$
defines a metric on~$\scrH$,
that~$\scrH$ is complete with respect to this metric,
and that if $(a_n)_n$ is a Cauchy sequence in~$V$,
then $(\eta(a_n))_n$ converges to the \emph{element}~$(a_n)_n$ of~$\scrH$
(so $V$ is dense in~$\scrH$).

Show that every uniformly continuous 
map $f\colon V\to X$ to a complete metric space~$X$
can be uniquely extended to a uniformly continuous map $g\colon \scrH\to X$.
(We say that~$g$ extends~$f$ when $f=g\circ \eta$.)

Show that addition, scalar multiplication, and inner product on~$V$
(being uniformly continuous)
can be uniquely extended to uniformly continuous operations on~$\scrH$,
and turn~$\scrH$ into a Hilbert space.
(Also verify that the extended inner product agrees with the complete
metric we've already put on~$\scrH$.)

Show that every bounded linear map $f\colon V\to\scrK$
to a Hilbert space~$\scrK$
can be uniquely extended to a bounded linear map $g\colon \scrH\to\scrK$.

(Categorically speaking,
Hilbert spaces
form a reflexive subcategory of
the category of bounded linear maps between
complex vector spaces with an inner product.)
\end{point}
\begin{point}{60}[gns]{Definition (Gelfand--Naimark--Segal construction)}%
\index{Gelfand--Naimark--Segal (GNS)}%
	\\
Let $\omega\colon \scrA\to\C$ be a p-map on a $C^*$-algebra~$\scrA$.

Let~$\Define{\scrH_\omega}$%
\index{Homega@$\scrH_\omega$} 
	denote the completion
of~$\scrA$ endowed with the inner product $[\,\cdot\,,\,\cdot\,]_\omega$
(see~\sref{state-inner-product})
to a Hilbert space as discussed in~\sref{inner-product-completion}.
Recall that we have an ``embedding''
$\Define{\eta_\omega}\colon \scrA\to\scrH_\omega$%
\index{etaomega@$\eta_\omega$}
with $\left<\eta_\omega(a),\eta_\omega(b)\right>
= [a,b]_\omega$ for all~$a,b\in \scrA$.

Since given~$a\in \scrA$
the map $b\mapsto ab,\ \scrA\to\scrA$ is
bounded with respect to~$\|\,\cdot\,\|_\omega$
(because $\|ab\|_\omega\leq \|\omega\|\|a\|\|b\|_\omega$
by~\sref{omega-norm-basic}),
it can be uniquely extended to a bounded linear map
$\scrH_\omega\to\scrH_\omega$
(by the universal property of~$\scrH_\omega$, 
see~\sref{inner-product-completion}),
which we'll denote by~$\Define{\varrho_\omega}(a)$.%
\index{rhoomega@$\varrho_\omega$}
So~$\varrho_\omega(a)$ is the unique
bounded linear map $\scrH_\omega\to\scrH_\omega$
with $\varrho_\omega(a)(\eta_\omega(b)) = \eta_\omega(ab)$
for all~$b\in\scrA$.
\end{point}
\begin{point}{70}{Proposition}%
The map $\varrho_\omega\colon \scrA\to\scrB(\scrH_\omega)$
given by~\sref{gns} is a miu-map.
\begin{point}{80}{Proof}%
Let~$a_1,a_2\in\scrA$ be given.
Since $\varrho_\omega(a_1+a_2)\,\eta_\omega(b)
= \eta_\omega((a_1+a_2)b)
= \eta_\omega(a_1b)+\eta_\omega(a_2b)
= (\varrho_\omega(a_1) + \varrho_\omega(a_2))\,\eta_\omega(b)$
for all $b\in\scrA$,
and~$\{\eta_\omega(b)\colon b\in\scrA\}$
is dense in~$\scrH_\omega$,
we see that $\varrho_\omega(a_1+a_2)
=\varrho_\omega(a_1)+\varrho_\omega(a_2)$.
Since similarly $\varrho_\omega(\lambda a)
= \lambda\varrho_\omega(a)$
for $\lambda\in\C$ and~$a\in\scrA$,
we see that~$\varrho_\omega$ is linear.

Since $\varrho_\omega(1)\,\eta_\omega(b)
= \eta_\omega(b)$ for all~$b\in\scrA$,
we have $\varrho_\omega(1)\,x=x$
for all~$x\in\scrH_\omega$,
and so~$\varrho_\omega$ is unital,
$\varrho_\omega(1)=1$.

To see that~$\varrho_\omega$ is multiplicative,
note that
$(\varrho_\omega(a_1)\,\varrho_\omega(a_2))\,\eta_\omega(b)
= \eta_\omega(a_1a_2b)=\varrho_\omega(a_1a_2)\,\eta_\omega(b)$
for all~$a_1,a_2,b\in\scrA$.

Let~$a\in\scrA$ be given.
To show that~$\varrho_\omega$ is involution preserving
it suffices to prove that~$\varrho_\omega(a^*)$
is the adjoint of~$\varrho_\omega(a)$.
Since~$\left<\varrho_\omega(a^*)\,\eta_\omega(b),\eta_\omega(c)\right>
\equiv [a^*b,c]_\omega = \omega(b^*ac)=[b,ac]_\omega
\equiv \left<\eta_\omega(b),\varrho_\omega(a)\,\eta_\omega(c)\right>$
for all~$b,c\in\scrA$,
and~$\{\eta_\omega(b)\colon b\in\scrA\}$
is dense in~$\scrH_\omega$,
we get~$\left<\varrho_\omega(a^*)x,y\right>=\left<x,\varrho_\omega(a)y\right>$
for all~$x,y\in\scrH_\omega$,
and so~$\varrho_\omega(a^*)=\varrho_\omega(a)^*$.\qed
\end{point}
\end{point}
\begin{point}{90}[gelfand-naimark-representation]{Definition}%
Given a collection~$\Omega$ of p-maps $\omega\colon \scrA\to\C$
on a $C^*$-algebra~$\scrA$,
let $\Define{\varrho_\Omega}\colon \scrA\to \scrB(\scrH_\Omega)$%
\index{rhoOmega@$\varrho_\Omega$}
be the miu-map given by~$\varrho_\Omega(a)x 
= \sum_{\omega\in\Omega} \varrho_\omega(a)x(\omega)$,
where~$\Define{\scrH_\Omega}=\bigoplus_{\omega\in\Omega}\scrH_\omega$%
\index{HOmega@$\scrH_\Omega$}
(and $\varrho_\omega$ is as in~\sref{gns}).
\end{point}
\begin{point}{100}[proto-gelfand-naimark]{Proposition}%
For a collection~$\Omega$ of positive maps $\scrA\to \C$
on a $C^*$-algebra~$\scrA$,
the following are equivalent.
\begin{enumerate}
\item
\label{proto-gelfand-naimark-1}
$\varrho_\Omega\colon \scrA\to\scrB(\scrH_\Omega)$
is injective;
\item
\label{proto-gelfand-naimark-2}
$\Omega$ is centre separating on~$\scrA$
(see~\sref{separating});
\item
\label{proto-gelfand-naimark-3}
$\Omega'=\{\,\omega(b^*(\,\cdot\,)b)\colon \, b\in\scrA,\,\omega\in\Omega\,\}$
is order separating on~$\scrA$.
\end{enumerate}
In that case, $\varrho_\Omega(\scrA)$ is a $C^*$-subalgebra
of~$\scrB(\scrH_\Omega)$,
and~$\varrho_\Omega$
restricts to a miu-isomorphism from~$\scrA$ to~$\varrho_\Omega(\scrA)$.
\begin{point}{110}{Proof}%
It is clear that~\ref{proto-gelfand-naimark-3}
entails~\ref{proto-gelfand-naimark-2}.
\begin{point}{120}{\ref{proto-gelfand-naimark-2}$\Longrightarrow$%
\ref{proto-gelfand-naimark-1}}%
Let~$a\in \scrA$ with $\varrho_\Omega(a)=0$ be given.
We must show that~$a=0$ (in order to show that~$\varrho_\Omega$
is injective),
and for this it is enough to prove that~$a^*a=0$.
Let~$b\in\scrA$ and~$\omega\in\Omega$ be given.
Since~$\Omega$ is centre separating,
it suffices to show that $0=\omega(b^*a^*ab) \equiv \|ab\|_\omega^2$.
Since~$\varrho_\Omega(a)=0$,
we have $\varrho_\omega(a)=0$,
thus $0=\varrho_\omega(a)\,\eta_\omega(b)
=\eta_\omega(ab)$,
and so $\|ab\|_\omega=0$.
Hence~$\varrho_\Omega$ is injective.
\end{point}
\begin{point}{130}{\ref{proto-gelfand-naimark-1}$\Longrightarrow$%
\ref{proto-gelfand-naimark-3}}%
Let~$a\in\scrA$ with $\omega(b^*a b)\geq 0$
for all~$\omega\in\Omega$ and~$b\in\scrA$
be given.
We must show that~$a\geq 0$.
Since~$\varrho_\Omega$ is injective,
we know by~\sref{injective-miu-iso-on-image}
that~$\varrho_\Omega(\scrA)$ is a $C^*$-subalgebra
of~$\scrB(\scrH_\Omega)$,
and~$\varrho_\Omega$ restricts to a miu-isomorphism
from~$\scrA$ to~$\varrho_\Omega(\scrA)$.
So in order to prove that~$a\geq 0$,
it suffices to show that $\varrho_\Omega(a)\geq 0$,
and for this we must prove that $\varrho_\omega(a)\geq 0$
for given $\omega\in \Omega$.
Since the vector states on~$\scrH_\omega$ are order separating
by~\sref{hilb-vector-states-order-separating}, it suffices to show that 
$\left<x,\varrho_\omega(a)x\right>\geq 0$
for given~$x\in \scrH_\omega$.
Since~$\{\eta_\omega(b)\colon b\in\scrA\}$
is dense in~$\scrH_\omega$,
we only need to prove 
that~$0\leq \left<\eta_\omega(b),\varrho_\omega(a)\eta_\omega(b)\right>
\equiv \omega(b^*ab)$ for given~$b\in \scrA$,
but this is true
by assumption.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{140}[gelfand-naimark]{Theorem (Gelfand--Naimark)}%
\index{Gelfand--Naimark's Theorem}
Every $C^*$-algebra~$\scrA$ is miu-isomorphic
to a $C^*$-algebra of operators on a Hilbert space.
\begin{point}{150}{Proof}%
Since the states on~$\scrA$
are separating
(\sref{states-order-separating}),
and therefore centre separating,
the miu-map $\varrho_\Omega\colon \scrA\to\scrB(\scrH_\Omega)$
(defined in~\sref{gelfand-naimark-representation})
restricts to a miu-isomorphism
from~$\scrA$ onto the $C^*$-subalgebra
$\varrho(\scrA)$ of~$\scrB(\scrH_\Omega)$
by~\sref{proto-gelfand-naimark}.\qed
\end{point}
\end{point}
\end{parsec}
\section{Matrices over $C^*$-algebras}
\begin{parsec}{310}%
\begin{point}{10}%
We have seen (in~\sref{hilb}) that the $N\times N$-matrices
($N$ being a natural number) over the complex numbers~$\C$
form a $C^*$-algebra (denoted by~$M_N$) by interpreting
them as bounded operators on the Hilbert space $\C^N$,
and proving
that the bounded operators~$\scrB(\scrH)$
on any Hilbert space~$\scrH$ form a $C^*$-algebra.

In this paragraph, we'll prove the analogous
and more general
result that the 
$N\times N$-matrices \emph{over a $C^*$-algebra~$\scrA$}
form a $C^*$-algebra by interpreting them
as \emph{adjointable module maps} on
the \emph{Hilbert $\scrA$-module} $\scrA^N$,
see~\sref{chilb-basic} and~\sref{bax-cstar}.
\end{point}
\end{parsec}
\begin{parsec}{320}%
\begin{point}{10}[chilb-basic]{Definition}%
	An ($\scrA$-valued)
	\Define{inner product}%
\index{inner product!$\scrA$-valued}
	on a right $\scrA$-module~$X$
($\scrA$ being a $C^*$-algebra) is a map
$\left<\,\cdot\,,\,\cdot\,\right>\colon X\times X\to\scrA$%
\index{$\left<\,\cdot\,,\,\cdot\,\right>$, inner product!$\scrA$-valued}
such that, for all $x,y\in X$,
$\left<x,\,\cdot\,\right>\colon X\to \scrA$
is a module map, $\left<x,x\right>\geq 0$,
and $\left<x,y\right>=\left<y,x\right>^*$.
We say that such an inner product is \Define{definite}%
	\index{$\left<\,\cdot\,,\,\cdot\,\right>$, inner product!$\scrA$-valued!definite}
if~$\left<x,x\right>=0\implies x=0$ for all~$x\in X$.

A \Define{pre-Hilbert $\scrA$-module}%
\index{pre-Hilbert $\scrA$-module}
$X$ (where~$\scrA$ is always assumed to be a $C^*$-algebra)
is a right $\scrA$-module endowed with a definite inner product.
Such~$X$ is called
a \Define{Hilbert $\scrA$-module}%
\index{Hilbert $\scrA$-module}
when it is complete
with respect to
the norm we'll define in~\sref{chilb-norm-basic}.

Let~$X$ and~$Y$ be pre-Hilbert $\scrA$-module.
We say that a map $T\colon X\to Y$
is adjoint to a map $S\colon Y\to X$
when
\begin{equation*}
\left<Tx,y\right>\ =\ \left<x,Sy\right>
\qquad \text{for all $x\in X$ and $y\in Y$}.
\end{equation*}
In that case, we call~$T$ \Define{adjointable}.%
\index{adjointable!map between pre-Hilbert $\scrA$-modules}
It is not difficult to see that~$T$
must be linear, and a module map, and 
adjoint to exactly one~$S$, which we denote by~$\Define{T^*}$.%
\index{adjoint!of a adjointable map between pre-Hilbert $\scrA$-modules}

(Note that we did not require that~$T$
is bounded, and in fact, it need not be, 
see~\sref{hellinger-toeplitz-needs-complete}.
However, if~$T$ is bounded, then so is~$T^*$, 
see~\sref{chilb-form-bounded},
and if either~$X$ or~$Y$ is complete,
then~$T$ is automatically bounded, see~\sref{hellinger-toeplitz}.)

The vector space of adjointable bounded module maps~$T\colon X\to Y$ 
is denoted
by~$\Define{\scrB^a(X,Y)}$,%
\index{BaXY@$\scrB^a(X,Y)$}
and we write $\Define{\scrB^a(X)}=\scrB^a(X,X)$.%
\index{BaX@$\scrB^a(X)$}
\end{point}
\begin{point}{20}{Example}%
We endow $\scrA^N$
(where~$\scrA$ is a $C^*$-algebra and~$N$ is a natural number)
with the inner product $\left<x,y\right>=\sum_n x_n^*y_n$,
making it a Hilbert $\scrA$-module.
\end{point}
\begin{point}{30}{Exercise}%
Let~$S$ and~$T$ be adjointable operators on a 
pre-Hilbert $\scrA$-module.
\begin{enumerate}
\item
	Show that~$T^*$ is adjoint to~$T$ (and so~$T^{**}=T$).
\item
Show that $(T+S)^*=T^*+S^*$ 
and $(\lambda S)^*=\overline{\lambda}S^*$ for $\lambda\in \C$.
\item
Show that $ST$ is adjoint to $T^*S^*$
(and so $(ST)^*=T^*S^*$).
\end{enumerate}
\end{point}
\begin{point}{40}{Exercise}%
Although a bounded linear map between Hilbert spaces
is always adjointable (see~\sref{hilb-adjoint}),
a bounded module map
between Hilbert $\scrA$-modules
might have no adjoint
as is revealed by the following example
(based on~\cite{paschke}, p.~447).

Prove that~$J:=\{\,f\in C[0,1]\colon\, f(0)=0\,\}$
is a closed right ideal of~$C[0,1]$, and thus a
Hilbert $C[0,1]$-module.

Show that the inclusion $T\colon J\to C[0,1]$
is a bounded module map,
which has no adjoint
by proving that there is no~$b\in J$
with $\left<b,a\right>=Ta\equiv a$ for all~$a\in J$
(for if~$T$ had an adjoint~$T^*$,
then $\left<T^*1,a\right>=\left<1,Ta\right>=a$
for all~$a\in J$).

\begin{point}{50}{Remark}%
Note that part of the problem here is the lack 
of the obvious analogue to
Riesz'~representation theorem (\sref{riesz-representation-theorem})  
for Hilbert $\scrA$-modules.
One solution (taken in the literature) is to simply 
add Riesz'~representation theorem as axiom
giving us the \emph{self-dual} Hilbert $\scrA$-modules.
For those
who like to keep Riesz'~representation theorem a theorem,
I'd like to mention that 
it is also possible to assume instead that the Hilbert $\scrA$-module
is complete with respect to a suitable uniformity,
as in done in my twin brother's thesis, \cite{bas}, see~\sref{dils-selfdual}.
\end{point}
\end{point}
\begin{point}{60}[chilb-cs]{Proposition (Cauchy--Schwarz)}%
\index{Cauchy--Schwarz inequality!for A-valued@for $\scrA$-valued inner products}
	We have
$\left<x,y\right>\,\left<y,x\right>
\,\leq\,\left\|\left<y,y\right>\right\|\,\left<x,x\right>$
for every inner product $\left<\,\cdot\,,\,\cdot\,\right>$
on a right $\scrA$-module~$X$,
and $x,y\in X$.
\begin{point}{70}{Remark}%
The symmetry-breaking norm symbols ``$\|$'' cannot simply 
be removed from this version of Cauchy--Schwarz,
because 
$0\leq \left<x,y\right>\,\left<y,x\right>
\leq \left<y,y\right>\left<x,x\right>$
would
imply
that $\left<y,y\right>\left<x,x\right>$
is positive, and self-adjoint,
and thus that $\left<y,y\right>$
and~$\left<x,x\right>$ commute,
which is not always the case.
\end{point}
\begin{point}{80}{Proof}%
Let~$\omega\colon \scrA\to \C$ be a state of~$\scrA$.
Since the states on~$\scrA$
are order separating (\sref{states-order-separating}), 
it suffices to show that
$\omega(\,\left<x,y\right>\,\left<y,x\right>\,)
\,\leq\,\left\|\left<y,y\right>\right\|\,\omega(\left<x,x\right>)$.
Noting that $(u,v)\mapsto \omega(\left<u,v\right>)$
is a complex-valued inner product on~$X$,
we compute
\begin{alignat*}{3}
	\omega&(\,\left<x,y\right>\,\left<y,x\right>\,)^2\\
\ &= \ 
\omega(\,\left<x,\,y\left<y,x\right>\right>\,)^2
\\
&\leq\ 
\omega(\left<x,x\right>)\ 
\omega(\,\left<\,y\left<y,x\right>,\, y\left<y,x\right>\,\right>\,)
\qquad
&&\text{by Cauchy--Schwarz, \sref{inner-product-basic}}
\\
&=\ 
\omega(\left<x,x\right>)\ 
\omega(\,\left<x,y\right> \,\left<y,y\right>\, \left<y,x\right>\,)
\\
&\leq\ 
\omega(\left<x,x\right>)\ 
\omega(\,\left<x,y\right>\left<y,x\right>\,)
\  \left\|\left<y,y\right>\right\|
\qquad
&&\text{since $\left<y,y\right>\leq \left\|\left<y,y\right>\right\|$.}
\end{alignat*}
It follows
(also when~$\omega(\,\left<x,y\right>\,\left<y,x\right>\,)=0$),
that 
\begin{equation*}
\omega(\,\left<x,y\right>\,\left<y,x\right>\,)\ \leq\ 
\left\|\left<y,y\right>\right\|\,
\omega(\left<x,x\right>),
\end{equation*}
and so we're done.\qed
\end{point}
\end{point}
\begin{point}{90}[chilb-norm-basic]{Exercise}%
Let~$X$ be a pre-Hilbert $\scrA$-module.
Verify that
\begin{enumerate}
	\item
$\Define{\|x\|} = \left\|\left<x,x\right>\right\|^{\nicefrac{1}{2}}$
defines a norm~$\left\|\,\cdot\,\right\|$%
\index{$"\"|\,\cdot\,"\"|$, norm!on a pre-Hilbert $\scrA$-module}
on~$X$, and
\item
$\left\|xb\right\|\leq \left\|x\right\|\left\|b\right\|$
and $\left\|\left<x,y\right>\right\|\leq \left\|x\right\|
\left\|y\right\|$
for all~$x,y\in X$ and $b\in \scrA$.
\end{enumerate}
\end{point}
\begin{point}{100}[chilb-form-bounded]{Lemma}%
For a linear map~$T\colon X\to Y$
between pre-Hilbert $\scrA$-modules,
and $B>0$,
the following are equivalent.
\begin{enumerate}
\item 
\label{chilb-form-bounded-1}
$\|Tx\|\leq B\,\|x\|$ for all~$x\in X$
(that is, $T$ is bounded by~$B$);
\item
\label{chilb-form-bounded-2}
$\left\|\left<y,Tx\right>\right\|\leq B\,\|y\|\|x\|$
for all~$x\in X$, $y\in Y$.
\end{enumerate}
Moreover,
if~$T$ is adjointable,
and bounded, then $\|T^*\|=\|T\|$.
\begin{point}{110}{Proof}%
If~$\|Tx\|\leq B\|x\|$ for all~$x\in X$,
then~$T$ is bounded, $\|T\|\leq B$, and therefore
$\left\|\left<y,Tx\right>\right\|
\leq \|y\|\,\|Tx\|\leq B \|y\|\|x\|$
for all~$x\in X$ and~$y\in Y$ using~\sref{chilb-cs}.

On the other hand,
if~\ref{chilb-form-bounded-2} holds,
and~$x\in X$ is given,
then we have $\|Tx\|^2=\left\|\left<Tx,Tx\right>\right\|
\leq B \,\|Tx\|\|x\|$,
entailing $\|Tx\|\leq B\|x\|$
(also when~$\|Tx\|=0$).

If~$T$ is adjointable, and bounded,
then~$\left\|\left<x,T^*y\right>\right\|=\left\|\left<y,Tx\right>\right\|
\leq \|T\|\|y\|\|x\|$ for all~$x\in X$, $y\in Y$,
so~$\|T^*\|\leq \|T\|$,
giving us that~$T^*$ is bounded.
Since by a similar reasoning $\|T\|\leq \|T^*\|$,
we get $\|T\|=\|T^*\|$.\qed
\end{point}
\end{point}
\begin{point}{120}[module-maps-cstar-identity]{Exercise}%
Show that $\|T^*T\|=\|T\|^2$
for every adjointable bounded map~$T$ on a pre-Hilbert $\scrA$-module.
(Hint: adapt the proof of~\sref{operators-cstar-identity}.)
\end{point}
\begin{point}{130}[bax-cstar]{Proposition}%
The adjointable bounded module maps
on a Hilbert $\scrA$-module
form a $C^*$-algebra%
\index{BaX@$\scrB^a(X)!as $C^*$-algebra}
$\scrB^a(X)$
with composition as multiplication,
adjoint as involution,
and the operator norm as norm.
\begin{point}{140}{Proof}%
Considering~\sref{bounded-operators-banach-algebra}
and~\sref{module-maps-cstar-identity},
the only thing that remains to be shown is that~$\scrB^a(X)$
is closed (with respect to the operator norm)
in the set of all bounded \emph{linear} maps $\scrB(X)$.
So let~$T\colon X\to X$ be a bounded linear map
which is the limit of a sequence $T_1,T_2,\dotsc$
of adjointable bounded module maps.

To see that~$T$ has an adjoint,
note that~$\left\|T_n^*-T_m^*\right\|
=\left\|(T_n-T_m)^*\right\|
=\left\|T_n-T_m\right\|$
for all~$n,m$, and so $T_1^*,\,T_2^*,\,\dotsc$
is a Cauchy sequence,
and converges to some bounded operator~$S$ on~$X$.
Since for~$x,y\in X$ and~$n$,
\begin{alignat*}{3}
\left\|\left<Sx,y\right>-\left<x,Ty\right>\right\|
\ &\leq\ 
\left\|\left<(S-T^*_n)x,y\right>\right\|
\,+\,
\left\|\left<x,(T_n-T)y\right>\right\|
\\
\ &\leq\ 
\|S-T^*_n\|\|x\|\|y\|\,+\,\|T_n-T\|\|x\|\|y\|,
\end{alignat*}
we see that $\left<Sx,y\right>=\left<x,Ty\right>$,
so~$S$ is the adjoint of~$T$,
and~$T$ is adjointable.
\qed
\end{point}
\end{point}
\begin{point}{150}[chilb-vector-states-order-separating]{Exercise}%
Let~$X$ be a Hilbert~$\scrA$-module.
Show that 
the vector states%
\index{vector functional!for a Hilbert $\scrA$-module}
of~$\scrB^a(X)$
are order separating (see~\sref{separating}).
Conclude that 
for an adjointable operator~$T$ on~$X$
\begin{enumerate}
\item
$T$ is self-adjoint iff $\left<x,Tx\right>$
is self-adjoint for all~$x\in (X)_1$;
\item
$0\leq T$ iff $0\leq\left<x,Tx\right>$
for all~$x\in (X)_1$;
\item
	$\|T\|=\sup_{x\in (X)_1}\|\left<x,Tx\right>\|$
when~$T$ is self-adjoint.
\end{enumerate}
(Hint:~adapt the proofs
of~\sref{hilb-vector-states-order-separating}
and~\sref{hilb-positive-operators}.)
\end{point}
\begin{point}{160}{Corollary}%
The operator
$T^*T$ is positive
in~$\scrB^a(X)$
for every adjointable operator~$T\colon X\to Y$
between Hilbert $\scrA$-modules.
\begin{point}{170}{Proof}%
$\left<x,T^*Tx\right>=
\left<Tx,Tx\right> \geq 0$
for all~$x\in X$,
and so~$T^*T\geq 0$ by~\sref{hilb-positive-operators}.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{330}
\begin{point}{10}[cstar-matrices]{Exercise}%
Let us consider matrices over a $C^*$-algebra $\scrA$.
\begin{enumerate}
\item
	Show that every $N\times M$-matrix~$A$ (over~$\scrA$)
gives a bounded module map~$\underline{A}\colon \scrA^N\to\scrA^M$ 
via $\underline{A}(a_1,\dotsc,a_N)= A(a_1,\dotsc,a_N)$,
which is adjoint to~$\underline{A^*}$
(where $\Define{A^*}= (A_{ji}^*)_{ij}$ is conjugate transpose).

\item
Show that $A\mapsto \underline{A}$
gives a linear bijection between the vector 
space of $N\times M$-matrices 
over~$\scrA$ and the vector space of adjointable bounded
module maps~$\scrB^a(\scrA^N,\scrA^M)$.

\item
Show that~$\underline{A}\circ \underline{B} = \underline{AB}$
when $A$ is an $N\times M$ and~$B$ an $M\times K$ matrix.

\item
Conclude that the vector space $\Define{M_N\scrA}$%
\index{$M_n\scrA$, the $n\times n$-matrices over~$\scrA$!as a $C^*$-algebra}
of $N\times N$-matrices over~$\scrA$
is a $C^*$-algebra
with matrix multiplication (as multiplication),
conjugate transpose as involution,
and the operator norm (as norm, so~$\|A\|=\|\underline{A}\|$).
\end{enumerate}
\end{point}
\begin{point}{20}[when-a-matrix-over-a-cstar-algebra-is-positive]{Exercise}%
Let us describe the positive  $N\times N$ matrices
over a $C^*$-algebra~$\scrA$.
\begin{enumerate}
\item
Show that an $N\times N$ matrix~$A$ over~$\scrA$
is positive iff $0\leq \sum_{i,j} a_i^* A_{ij} a_j$
for all~$a_1,\dotsc,a_N\in\scrA$.
(Hint: use~\sref{hilb-vector-states-order-separating}.)
\item
Show that the matrix $(\,\left<x_i,x_j\right>\,)_{ij}$
is positive for all vectors $x_1,\dotsc,x_N$
from a pre-Hilbert $\scrA$-module~$X$.
\item
Show that the matrix $(a^*_ia_j)_{ij}$
is positive for all $a_1,\dotsc,a_N\in\scrA$.
\end{enumerate}
\end{point}
\begin{point}{30}[mnf]{Exercise}%
Let~$f\colon \scrA\to\scrB$ be a linear map between $C^*$-algebras.
\begin{enumerate}
\item
Show that applying~$f$ entry-wise to a $N\times N$ matrix~$A$
over~$\scrA$ (yielding the matrix $(f(A_{ij}))_{ij}$ over~$\scrB$)
gives a linear map,
which we'll denote by~$\Define{M_Nf}\colon M_N\scrA\to M_N\scrB$.%
\index{$M_nf$}
\item
The map~$M_Nf$ inherits some traits of~$f$:
show that if~$f$ is unital, then~$M_Nf$ unital;
if~$f$ is multiplicative, then $M_Nf$ is multiplicative; and
if~$f$ is involution preserving, then so is~$M_Nf$.
\item
However,
show that $M_nf$ need not be positive when~$f$ is positive,
and that~$M_nf$ need not be bounded, when~$f$ is.
\end{enumerate}
\end{point}
\end{parsec}
\begin{parsec}{340}%
\begin{point}{10}%
Let us briefly return
to the completely positive maps (defined in~\sref{maps}),
to show that a map $f$ between $C^*$-algebras
is completely positive precisely
when~$M_Nf$ is positive for all~$N$,
and to give some examples of completely positive maps.

We also prove two lemmas
stating special properties of completely positive maps (setting
them apart from plain positive maps),
that'll come in very handy later on.
The first one is a variation on Cauchy--Schwarz
(\sref{cp-cs}),
and the second one concerns
the points at which a cpu-map is multiplicative (\sref{choi}).

Completely positive maps are often touted as 
a good models for quantum processes
(over plain positive maps)
with an argument involving the tensor product,
and while we agree,
we submit that the absence of analogues of \sref{cp-cs} and~\sref{choi}
for positive maps
is already enough to make complete positivity indispensable.
\end{point}
\begin{point}{20}[n-pos]{Lemma}%
\index{completely positive!map between $C^*$-algebras}
For a linear map $f\colon \scrA\to\scrB$
between $C^*$-algebras,
and natural number~$N$,
the following are equivalent.
\begin{enumerate}
\item
\label{n-pos-1}
$M_Nf\colon M_N\scrA\to M_N\scrB$
is positive;
\item
\label{n-pos-2}
	$\sum_{ij} b^*_if(a^*_ia_j)b_j \geq 0$
	for all~$a\equiv(a_1,\dotsc,a_N)\in \scrA^N$
	and $b\in \scrB^N$;
\item
\label{n-pos-3}
the matrix $(\,f(a_i^*a_j)\,)_{ij}$
is positive in $M_N\scrB$ for all $a\in\scrA^N$.
\end{enumerate}
\begin{point}{30}{Proof}%
Recall that~$M_Nf$ is positive
iff $(M_Nf)(C)$ is positive for all $C\in \pos{(M_N\scrA)}$.
The trick is to note that such~$C$ can be written as $C\equiv A^*A$
for some~$A\in M_N\scrA$,
and thus as $C \equiv (a_1^T)^* a_1^T+\dotsb+(a_N^T)^*a_N^T$,
where $a_n\equiv(A_{n1},\dotsc,A_{nN})$ is the $n$-th row of~$A$.
Hence~$M_Nf$ is positive
iff $(M_Nf)(\,(a^T)^*a^T\, )\equiv(\,f(a_i^*a_j)\,)_{i,j}$ is positive
for all tuples~$a\in\scrA^N$.
Since~$B\in M_N\scrB$ is positive iff $\left<b,Bb\right>\geq 0$
for all~$b\in \scrB^N$,
we conclude:
$M_Nf$ is positive iff 
$0\leq\left<b,(M_Nf)(\, (a^T)^*a^T\,) b\right>
= \sum_{ij} b_i^*f(a_i^*a_j)b_j$
for all~$a\in\scrA^N$ and~$b\in\scrB^N$.\qed
\end{point}
\end{point}
\begin{point}{40}[cp]{Exercise}%
Conclude from~\sref{n-pos}
that a linear map~$f$ between $C^*$-algebras
is completely positive iff~$M_Nf$ is positive for all~$N$
iff 
for all~$N$ and~$a\in \scrA^N$
the matrix $(\,f(a_i^*a_j)\,)_{i,j}$ 
is positive in~$M_N\scrB$.

Deduce that the composition of cp-maps is
completely positive.

Show that a mi-map~$f$ is completely positive.
(Hint: $M_Nf$ is a mi-map too.)
\end{point}
\begin{point}{50}[ad-cp]{Exercise}%
Show that
given a $C^*$-algebra~$\scrA$,
the following maps are completely positive:
\begin{enumerate}
\item
$b\mapsto a^*ba\colon \scrA\to\scrA$
for every~$a\in\scrA$;%
\index{$a^*(\,\cdot\,)a\colon \scrA\to\scrA$!is completely positive}
\item
$T\mapsto S^* T S\colon \scrB^a(X)
\to\scrB^a(Y)$
\index{$A^*(\,\cdot\,) A\colon \scrB^a(X)
\to\scrB^a(Y)$!is completely positive}
for every adjointable operator $S\colon Y\to X$
between Hilbert $\scrA$-modules;
\item
$T\mapsto \left<x,Tx\right>,\scrB^a(X)\to \scrA$%
		\index{vector functional!is completely positive}
for every element~$x$ of a Hilbert $\scrA$-module~$X$.
\end{enumerate}
\end{point}
\begin{point}{60}[cstar-product-4]{Exercise}%
\index{product!in $\Cstar{cpsu}$}%
\index{equaliser!in $\Cstar{cpsu}$}%
Show that the product 
of a family of $C^*$-algebras $(\scrA_i)_i$
in the category~$\Cstar{cpsu}$
(see~\sref{maps}) 
is given by~$\bigoplus_i \scrA_i$
with the same projections as in~\sref{cstar-product-2}.

Show that the equaliser
of miu-maps $f,g\colon\scrA\to\scrB$
in~$\Cstar{cpsu}$
is the inclusion of
the $C^*$-subalgebra
$\{\,a\in\scrA\colon\, f(a)=g(a)\,\}$
of~$\scrA$ into~$\scrA$.
\end{point}
\begin{point}{70}[ccstar-pos-mat]{Lemma}%
Let~$\scrA$ be a commutative $C^*$-algebra,
and let~$N$ be a natural number.
The set of  matrices of the form $\sum_k a_k B_k$,
where $a_1,\dotsc,a_K\in \scrA_+$
and $B_1,\dotsc,B_K\in M_N(\C)_+$,
is norm dense in~$(M_N\scrA)_+$.
\begin{point}{80}{Proof}%
Since~$\scrA$ is isomorphic to~$C(X)$ for some compact
Hausdorff space~$X$ (by~$\sref{gelfand})$),
we may as well assume that~$\scrA\equiv C(X)$.

Let~$A\in M_N(C(X))_+$ and~$\varepsilon>0$ be given.
We're looking for $g_1,\dotsc,g_K\in C(X)_+$
and $B_1,\dotsc,B_K\in (M_N)_+$
with $\|A-\sum_k g_k B_k\|\leq \varepsilon$.
Since $A(x):=(A_{ij}(x))_{ij}$
gives a continuous map $X\to M_N$,
the sets
$U_x = \{\,y\in X\colon \, \|A(x)-  A(y)\| < \varepsilon\,\}$
form an open cover of~$X$.
By compactness of~$X$
this cover has a finite subcover;
there are $x_1,\dotsc,x_K\in X$ with
$U_{x_1}\cup\dotsb\cup U_{x_K}=X$.

Let~$y\in X$ be given. Since $y\in U_{x_k}$
for some~$k$, there is, by complete regularity of~$X$,
a function $f_y\in (C(X))_+$
with $f_y(y)>0$
and $\supp(f_y)\subseteq U_{x_k}$.
Since the open subsets~$\supp(f_y)$
cover~$X$
there are (by compactness of~$X$) finitely many $y_1,\dotsc y_L$
with $X = \supp(f_{y_1})\cup \dotsb \cup \supp(f_{y_L})$,
and so~$\sum_\ell f_{y_\ell} > 0$.
Let us group together the $f_{y_\ell}$s:
pick for each~$\ell$ an $k_\ell$ with $\supp(f_{y_\ell})\subseteq 
U_{x_{k_\ell}}$,
and let $g_k:= \sum\{f_\ell\colon k_\ell = k\}$.
Then $g_k\in (C(X))_+$,
$\supp(g_k)\subseteq U_k$,
and $\sum_k g_k >0$.
	Upon replacing $g_k$ with $(\sum_\ell g_\ell)^{-1} g_k$ if necessary,
we see that $\sum_k g_k=1$.

Since~$\supp(g_k)\subseteq U_{x_k}$,
we have $-\varepsilon \leq A(x)-A(x_k)\leq \varepsilon$
for all~$x\in \supp(g_k)$,
and so  $-g_k(x) \varepsilon
\,\leq\, g_k(x) A(x) - g_k(x) A(x_k)\,\leq\, g_k(x) \varepsilon$
for all~$x\in X$,
that is,  $-g_k \varepsilon
\,\leq\, g_k A - g_k A(x_k)\,\leq\, g_k \varepsilon$.
Summing yields
$-\varepsilon \,\leq\, A- \sum_k g_k A(x_k)\,\leq\, \varepsilon$,
and so $\|A-\sum_k g_k A(x_k)\|\leq \varepsilon$.\qed
\end{point}
\end{point}
\begin{point}{90}[cp-commutative]{Proposition}%
Let~$f\colon \scrA\to\scrB$ be a 
positive map between $C^*$-algebras.
If either~$\scrA$ or~$\scrB$ is commutative,
then~$f$ is completely positive.
\begin{point}{100}{Proof}%
Suppose that~$\scrB$ is commutative,
and let~$a_1,\dotsc,a_N\in \scrA$,
$b_1,\dotsc,b_N\in\scrB$ be given.
We must
show that $\sum_{i,j} b_i^*f(a_i^*a_j)b_j$ is positive.
This follows from the observation that
$\omega(\,\sum_{i,j} b_i^*f(a_i^*a_j)b_j\,)
= \omega(f(\,\sum_{i,j}(a_i\omega(b_i))^*\,a_j \omega(b_j)\,))\,\geq \,0$
for every~$\omega\in\spec(\scrB)$.
\begin{point}{110}%
Suppose instead that~$\scrA$ is commutative,
and let $A\in (M_N\scrA)_+$
be given for some natural number~$N$.
We must show that~$(M_Nf)(A)$ is positive in~$M_N\scrB$.
By~\sref{ccstar-pos-mat},
the problem reduces to the case that~$A\equiv a B$
where~$a\in \scrA_+$ and~$B\in (M_N)_+$.
Since $(M_Nf)(aB)\equiv f(a)B$ is clearly positive
in~$M_N\scrB$,
we are done.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{120}[cstar-positive-2x2matrix]{Lemma}%
For a positive
matrix $A\equiv \bigl(\begin{smallmatrix} p & a \\ a^* & q
\end{smallmatrix}\bigr)$
over a $C^*$-algebra~$\scrA$
we have
\begin{equation*}
	a^*a\ \leq\  \|p\|q
	\quad\text{ and }\quad
	aa^*\leq \|q\|p.
\end{equation*}
In particular,
if $p=0$ or~$q=0$, then~$a=a^*=0$.
\begin{point}{130}{Proof}%
Since $(x,y)\mapsto \left<x,Ay\right>$
gives an $\scrA$-valued inner product
on~$\scrA^2$,
{
\newcommand\twovect[2]{%
\left(\begin{smallmatrix}#1\\#2\end{smallmatrix}\right)}
\newcommand\onezero{\twovect{1}{0}}
\newcommand\zeroone{\twovect{0}{1}}
\begin{alignat*}{3}
	aa^*
	\ &=\  \left<\,\onezero,\,A\zeroone\,\right> \ 
\left<\,\zeroone,\,A\onezero\,\right> \\
\ &\leq\  
\left\|\left<\,\zeroone,\,A\zeroone\right>\right\| \ 
\left<\,\onezero,\,A\onezero\,\right>
\ =\  \|q\|\  p
\end{alignat*}
}
by Cauchy--Schwarz (see \sref{chilb-cs}).

By a similar reasoning, we get $a^*a\leq \|p\|q$.\qed
\end{point}
\end{point}
\begin{point}{140}[cp-cs]{Lemma}%
We have $f(a^*b) f(b^*a)\leq \|f(b^*b)\|\,f(a^*a)$
for every p-map $f\colon \scrA\to\scrB$
between $C^*$-algebras
and $a,b\in\scrA$,
provided that $M_2f$ is positive.
\begin{point}{150}{Proof}%
Since writing $x\equiv (a,b)\in \scrA^2$,
the $2\times 2$ matrix $(x^T)^* x^T\equiv 
	\bigl(
\begin{smallmatrix}
a^*a & a^*b \\
b^*a & b^* b
\end{smallmatrix} \bigr)$
in $M_2\scrA$
is positive,
the $2\times 2$ matrix $T:=\bigl(
\begin{smallmatrix}
	f(a^*a) & f(a^*b) \\
	f(b^*a) & f(b^* b)
\end{smallmatrix}\bigr)$
in~$M_2\scrB$ is positive.
Thus we get $f(a^*b) f(b^*a)\leq \|f(b^*b)\|\,f(a^*a)$
by~\sref{cstar-positive-2x2matrix}.\qed
\end{point}
\end{point}
\begin{point}{160}[cp-russo-dye]{Corollary}%
$\|f\|= \|f(1)\|$
for every cp-map $f\colon \scrA\to\scrB$ between $C^*$-algebras.
\begin{point}{170}{Proof}%
Let~$a\in\scrA$ be given.
It suffices to show that $\|f(a)\|\leq \|f(1)\|\,\|a\|$
so that~$\|f\|\leq\|f(1)\|$,
because we already know that~$\|f(1)\|\leq \|f\|\,\|1\| = \|f\|$.
Since $\|f(a^*a)\|\leq \|f(1)\|\,\|a^*a\|$
by~\sref{weak-russo-dye},
we have
 $\|f(a)\|^2=\|f(a)^*f(a)\|=\|f(a^*1)f(1^*a)\|
\leq \|f(1^*1)\|\,\|f(a^*a)\|
\leq \|f(1)\|\, \|f(1)\|\|a^*a\|
= \|f(1)\|^2 \|a\|^2$
by~\sref{cp-cs},
and so~$\|f(a)\|\leq \|f(1)\|\,\|a\|$.\qed
\end{point}
\end{point}
\begin{point}{180}[choi]{Lemma (Choi\cite{choi})}%
\index{Choi's Theorem}%
We have
$f(a)^*f(a) \leq f(a^* a)$ for
every
cpu-map~$f\colon \scrA\to\scrB$ between $C^*$-algebras,
and~$a\in\scrA$.
Moreover, if $f(a^*a)=f(a)^*f(a)$
for some~$a\in\scrA$,
then~$f(ba)=f(b)f(a)$
for all~$b\in \scrA$.
\begin{point}{190}{Proof}%
By~\sref{cp-cs}
we have $f(a)^*f(a)=f(a^* 1)f(1^* a) \leq
\|f(1^*1)\| f(a^*a)=f(a^*a)$,
where we used that~$f$ is unital, viz.~$f(1)=1$.

Let~$a,b\in \scrA$ be given,
and assume that $f(a^*a)=f(a)^*f(a)$.
Instead of~$f(ba)=f(b)f(a)$
we'll prove that $f(a^*b)=f(a)^*f(b)$
(but this is nothing more than  a reformulation).
Since~$M_2f$ is cp,
we have, writing 
$A\equiv\bigl(\begin{smallmatrix}a&b\\0&0\end{smallmatrix}\bigr)$,
\begin{alignat*}{3}
\left(\,\begin{matrix}f(a)^*f(a)&f(a)^*f(b)\\
f(b)^*f(a)&f(b)^*f(b)\end{matrix}\,\right) 
	\ &=\ (M_2f)(A)^*\,(M_2f)(A)\\
\ &\leq\ (M_2f)(A^*A) \ =\ 
\left(\,\begin{matrix}f(a^*a)&f(a^*b)\\
f(b^*a)&f(b^*b)\end{matrix}\,\right).
\end{alignat*}
Hence
(using that $f(a^*a)=f(a)^*f(a)$)
the following matrix is positive.
\begin{equation*}
\left(\,\begin{matrix}
0 & f(a^*b) - f(a)^*f(b) \\
f(b^*a)-f(b)^*f(a) & f(b^*b)-f(b)^*f(b)
\end{matrix}\,\right)
\end{equation*}
But then
by~\sref{cstar-positive-2x2matrix}
we have
$f(a^*b)-f(a)^*f(b)=0$.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{341}%
\begin{point}{10}%
We've just seen in~\sref{cp-russo-dye}
that the norm of a \emph{completely} positive map 
    $f\colon \scrA\to\scrB$ between $C^*$-algebras
is given by $\left\|f\right\|=\left\|f(1)\right\|$.
We'll show here that the same result holds
when~$f$ is just positive.
This result will be usefull at the end of this
thesis in~\sref{lem:sef-instrument}, where we'll
try to consider the broadest possible class
of duplicators
$\delta\colon \scrA \otimes \scrA\to\scrA$
    (see~\sref{def:duplicator})
being a priori just positive,
not completely positive.
The proof consists of two ingredients:
    the fact, \sref{normal-russo-dye},
    that $\left\|f(a)\right\|\leq \|f(1)\| \|a\|$
    for all \emph{normal}~$a\in\scrA$ (see~\sref{functional-calculus}),
and the result,
known as Russo--Dye's theorem,
\sref{russo-dye},
that the convex combinations
of unitaries (being normal)
are norm dense in the unit ball~$(\scrA)_1$
of~$\scrA$.
\end{point}
\begin{point}{20}[normal-russo-dye]{Lemma}%
We have $\left\|f(a)\right\| \leq \left\|f(1)\right\|\,\left\|a\right\|$
for every p-map $f\colon \scrA\to\scrB$ between $C^*$-algebras,
and \emph{normal} $a\in\scrA$.
    \begin{point}{30}{Proof}%
Since~$a$ is normal,
the $C^*$-subsalgebra $C^*(a)$
of~$\scrA$ generated by~$a$
is commutative (see~\sref{functional-calculus}),
and so the restriction of~$f$
to a map $f\colon C^*(a)\to\scrB$
is completely positive by~\sref{cp-commutative}.
Thus~$\|f(a)\|\leq \|f(1)\|\,\|a\|$ by~\sref{cp-russo-dye}.\qed
\end{point}
\end{point}
\begin{point}{40}[cstar-unitary]{Definition}%
An element~$u$ of a $C^*$-algebra
is \Define{unitary}\index{unitary!in a $C^*$-algebra}
when $u^*u=1$ and~$uu^*=1$.

    In that case we also say that~$u$ is \Define{\emph{a} unitary}.
\end{point}
\begin{point}{50}{Exercise}%
Let~$\scrA$ be a $C^*$-algebra.
\begin{enumerate}
\item
Show that any~$\lambda\in\C$ with $\left|\lambda\right|=1$
is unitary in~$\scrA$.

In particular, the unit, $1$, of~$\scrA$ is unitary.
\item
Show that a unitary~$u\in \scrA$ is invertible with inverse~$u^{-1}=u^*$,
and that~$u^*$ is a unitary as well.
\item
Show that the product $uv$ of unitaries $u,v\in\scrA$
is unitary.
\item
Show that every unitary~$u$ of~$\scrA$ is normal,
that is, $uu^*=u^*u$
(see~\sref{functional-calculus}).

Show that a normal element~$a$ of~$\scrA$ is unitary
        iff $\Real{a}^2 + \Imag{a}^2 = 1$.
\item
Show that every self-adjoint element~$a$ of~$\scrA$
with~$\|a\|\leq 1$
is the real part of some unitary~$u$, 
        so~$a=\Real{u}\equiv \frac{1}{2}(u+u^*)$.
(Hint:
        take~$u := a + i\sqrt{1-a^2}$.)
\item
Show that every invertible element~$a$ of~$\scrA$ can be written
as $a=u\sqrt{a^*a}$,
where~$u$ is a unitary.
(Hint: take $u=\sqrt{a^{-1}(a^{-1})^*}$.)

This is a variation on the polar decomposition
we'll see in~\sref{polar-decomposition}.
\end{enumerate}
\end{point}
\begin{point}{60}{Exercise}
(Based on II.3.2.14--17 of~\cite{blackadar2006operator}.)
Let~$\scrA$ be a $C^*$-algebra.
\begin{enumerate}
\item
Show that every invertible element~$a$ of~$\scrA$
with~$\|a\|\leq 2$ can be written
as the sum of two unitaries.
(Hint: write $a=u\sqrt{a^*a}$ with~$u$ as above.)
\item
Let~$u\in \scrA$ be a unitary, and~$a\in\scrA$ with $\|a\|< 1$.

Show that~$u+a$ is the sum of two unitaries.

(Hint: write~$u+a=u(1+u^*a)$, and
        note that $1+u^*a$ is invertible by~\sref{geometric}.)
\item
Let~$a\in \scrA$ be given,
and let~$N$ be a natural number with $\|a\| < N$.

Show that~$a$ is the sum of~$N+2$ unitaries. 

(Hint: write $a=1+(N+1)b$ where $b:=\frac{a-1}{N+1}$,
and show that $\|b\|< 1$.)

\item
Prove the following theorem.
\end{enumerate}
\end{point}
\begin{point}{70}[russo-dye]{Theorem (Russo--Dye)}%
\index{Russo--Dye's Theorem}
An element~$a$ of a $C^*$-algebra $\scrA$
with $\|a\|< 1-\frac{2}{N}$
for some natural number~$N$
can be written as~$a=\frac{1}{N}(u_1+\dotsb + u_N)$
for some unitaries $u_1,\dotsc,u_N\in \scrA$.
\end{point}
\begin{point}{80}[russo-dye-cor]{Corollary}%
The operator norm of a positive
    linear map~$f\colon\scrA\to\scrB$ between
    $C^*$-algebras is given by  $\|f\|=\|f(1)\|$.
\begin{point}{90}{Proof}%
We must show that $\|f(a)\|\leq \|f(1)\|$
for every~$a\in\scrA$ with~$\|a\|\leq 1$.
Since by Russo--Dye's theorem 
every~$a\in\scrA$ with~$\|a\|\leq 1$ 
may be approximated with respect to the norm by a sequence
of elements of the form  $b:=\frac{1}{N}(u_1+\dotsb+u_N)$,
where~$u_1,\dotsc,u_N$ are unitaries,
it suffices to show that $\|f(b)\|\leq \|f(1)\|$
for such~$b$.
Since~$u_n$ is normal,
    and thus $\|f(u_n)\|\leq \|f(1)\|\,\|u_n\|\leq \|f(1)\|$
    by~\sref{normal-russo-dye},
    we get $\|f(b)\|\leq \frac{1}{N}(\|f(u_1)\|\,+\,\dotsb\,+\,\|f(u_N)\|)
    \leq \|f(1)\|$, and so $\|f\|=\|f(1)\|$.\qed
\end{point}
\end{point}
\end{parsec}

\section{Towards von Neumann Algebras}
\begin{parsec}{350}%
\begin{point}{10}%
Let us work towards
the subject of the next chapter, von Neumann algebras,
by pointing out two special properties
of~$\scrB(\scrH)$
on which the definition of a von Neumann algebra is based,
namely that
\begin{enumerate}
\item
any norm-bounded directed subset of
self-adjoint operators on~$\scrH$
has a supremum (in~$\Real{\scrB(\scrH)}$), and
\item
all vector functionals
$\left<x,(\,\cdot\,)x\right>\colon \scrB(\scrH)\to\C$ 
preserve these suprema.
\end{enumerate}
We'll end
the chapter
by showing
in~\sref{bh-np}
 that every  functional on~$\scrB(\scrH)$
that  preserves the aforementioned
suprema
is a (possibly infinite) sum of vector functionals.
\end{point}

\subsection{Directed Suprema}
\begin{point}{20}[pub]{Theorem (Uniform Boundedness)}%
\index{Principle of Uniform Boundedness}%
\index{Uniform Boundedness Theorem}%
A set~$\scrF$ of bounded linear maps 
from a complete normed vector space~$\scrX$
to a normed vector space~$\scrY$
is bounded
in the sense that $\sup_{T\in \scrF} \|T\|<\infty$
provided that 
 $\sup_{T\in \scrF} \|Tx\|<\infty$
 for all~$x\in \scrX$.
\begin{point}{30}{Proof}%
Based on~\cite{sokal}.
\begin{point}{40}[sokal-lemma]%
Let $r>0$ and~$T\in\scrF$ be given.
Writing~$B_r(x)=\{\,y\in\scrX\colon \|x-y\|\leq r\,\}$
for the ball around~$x\in\scrX$ with radius~$r$,
note that $r\|T\|=\sup_{\xi\in B_r(0)} \|T \xi\|$
almost by definition of the operator norm.
We will need the less obvious fact
that $r\|T\|\leq \sup_{\xi \in B_r(x)}\|T \xi\|$
for every~$x\in \scrX$.

To see why this is true,
note that for~$\xi\in B_r(0)$
either $\|T\xi\|\leq \|T(x+\xi)\|$
or $\|T\xi\|\leq \|T(x-\xi)\|$,
because we would otherwise have
$2\|T\xi\| = \|T(x+\xi)-T(x-\xi)\|
\leq \|T(x+\xi)\|+\|T(x-\xi)\|<2\|T\xi\|$.
Hence
$r\|T\|=\sup_{\xi\in B_r(0)} \|T\xi\|\leq  
\sup_{\xi \in B_r(x)} \|T\xi \|$.
\end{point}
\begin{point}{50}%
Suppose towards a contradiction
that $\sup_{T\in\scrF}\|T\|=\infty$,
and pick~$T_1,T_2,\dotsc$ with $\|T_n\|\geq n3^{n}$.
Using~\sref{sokal-lemma},
choose $x_1,x_2,\dotsc$ in~$\scrX$
with $\|x_{n}-x_{n-1}\|\leq 3^{-n}$
and~$\|T_{n} x_{n}\|\geq \frac{2}{3}3^{-n}\|T_{n}\|$,
so that~$(x_n)_n$ is a Cauchy sequence, 
and therefore converges to some
$x\in\scrX$.
Note that~$\|x-x_n\|\leq \frac{1}{2}3^{-n}$
(because $\sum_{k=0}^\infty 3^{-k}=\frac{3}{2}$),
and so $\|T_n x\|\geq  \|T_nx_n\| - \|T_n(x_n-x)\|
\geq \frac{2}{3}3^{-n}\|T_n\|-\frac{1}{2}3^{-n}\|T_n\|
\geq \frac{1}{6}n$,
which contradicts
the assumption that $\sup_{T\in \scrF} \|Tx\| <\infty$.\qed
\end{point}
\end{point}
\end{point}
\begin{point}{60}[hellinger-toeplitz]{Theorem}%
Let~$T\colon X\to Y$ be an adjointable map
between pre-Hilbert $\scrA$-modules.
If either~$X$ or~$Y$ is complete,
then~$T$ and~$T^*$ are bounded.
\begin{point}{70}{Proof}%
We may assume without loss of generality
that~$X$ is complete (by swapping~$T$ for~$T^*$
and~$X$ with~$Y$ if necessary).

Note that for every~$y\in Y$,
the linear map $\left<y,T\,\cdot\,\right>\equiv
\left<T^*y,\,\cdot\,\right>\colon Y\to \scrA$
is bounded,
because $\left\|\left<T^*y,x\right>\right\| \leq \|T^*y\|\|x\|$
for all~$x\in X$ (see~\sref{chilb-cs}).

Since 
on the other hand,
$\left\|\left<y,Tx\right>\right\|
\leq \|y\|\,\|Tx\|\leq \|Tx\|$
for all~$x\in X$ and~$y\in Y$ with $\|y\|\leq 1$,
we have $\sup_{\|y\|\leq 1} \|\left<y,Tx\right>\| \leq \|Tx\|<\infty$
for all~$x\in X$,
and thus $B:=\sup_{\|y\|\leq 1} \|\left<y,T\,\cdot\,\right>\|<\infty$
by~\sref{pub}.

It follows that~$\|\left<y,Tx\right>\|\leq B\|y\|\|x\|$
for all~$y\in Y$ and~$x\in X$,
and thus~$T$ and~$T^*$ are bounded, by~\sref{chilb-form-bounded}.\qed
\end{point}
\begin{point}{80}{Remark}%
As a special case of the preceding theorem
we get the fact,
known as the \Define{Hellinger--Toeplitz theorem},%
\index{Hellinger--Toeplitz's Theorem}
that every symmetric
operator on a Hilbert space is bounded.
\end{point}
\begin{point}{90}[hellinger-toeplitz-needs-complete]{Example}%
The condition that either~$X$ or~$Y$ be complete may not be dropped:
the linear map $T\colon c_{00}\to c_{00}$
given by $T\alpha = (n\alpha_n)_n$ for $\alpha\in c_{00}$
is self-adjoint,
but not bounded,
because~$T$ maps $(1,\frac{1}{2},\dotsc,\frac{1}{n},0,0,\dotsc)$
having 2-norm below~$\frac{\pi}{\sqrt{6}}$
to $(1,1,\dotsc,1,0,0,\dotsc)$,
which has $2$-norm equal to~$\sqrt{n}$.
\end{point}
\end{point}
\end{parsec}

\begin{parsec}{360}%
\begin{point}{10}[self-dual]{Definition}%
A Hilbert $\scrA$-module~$X$ is \Define{self-dual}%
\index{Hilbert $\scrA$-module!self dual}
when every bounded module map $r\colon X\to \scrA$
is of the form $r\equiv \left<y,(\,\cdot\,)\right>$
for some~$y\in X$.
\end{point}
\begin{point}{20}{Example}%
By Riesz' representation theorem (\sref{riesz-representation-theorem})
every Hilbert space is self-dual.
\end{point}
\begin{point}{30}{Exercise}%
Show that given a $C^*$-algebra~$\scrA$
the Hilbert $\scrA$-module $\scrA^{N}$
of $N$-tuples is self dual.
\end{point}
\begin{point}{40}[chilb-form]{Definition}%
Let us say that a \Define{(bounded) form}%
\index{form, between Hilbert $\scrA$-modules}%
\index{form, between Hilbert $\scrA$-modules!bounded}
on Hilbert $\scrA$-modules
$X$ and~$Y$
is a map $[\,\cdot\,,\,\cdot\,]\colon X\times Y\to \scrA$
such that $[x,\,\cdot\,]\colon Y\to \scrA$
and $[\,\cdot\,,y]^*\colon X\to \scrA$
are (bounded) module maps for all~$x\in X$ and~$y\in Y$.
\end{point}
\begin{point}{50}[chilb-form-representation]{Proposition}%
For every bounded form  $[\,\cdot\,,\,\cdot\,]\colon X\times Y\rightarrow \scrA$
on self-dual Hilbert $\scrA$-modules
$X$ and~$Y$
there is a unique adjointable bounded module map
$T\colon X\to Y$.
with
$[x,y]\equiv \left<Tx,y\right>$
for all $x\in X$ and~$y\in Y$.
\begin{point}{60}{Proof}%
Let $x\in X$ be given.
Since~$[x,\,\cdot\,]\colon Y\to \scrA$ is a
a bounded module map,
and~$Y$ is self-dual,
there is a unique $Tx\in Y$ with
$[x,y]=\left<Tx,y\right>$
for all~$y\in Y$,
giving a map $T\colon X\to Y$.
For a similar reason
we get a map $S\colon Y\to X$
with $\left<Sy,x\right>=[x,y]^*$ 
for all~$x\in X$ and~$y\in Y$.
Since $S$ and~$T$ are clearly adjoint,
they are bounded module maps by~\sref{hellinger-toeplitz}.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{370}%
\begin{point}{10}%
	Another consequence of~\sref{pub}
	is this:
\end{point}
\begin{point}{20}[hilb-weakly-bounded-complete]{Proposition}%
Given a net~$(y_\alpha)_\alpha$
in a Hilbert space~$\scrH$
for which $\left<y_\alpha,x\right>$
is Cauchy \emph{and bounded}
for every~$x\in \scrH$,
there is a unique~$y\in\scrH$
with $\left<y,x\right>=\lim_\alpha \left<y_\alpha,x\right>$
for all~$x\in\scrH$.
\begin{point}{30}{Proof}%
To obtain~$y$,
we want to apply  Riesz' representation theorem
(\sref{riesz-representation-theorem})
to the linear map $f\colon \scrH\to\C$
defined by~$f(x)=\lim_\alpha\left<y_\alpha,x\right>$,
but must first show that~$f$ is bounded.
For this it suffices to show
that~$\sup_\alpha \left \|\left<y_\alpha,(\,\cdot\,)\right>\right\|<\infty$,
and this follows by~\sref{pub}
from the assumption 
that $\sup_{\alpha} \left|\left<y_\alpha,x\right>\right| <\infty$
for every~$x\in \scrH$.

By Riesz' representation theorem (\sref{riesz-representation-theorem}),
there is a unique~$y\in\scrH$ with 
$\left<y,x\right>=f(x)\equiv \lim_\alpha \left<y_\alpha,x\right>$
for all~$x\in \scrH$,
and so we're done.\qed
\end{point}
\begin{point}{40}{Remark}%
The condition in~\sref{hilb-weakly-bounded-complete} 
that the net~$(\,\left<y_\alpha,x\right>\,)_\alpha$
be bounded for every~$x$ may not be omitted
(even though $(\,\left<y_\alpha,x\right>\,)_\alpha$
being Cauchy is eventually bounded).

To see this,
consider a linear map $f\colon \scrH\to\C$ on a Hilbert space~$\scrH$
which is not bounded.
We claim that there is a net~$(y_\alpha)_\alpha$ in~$\scrH$
with $f(x)=\lim_\alpha \left<y_\alpha,x\right>$ for all~$x\in\scrH$,
and so there can be no~$y\in \scrH$ 
with $\left<y,x\right> = \lim_\alpha \left<y_\alpha,x\right>$
for all~$x\in \scrH$, because 
that would imply that~$f$ is bounded.

To create this net,
note that~$f$ is bounded
on the span $\left<F\right>$ of every 
finite subset $F\equiv \{x_1,\dotsc,x_n\}$
of vectors from~$\scrH$,
and so by Riesz' representation theorem~\sref{riesz-representation-theorem}
applied to~$f$ restricted to closed subspace~$\left<F\right>$
of~$\scrH$ there is a unique $y_F\in \left<F\right>$
such that~$f(x)=\left<y_F,x\right>$
for all~$x\in\left<F\right>$.

These $y_F$'s form a net in~$\scrH$
(when we order the finite subsets~$F$ of~$\scrH$ by inclusion),
which approximates~$f$ in the
sense that~$f(x)=\lim_F \left<y_F,x\right>$
for every~$x\in \scrH$,
(because 
$f(x)=\left<y_F,x\right>$
for every~$F$
with $\{x\}\subseteq F$).
\end{point}
\end{point}
\begin{point}{50}[swot]{Definition}%
Let~$\scrH$ be a Hilbert space.
\begin{enumerate}
\item
	The \Define{weak operator topology (WOT)}%
\index{WOT, weak operator topology}
on~$\scrB(\scrH)$ is the least topology
with respect to which $T\mapsto \left<x,Tx\right>,\,\scrB(\scrH)\to\C$
is continuous for every~$x\in\scrH$.

So a net $(T_\alpha)_\alpha$
		converges to~$T$ in $\scrB(\scrH)$
		with respect to the weak operator topology
		iff $\left<x,T_\alpha x\right>\to \left<x,Tx\right>$
		as~$\alpha\to\infty$ for all~$x\in \scrH$.
\item
	The \Define{strong operator topology (SOT)}%
\index{SOT, strong operator topology}
on~$\scrB(\scrH)$ is the least topology
with respect to which $T\mapsto \|Tx\|\equiv \smash{\left<x,T^*Tx\right>^{%
\nicefrac{1}{2}}}$
is continuous for every~$x\in\scrH$.

		So a net $(T_\alpha)_\alpha$
		converges to~$T$
		in $\scrB(\scrH)$
		with respect to the strong operator topology iff
		$\|T_\alpha x -Tx \| \to 0$ as $\alpha\to\infty$
		for all~$x\in\scrH$.
\end{enumerate}
\begin{point}{60}{Remark}%
Although we'll only make use of the weak operator
topology we have nonetheless included
the definition of the strong operator topology here
for comparison
with the \emph{ultrastrong
topology} that appears in the next chapter.
\end{point}
\end{point}
\begin{point}{70}[bh-wot-bounded-complete]{Lemma}%
Let~$(T_\alpha)_\alpha$ be a net of bounded operators
on a Hilbert space~$\scrH$
such that $(\,\left<x,T_\alpha x \right>\,)$ is
Cauchy and bounded for every~$x\in \scrH$.

Then~$(T_\alpha)_\alpha$
WOT-converges to some bounded operator~$T$ in $\scrB(\scrH)$.
\begin{point}{80}{Proof}%
Let~$x,y\in \scrH$ be given.
Since by a simple computation
\begin{equation*}
	\textstyle
	\left<y,T_\alpha x\right>
	\ = \ \frac{1}{4}\sum_{k=0}^3
	i^k\left<\,i^ky+x,\,T_\alpha (i^ky+x)\,\right>,
\end{equation*}
 $(\,\left<y,T_\alpha x\right>\,)_\alpha$
is bounded for every~$y\in \scrH$,
and so by~\sref{hilb-weakly-bounded-complete} there is~$Tx\in \scrH$ 
with $\left<y,Tx\right>=\lim_\alpha \left<y,T_\alpha x\right>$
for all~$y\in\scrH$,
giving us a linear map $T\colon \scrH\to \scrH$.
It is clear that~$(T_\alpha)_\alpha$
WOT-converges to~$T$,
provided that~$T$ is bounded.

So to complete the proof,
we must show that~$T$ is bounded,
and we'll do this by showing that~$T$ has an adjoint
(see~\sref{hellinger-toeplitz}).
Note that $\left<x,T_\alpha^* x\right>=\overline{\left<x,T_\alpha x\right>}$
is Cauchy and bounded (with~$\alpha$ running),
so by a similar reasoning as before (but with~$T^*_\alpha$
instead of~$T_\alpha$)
we get a map
$S\colon \scrH\to\scrH$
with $\left<x,Sy\right>=\lim_\alpha \left<x,T^*_\alpha y\right>$
for all~$x,y\in\scrH$, which will be adjoint to~$T$,
which is therefore bounded.\qed
\end{point}
\end{point}
\begin{point}{90}[hilb-suprema]{Proposition}%
Let~$\scrH$ be a Hilbert space,
and~$\scrD$ an upwards directed subset of~$\Real{\scrB(\scrH)}$
with $\sup_{T\in \scrD} \left<x,Tx\right> <\infty$
for all~$x\in \scrH$. Then
\begin{enumerate}
\item
$(T)_{T\in\scrD}$
converges 
in the weak operator topology
to some~$T'$ in~$\Real{(\scrB(\scrH))}$,
\item
$T'$ is the supremum of~$\scrD$
in $\Real{(\scrB(\scrH))}$,
and 
\item
$\left<x,T'x\right> = 
\sup_{T\in\scrD}\left<x,Tx\right> $
for all~$x\in \scrH$.
\end{enumerate}
\begin{point}{100}{Proof}%
Let~$x\in \scrH$.
Since $\left<x,(\,\cdot\,)x\right>\colon \scrB(\scrH)\to \C$
is positive
we see that
$(\left<x,Tx\right>)_{T\in\scrD}$
is an increasing net in~$\R$, 
bounded from above (by assumption),
and therefore converges to~$\sup_{T\in\scrD}\left<x,Tx\right>$.
In particular,~$(T)_{T\in\scrD}$
is WOT-Cauchy,
and ``WOT-bounded'',
and thus
(by~\sref{bh-wot-bounded-complete})
WOT-converges to some self-adjoint~$T'$ from~$\scrB(\scrH)$.

Since $(\,\left<x,Tx\right>\,)_{T\in\scrD}$
converges both to~$\left<x,T'x\right>$,
and to~$\sup_{T\in\scrD} \left<x,Tx\right>$,
we conclude that $\left<x,T'x\right>=\sup_{T\in\scrD}\left<x,Tx\right>$
for every~$x\in\scrH$.
In particular,  $\left<x,Tx\right>\leq \left<x,T'x\right>$
for all~$x\in\scrH$ and $T\in\scrD$, and thus $T\leq T'$
for all~$T\in\scrD$.

Let~$S$ be a self-adjoint bounded operator on~$\scrH$ with $T\leq S$
for all~$T\in\scrD$.
To prove that~$T'$ is the supremum of~$\scrD$,
we must show that~$T'\leq S$.
Let~$x\in \scrH$ be given.
Since $\left<x,Tx\right>\leq \left<x,Sx\right>$
for each~$T\in \scrD$ (because $T\leq S$),
we have $\left<x,T'x\right>\equiv \sup_{T\in\scrD} \left<x,Tx\right>
\leq \left<x,Sx\right>$,
and therefore $T'\leq S$ by~\sref{hilb-positive-operators}.\qed
\end{point}
\end{point}
\begin{point}{110}{Definition}%
Let~$\scrH$ be a Hilbert space.
The supremum of a (norm) bounded directed subset~$\scrD$ 
in~$\Real{(\scrB(\scrH))}$
(which exists by~\sref{hilb-suprema})
is denoted by~$\Define{\bigvee\scrD}$.%
\index{*sup@$\bigvee D$, supremum of~$D$!in $\scrB(\scrH)$}
\end{point}
\end{parsec}
\subsection{Normal Functionals}
\begin{parsec}{380}%
\begin{point}{10}[bh-normal]{Definition}%
Given a Hilbert space~$\scrH$
a p-map $\omega\colon \scrB(\scrH)\to\C$
is called \Define{\textbf{n}ormal}%
\index{normal!positive functional!on $\scrB(\scrH)$}
when
$\omega(\bigvee \scrD)=\bigvee_{T\in\scrD} \omega(T)$
for every bounded directed subset $\scrD$ of~$\Real{\scrB(\scrH)}$.
\end{point}
\begin{point}{20}{Example}%
All vector functionals%
\index{vector functional!for a Hilbert space!is normal}
$\left<x,(\,\cdot\,)x\right>$ are normal by~\sref{hilb-suprema}.
\end{point}
\begin{point}{30}[bh-normal-effects]{Exercise}%
To show that a positive linear functional is normal, it suffices to show 
that it preserves directed suprema of \emph{effects}: 
show that given a Hilbert space~$\scrH$
a  positive map $\omega\colon \scrB(\scrH)\to\C$
is normal 
provided that $\omega(\bigvee \scrD) = \bigvee_{T\in\scrD} \omega(T)$
for every directed subset $\scrD$ of $[0,1]_{\scrB(\scrH)}$.
\end{point}
\begin{point}{40}[bh-functional-lemma]{Lemma}%
Every sequence $x_1,x_2,\dotsc $ in a Hilbert space~$\scrH$
with $\sum_n \|x_n\|^2 < \infty$
gives a np-map $\omega\colon\scrB(\scrH)\to\C$
defined by~$\omega(T)=\sum_n \left<x_n,Tx_n\right>$.
\begin{point}{50}{Proof}%
Given $T\in\scrB(\scrH)$ 
we have $\left|\left<x_n,Tx_n\right>\right|\leq \|x_n\|^2\|T\|$ 
by Cauchy--Schwarz (\sref{inner-product-basic}),
so $\sum_n \left|\left<x_n,Tx_n\right>\right|
\leq \|T\| \sum_n \|x_n\|^2$,
which means that~$\sum_n \left<x_n,Tx_n\right>$
converges, 
and so we may define~$\omega$ as above.

It is easy to see that~$\omega$ is linear and positive,
so we'll only show that~$\omega$ is normal.
We must prove that $\omega(\bigvee \scrD)=\bigvee_{T\in\scrD} \omega(T)$
for every bounded directed subset of~$\Real{(\scrB(\scrH))}$.
By~\sref{bh-normal-effects}
we may assume without loss of generality that 
$\scrD\subseteq [0,1]_{\scrB(\scrH)}$.
This has the benefit that $\left<x_n,T x_n\right>$
is positive for all~$n$ and~$T\in\scrD$,
so that their sum (over~$n$) is given by
a supremum over partial sums, viz.~$\sum_n\left<x_n,Tx_n\right>
=\bigvee_N\sum_{n=1}^N\left<x_n,Tx_n\right>$.
Completing the proof is now simply a matter of
interchanging suprema,
\begin{alignat*}{3}
	\textstyle \bigvee_{T\in \scrD} \omega(T)
	\ &=\ 
	\textstyle\bigvee_{T\in \scrD} \bigvee_N \sum_{n=1}^N 
	\left<x_n,Tx_n\right>\\
	\ &=\ 
	\textstyle\bigvee_N \bigvee_{T\in \scrD}\sum_{n=1}^N 
	\left<x_n,Tx_n\right>\\
	\ &=\ 
	\textstyle\bigvee_N \sum_{n=1}^N \left<x_n,(\bigvee \scrD )\,x_n\right>
	\ =\ \textstyle\omega(\bigvee\scrD),
\end{alignat*}
where we used that~$\sum_{n=1}^N \left<x_n,(\,\cdot\,)x_n\right>$
is normal.\qed
\end{point}
\end{point}
\begin{point}{60}[vector-functional-convergence]{Exercise}%
The following observations
regarding
a net~$(x_\alpha)_\alpha$ in a Hilbert space~$\scrH$
will be useful later on.
\begin{enumerate}
\item
Show that~$\sum_\alpha \|x_\alpha\|^2<\infty$
if and only if~$\sum_\alpha \left<x_\alpha,(\,\cdot\,)x_\alpha\right>$
converges with respect to the operator norm
to some bounded functional on~$\scrB(\scrH)$.
\item
Given some~$x\in \scrH$,
show that~$x_\alpha$ converges to~$x$
if and only if  $\left<x_\alpha,(\,\cdot\,)x_\alpha\right>$
operator-norm converges to~$\left<x,(\,\cdot\,)x\right>$.

(For the ``if'' part it may be convenient
to first prove that $\left<x_\alpha,x\right>\to \left<x,x\right>$
by considering the bounded operator
$\ketbra{x}{x}$ on~$\scrB(\scrH)$.)

\end{enumerate}
\end{point}
\end{parsec}
\begin{parsec}{390}%
\begin{point}{10}%
The final project of this chapter
is to show that each normal positive functional~$\omega$ 
on a~$\scrB(\scrH)$
	is of the form 
	$\omega\equiv \sum_{n=0}^\infty\left<x_n,(\,\cdot\,)x_n\right>$
	for some~$x_1,x_2,\dotsc$
	with~$\sum_n\|x_n\|^2<\infty$.
For this we'll need some more nuggets
from the theory of Hilbert spaces.
\end{point}
\begin{point}{20}{Definition}%
A subset~$\scrE$ of a Hilbert space
is called \Define{orthonormal}%
\index{orthonormal, subset of a Hilbert space}
if $\left<e,e'\right>=0$
for all~$e,e'\in\scrE$ with~$e\neq e'$,
and~$\left<e,e\right>=1$
for all~$e\in\scrE$.
We say that~$\scrE$ is \Define{maximal}
\index{orthonormal, subset of a Hilbert space!maximal}
when~$\scrE$ is maximal
among all orthonormal  subsets of~$\scrH$
ordered by inclusion,
and in that case
we call~$\scrE$ an \Define{orthonormal basis}%
\index{orthonormal basis, for a Hilbert space}
for~$\scrH$
for reasons that will be become clear in~\sref{orthonormal}
below.
\begin{point}{30}{Remark}%
Clearly, by Zorn's lemma,
each Hilbert space has an orthonormal basis.
\end{point}
\end{point}
\begin{point}{40}[orthonormal]{Proposition}%
Given an orthonormal subset~$\scrE$
of a Hilbert space~$\scrH$,
and~$x\in \scrH$,
\begin{enumerate}
\item
\label{orthonormal-1}
\Define{(Bessel's inequality)}%
\index{Bessel's inequality}
\ 
 $\sum_{e\in \scrE}\left|\left<e,x\right>\right|^2
\leq \|x\|^2$;
\item
\label{orthonormal-2}
$\sum_{e\in \scrE} \left<e,x\right>e$
converges in~$\scrH$,
\item
\label{orthonormal-3}
$\sum_{e\in \scrE} \left<e,x\right>e=x$
if~$\scrE$ is maximal, and
\item
\label{orthonormal-4}
\Define{(Parseval's identity)}%
\index{Parseval's identity}
$\sum_{e\in\scrE}\left|\left<e,x\right>\right|^2 = \|x\|^2$
if~$\scrE$ is maximal.
\end{enumerate}
\begin{point}{50}{Proof}%
\ref{orthonormal-1}\ 
Since for finite subset $\scrF$ of $\scrE$
we have $0\leq \|x-\sum_{e\in \scrF} \left<e,x\right>e\|^2
= \|x\|^2-2\sum_{e\in \scrF} \left<e,x\right>\left<x,e\right>
+ \sum_{e,e'\in\scrF} \left<x,e'\right>\left<e',e\right>\left<e,x\right>
= \|x\|^2-\sum_{e\in\scrF}\left|\left<e,x\right>\right|^2$,
and so~$\sum_{e\in\scrF} \left|\left<e,x\right>\right|^2\leq \|x\|^2$,
we get~$\sum_{e\in\scrE}\left|\left<e,x\right>\right|^2\leq \|x\|^2$.

\ref{orthonormal-2}\ 
From the observation
that~$\|\sum_{e\in\scrF} \left<e,x\right>e\|^2
= \sum_{e\in \scrF} \left|\left<e,x\right>\right|^2$
for any finite~$\scrF\subseteq \scrE$,
and the fact that~$\sum_{e\in \scrE} \left|\left<e,x\right>\right|^2$
converges (by the previous point),
one deduces that~$(\sum_{e\in\scrF} \left<e,x\right>e)_\scrF$
is Cauchy,
and so~$\sum_{e\in\scrE} \left<e,x\right>e$
converges.

\ref{orthonormal-3}\
Writing~$y:=\sum_{e\in\scrE} \left<e,x\right>e$ we must show that~$x=y$.
If it were not so,
if~$x\neq y$,
then~$e':=\|x-y\|^{-1}(x-y)$
satisfies
$\left<e',e'\right>=1$
and
$\left<e',e\right>=0$
for all~$e\in\scrE$,
and so may be added to~$\scrE$
to yield an orthonormal basis~$\scrE\cup\{e'\}$
extending~$\scrE$
contradicting~$\scrE$s  maximality.

\ref{orthonormal-4}\ 
Finally,
$\|x\|^2=\left<x,x\right>
= \sum_{e,e'\in\scrE}
\left<x,e'\right>\left<e',e\right>\left<e,x\right>
= \sum_{e\in \scrE} \left|\left<e,x\right>\right|^2$.\qed
\end{point}
\end{point}
\begin{point}{60}[sum-ketbras]{Exercise}%
Let~$\scrE$ be an orthonormal basis of a Hilbert space~$\scrH$.
\begin{enumerate}
\item
Show that~$\sum_{e\in\scrE} \ketbra{e}{e}$
converges to~$1$
in the weak operator topology.
\item
Show that $\sum_{e\in\scrE}\ketbra{e}{e}=1$
also in the sense
that the directed
set of partial sums
$\sum_{e\in \scrF} \ketbra{e}{e}$,
where~$\scrF$ is a finite subset of~$\scrE$,
has~$1$ as its supremum.
\item
Conclude that~$\omega(1)=\sum_{e\in\scrE} \omega(\ketbra{e}{e})$
for every np-map $\omega\colon \scrB(\scrH)\to\C$.
\end{enumerate}
\end{point}
\begin{point}{70}[bh-np-lemma]{Lemma}%
Given a Hilbert space~$\scrH$
with orthonormal basis~$\scrE$,
we have
\begin{equation*}
	\omega(A)\ = \ 
	\sum_{e,e'\in\scrE} \left<e,Ae'\right>\ \omega(\,\ketbra{e}{e'}\,).
\end{equation*}
for every normal p-map $\omega\colon \scrB(\scrH)\to\C$
and~$A\in\scrB(\scrH)$.
\begin{point}{80}{Proof}%
Let~$\scrF$ be a finite subset of~$\scrE$,
and write $P=\sum_{e\in \scrF} \ketbra{e}{e}$.
Since $PAP
= \sum_{e,e'\in\scrF}\left<e,Ae'\right>\,\ketbra{e}{e'}$
it suffices
to show that~$\omega(A-PAP)$
vanishes as~$\scrF$ increases.
Note that~$P^*P=P$ and
$(P^\perp)^*P^\perp=P^\perp$.
Further,
since
$\|P\|\leq 1$,
and~$A-PAP=P^\perp A + PAP^\perp$,
we have,
by Kadison's inequality,
\begin{alignat*}{3}
\left|\omega(A-PAP)\right|
\ &\leq\  \left|\omega(P^\perp A)\right| \,+\,\left|\omega(PAP^\perp)\right| \\
\ &\leq\  
\omega(P^\perp)^{\nicefrac{1}{2}}\,
\omega(A^*A)^{\nicefrac{1}{2}}
\ +\ 
\omega(PAA^*P)^{\nicefrac{1}{2}}\,
\omega(P^\perp)^{\nicefrac{1}{2}}\\
\ &\leq\  
2\|A\| 
\omega(1)^{\nicefrac{1}{2}}\ 
\omega(P^\perp)^{\nicefrac{1}{2}}.
\end{alignat*}
But since~$\sum_{e\in\scrE} \omega(\ketbra{e}{e})=\omega(1)$
by~\sref{sum-ketbras}
we see that~$\omega(P^\perp)\to0$
as~$\scrF\to\infty$.\qed
\end{point}
\end{point}
\begin{point}{90}[bh-np]{Theorem}%
\index{normal!positive functional!on~$\scrB(\scrH)$}%
Let~$\scrH$ be a Hilbert space.
Every normal p-map $\omega\colon \scrB(\scrH)\to \C$
is of the form $\omega = \sum_n\left<x_n,(\,\cdot\,)x_n\right>$
where $x_1,x_2,\dotsc\in \scrH$
with~$\sum_n \|x_n\|^2=\|\omega\|$.
\begin{point}{100}{Proof}%
By~\sref{chilb-form-representation}
there is a
unique
$\varrho\in\scrB(\scrH)$
with $\omega(\ketbra{y}{x})=\left<x,\varrho y\right>$
for all~$x,y\in\scrH$,
because
$(x,y)\mapsto \omega(\ketbra{y}{x}),\,
\scrH\times\scrH\to\C$
is a bounded form in the sense of~\sref{chilb-form}.
Note that~$\varrho$ 
is positive by~\sref{hilb-positive-operators}
because $\left<x,\varrho x\right>
=\omega(\ketbra{x}{x})\geq 0$
for all~$x\in\scrH$.
Now, let~$\scrE$ be an orthonormal basis for~$\scrH$.
Since~$\omega$ is normal,
\sref{sum-ketbras} gives us
$\omega(1)=\sum_{e\in\scrE} \omega(\ketbra{e}{e})
= \sum_{e\in\scrE} \left<e,\varrho e\right>
= \sum_{e\in \scrE} \|\sqrt{\varrho} e\|^2$,
so that $\omega':=\sum_{e\in\scrE} \left<\sqrt{\varrho}e,(\,\cdot\,)
\sqrt{\varrho}e\right>$
defines a normal positive functional on~$\scrB(\scrH)$
by~\sref{vector-functional-convergence}.
Thus,
we are done if
can show that~$\omega'=\omega$,
(because $\sqrt{\varrho}e$
is non-zero for at most countably many~$e\in\scrE$).
To this end,
note that
$\omega(\ketbra{x}{x})
= \left<\sqrt{\varrho}x,\sqrt{\varrho}x\right>
= \sum_{e\in \scrE}
\left<\sqrt{\varrho}x,e\right>
\left<e,\sqrt{\varrho}x\right>
= \sum_{e\in \scrE} \left< \sqrt{\varrho}e,
\ketbra{x}{x} \sqrt{\varrho}e \right>
=\omega'(\ketbra{x}{x})$
for each~$x\in\scrH$,
and so $\omega(\ketbra{x}{y})=\omega'(\ketbra{x}{y})$
for all~$x,y\in\scrH$
by polarization,
and thus~$\omega=\omega'$
by~\sref{bh-np-lemma}.\qed
\end{point}
\end{point}
\end{parsec}
\begin{parsec}{400}
\begin{point}{10}
In this chapter
we've studied
the algebraic  structure of 
the space~$\scrB(\scrH)$
of bounded operators on a Hilbert space~$\scrH$
abstractly 
via the notion
of
a $C^*$-algebra.
We've seen not only that every $C^*$-algebra
is miu-isomorphic to a $C^*$-subalgebra
	of such a $\scrB(\scrH)$ (in~\sref{gelfand-naimark}),
but also that any commutative $C^*$-algebra
is miu-isomorphic
to the space~$C(X)$ of continuous functions
	on some compact Hausdorff space (in~\sref{gelfand}).
But there's more to~$\scrB(\scrH)$
than just being a $C^*$-algebra:
it has the two additional properties
of having suprema of bounded directed subsets (see~\sref{hilb-suprema}),
and having a faithful collection 
of normal functionals (viz.~the vector functionals,
\sref{hilb-vector-states-order-separating}).
This leads us to the study of von Neumann algebras---the topic
of the next chapter.
\end{point}
\end{parsec}
